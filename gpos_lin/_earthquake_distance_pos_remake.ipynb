{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "_earthquake_distance_LDA.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "_PCVERSION_=True\n",
        "\n",
        "\n",
        "if _PCVERSION_:\n",
        "    basedir=\"C:/Users/sipocz/OneDrive/Dokumentumok/GitHub/_EarthQuake/gpos_lin\"\n",
        "else:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive',force_remount=True)\n",
        "    basedir=\"/content/drive/My Drive/001_AI/_EarthQuake/gpos_lin\"\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "#--------------scikit import \n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "#--------------\n",
        "\n",
        "def outlierStatistic(X_train_predict):\n",
        "    print(X_train_predict)\n",
        "    maxX=len(X_train_predict)\n",
        "    outlier=0\n",
        "    for i in X_train_predict:\n",
        "        if i==-1:\n",
        "            outlier+=1\n",
        "    print(f\"A összes ({maxX} darabból {outlier} darab outlier van. Az {outlier/maxX*100:5.1f} %.)\")\n",
        "\n",
        "\n",
        "def checkvalues(df,columnname,key):\n",
        "    print(f\"{columnname} ellenőrzése !\")\n",
        "    numok=0\n",
        "    numerr=0\n",
        "    for i in df.index:\n",
        "        if df.at[i,columnname] in key:\n",
        "            #print(df.at[i,columnname])\n",
        "            numok+=1\n",
        "            pass\n",
        "        else:\n",
        "            numerr+=1\n",
        "            print(df.at[i,columnname],end=\", \")\n",
        "    sumall=numok+numerr\n",
        "    print(f\"\\n{sumall} mintából {numerr} db nem volt megfelelő\")\n",
        "\n",
        "\n",
        "def createcolumn(df,columnname,keys):\n",
        "    print(f\"{columnname} cseréje megy\")\n",
        "    for key in keys:\n",
        "        df[keys[key]]=0.0\n",
        "    for key in keys:\n",
        "        for i in df.index:\n",
        "            if df.at[i,columnname]==key:\n",
        "                df.at[i,keys[key]]=1.0\n",
        "\n",
        "\n",
        "def create_dict(idx,list):\n",
        "    o={}\n",
        "    for i in list:\n",
        "        o[i]=idx+\"_\"+str(i)\n",
        "    return o\n",
        "\n",
        "def create_base_data(df):\n",
        "    t=['n', 't', 'o']\n",
        "    columnname=\"land_surface_condition\"\n",
        "    key=create_dict(columnname,t)\n",
        "\n",
        "\n",
        "    #checkvalues(df,columnname,key)\n",
        "    createcolumn(df,columnname,key)\n",
        "\n",
        "    t= ['h', 'w', 'i', 'r', 'u']\n",
        "    columnname=\"foundation_type\"\n",
        "    key=create_dict(columnname,t)\n",
        "\n",
        "    #checkvalues(df,columnname,key)\n",
        "    createcolumn(df,columnname,key)\n",
        "\n",
        "    t=  ['q', 'n', 'x']\n",
        "    columnname=\"roof_type\"\n",
        "    key=create_dict(columnname,t)\n",
        "\n",
        "    #checkvalues(df,columnname,key)\n",
        "    createcolumn(df,columnname,key)\n",
        "\n",
        "    t=  ['z', 'v', 'f', 'm', 'x']\n",
        "    columnname=\"ground_floor_type\"\n",
        "    key=create_dict(columnname,t)\n",
        "\n",
        "    #checkvalues(df,columnname,key)\n",
        "    createcolumn(df,columnname,key)\n",
        "\n",
        "    t=   ['q', 's', 'j', 'x']\n",
        "    columnname=\"other_floor_type\"\n",
        "    key=create_dict(columnname,t)\n",
        "\n",
        "    #checkvalues(df,columnname,key)\n",
        "    createcolumn(df,columnname,key)\n",
        "\n",
        "    t=   ['j', 's', 't', 'o']\n",
        "    columnname=\"position\"\n",
        "    key=create_dict(columnname,t)\n",
        "\n",
        "    #checkvalues(df,columnname,key)\n",
        "    createcolumn(df,columnname,key)\n",
        "\n",
        "    t=   ['c', 's', 'f', 'd', 'm', 'a', 'q', 'u', 'n', 'o']\n",
        "    columnname=\"plan_configuration\"\n",
        "    key=create_dict(columnname,t)\n",
        "\n",
        "    #checkvalues(df,columnname,key)\n",
        "    createcolumn(df,columnname,key)\n",
        "\n",
        "    t=['a', 'w', 'r', 'v']\n",
        "    columnname=\"legal_ownership_status\"\n",
        "    key=create_dict(columnname,t)\n",
        "\n",
        "    #checkvalues(df,columnname,key)\n",
        "    createcolumn(df,columnname,key)\n",
        "\n",
        "    # level 1: 0-30, level 2: 0-1427, level 3: 0-12567.\n",
        "    # level1:0--30\n",
        "    # level2: 0.0000-----------0.9999\n",
        "    # level3: 0.000000000------0.000099999\n",
        "    l1=df.geo_level_1_id\n",
        "    l2=(df.geo_level_2_id/1427*9999)/10000\n",
        "    l3=(df.geo_level_2_id/12567*99999)/1000000000\n",
        "    df[\"geopos\"]=l1+l2+l3\n",
        "    return(df)\n",
        "\n",
        "\n",
        "\n",
        "def kill_columns(df):\n",
        "    notkey=[\"Unnamed: 0\",\"building_id\",\"legal_ownership_status\",\"geo_level_1_id\",\t\"geo_level_2_id\",\t\"geo_level_3_id\", \"land_surface_condition\",\t\"foundation_type\",\t\"roof_type\",\t\"ground_floor_type\",\t\"other_floor_type\",\t\"position\",\t\"plan_configuration\"]\n",
        "    for i in df.columns:\n",
        "        #print(i)\n",
        "        if i in notkey:\n",
        "            df.drop(columns=[i], inplace=True)\n",
        "    return df\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {},
      "outputs": [],
      "source": [
        "def check_similarity_np(np1,np2):\n",
        "    db=0\n",
        "    maxi=0\n",
        "    for ind, i1 in enumerate(np1):\n",
        "        maxi+=1\n",
        "        i2=np2[ind]\n",
        "        if i1!=i2:\n",
        "            db+=1\n",
        "    #print(f\"{ind}. eset:  {i:3},{i2:3}\")\n",
        "    print(f\"hiba:{db} max:{maxi} -- error:{db/maxi*100.0 : 2.6} good %:{100-db/maxi*100.0 : 2.6} %\")\n",
        "    return(1-db/maxi)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 502
        },
        "id": "HZHYUIgKIlT9",
        "outputId": "3740844a-f48c-4b06-87fd-f1eddb0431ab"
      },
      "source": [
        "\n",
        "features_train=basedir+\"/orig/train_values.csv\"\n",
        "labels_train=basedir+\"/orig/train_labels.csv\"\n",
        "features_predict=basedir+\"/orig/test_values.csv\"\n",
        "X_train=pd.read_csv(features_train)\n",
        "X_pred=pd.read_csv(features_predict)\n",
        "y_train=pd.read_csv(labels_train)\n",
        "\n"
      ],
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['building_id', 'geo_level_1_id', 'geo_level_2_id', 'geo_level_3_id',\n",
              "       'count_floors_pre_eq', 'age', 'area_percentage', 'height_percentage',\n",
              "       'land_surface_condition', 'foundation_type', 'roof_type',\n",
              "       'ground_floor_type', 'other_floor_type', 'position',\n",
              "       'plan_configuration', 'has_superstructure_adobe_mud',\n",
              "       'has_superstructure_mud_mortar_stone', 'has_superstructure_stone_flag',\n",
              "       'has_superstructure_cement_mortar_stone',\n",
              "       'has_superstructure_mud_mortar_brick',\n",
              "       'has_superstructure_cement_mortar_brick', 'has_superstructure_timber',\n",
              "       'has_superstructure_bamboo', 'has_superstructure_rc_non_engineered',\n",
              "       'has_superstructure_rc_engineered', 'has_superstructure_other',\n",
              "       'legal_ownership_status', 'count_families', 'has_secondary_use',\n",
              "       'has_secondary_use_agriculture', 'has_secondary_use_hotel',\n",
              "       'has_secondary_use_rental', 'has_secondary_use_institution',\n",
              "       'has_secondary_use_school', 'has_secondary_use_industry',\n",
              "       'has_secondary_use_health_post', 'has_secondary_use_gov_office',\n",
              "       'has_secondary_use_use_police', 'has_secondary_use_other'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 123
        }
      ],
      "source": [
        "X_train.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {},
      "outputs": [],
      "source": [
        "added=pd.concat([X_train.geo_level_1_id,X_train.geo_level_2_id,X_train.geo_level_3_id,y_train.damage_grade], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {},
      "outputs": [],
      "source": [
        "#added"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<Figure size 432x288 with 1 Axes>",
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<!-- Created with matplotlib (https://matplotlib.org/) -->\r\n<svg height=\"248.518125pt\" version=\"1.1\" viewBox=\"0 0 388.0125 248.518125\" width=\"388.0125pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <metadata>\r\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\r\n   <cc:Work>\r\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\r\n    <dc:date>2020-12-20T00:17:26.508830</dc:date>\r\n    <dc:format>image/svg+xml</dc:format>\r\n    <dc:creator>\r\n     <cc:Agent>\r\n      <dc:title>Matplotlib v3.3.3, https://matplotlib.org/</dc:title>\r\n     </cc:Agent>\r\n    </dc:creator>\r\n   </cc:Work>\r\n  </rdf:RDF>\r\n </metadata>\r\n <defs>\r\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M 0 248.518125 \r\nL 388.0125 248.518125 \r\nL 388.0125 0 \r\nL 0 0 \r\nz\r\n\" style=\"fill:none;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g id=\"patch_2\">\r\n    <path d=\"M 46.0125 224.64 \r\nL 380.8125 224.64 \r\nL 380.8125 7.2 \r\nL 46.0125 7.2 \r\nz\r\n\" style=\"fill:#ffffff;\"/>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path clip-path=\"url(#peaccb2b5a3)\" d=\"M 61.230682 224.64 \r\nL 71.048864 224.64 \r\nL 71.048864 217.598028 \r\nL 61.230682 217.598028 \r\nz\r\n\" style=\"fill:#1f77b4;\"/>\r\n   </g>\r\n   <g id=\"patch_4\">\r\n    <path clip-path=\"url(#peaccb2b5a3)\" d=\"M 71.048864 224.64 \r\nL 80.867045 224.64 \r\nL 80.867045 221.054355 \r\nL 71.048864 221.054355 \r\nz\r\n\" style=\"fill:#1f77b4;\"/>\r\n   </g>\r\n   <g id=\"patch_5\">\r\n    <path clip-path=\"url(#peaccb2b5a3)\" d=\"M 80.867045 224.64 \r\nL 90.685227 224.64 \r\nL 90.685227 221.865533 \r\nL 80.867045 221.865533 \r\nz\r\n\" style=\"fill:#1f77b4;\"/>\r\n   </g>\r\n   <g id=\"patch_6\">\r\n    <path clip-path=\"url(#peaccb2b5a3)\" d=\"M 90.685227 224.64 \r\nL 100.503409 224.64 \r\nL 100.503409 192.369192 \r\nL 90.685227 192.369192 \r\nz\r\n\" style=\"fill:#1f77b4;\"/>\r\n   </g>\r\n   <g id=\"patch_7\">\r\n    <path clip-path=\"url(#peaccb2b5a3)\" d=\"M 100.503409 224.64 \r\nL 110.321591 224.64 \r\nL 110.321591 190.746834 \r\nL 100.503409 190.746834 \r\nz\r\n\" style=\"fill:#1f77b4;\"/>\r\n   </g>\r\n   <g id=\"patch_8\">\r\n    <path clip-path=\"url(#peaccb2b5a3)\" d=\"M 110.321591 224.64 \r\nL 120.139773 224.64 \r\nL 120.139773 221.936071 \r\nL 110.321591 221.936071 \r\nz\r\n\" style=\"fill:#1f77b4;\"/>\r\n   </g>\r\n   <g id=\"patch_9\">\r\n    <path clip-path=\"url(#peaccb2b5a3)\" d=\"M 120.139773 224.64 \r\nL 129.957955 224.64 \r\nL 129.957955 153.503147 \r\nL 120.139773 153.503147 \r\nz\r\n\" style=\"fill:#1f77b4;\"/>\r\n   </g>\r\n   <g id=\"patch_10\">\r\n    <path clip-path=\"url(#peaccb2b5a3)\" d=\"M 129.957955 224.64 \r\nL 139.776136 224.64 \r\nL 139.776136 146.014439 \r\nL 129.957955 146.014439 \r\nz\r\n\" style=\"fill:#1f77b4;\"/>\r\n   </g>\r\n   <g id=\"patch_11\">\r\n    <path clip-path=\"url(#peaccb2b5a3)\" d=\"M 139.776136 224.64 \r\nL 149.594318 224.64 \r\nL 149.594318 108.100648 \r\nL 139.776136 108.100648 \r\nz\r\n\" style=\"fill:#1f77b4;\"/>\r\n   </g>\r\n   <g id=\"patch_12\">\r\n    <path clip-path=\"url(#peaccb2b5a3)\" d=\"M 149.594318 224.64 \r\nL 159.4125 224.64 \r\nL 159.4125 216.833874 \r\nL 149.594318 216.833874 \r\nz\r\n\" style=\"fill:#1f77b4;\"/>\r\n   </g>\r\n   <g id=\"patch_13\">\r\n    <path clip-path=\"url(#peaccb2b5a3)\" d=\"M 159.4125 224.64 \r\nL 169.230682 224.64 \r\nL 169.230682 121.643807 \r\nL 159.4125 121.643807 \r\nz\r\n\" style=\"fill:#1f77b4;\"/>\r\n   </g>\r\n   <g id=\"patch_14\">\r\n    <path clip-path=\"url(#peaccb2b5a3)\" d=\"M 169.230682 224.64 \r\nL 179.048864 224.64 \r\nL 179.048864 187.46685 \r\nL 169.230682 187.46685 \r\nz\r\n\" style=\"fill:#1f77b4;\"/>\r\n   </g>\r\n   <g id=\"patch_15\">\r\n    <path clip-path=\"url(#peaccb2b5a3)\" d=\"M 179.048864 224.64 \r\nL 188.867045 224.64 \r\nL 188.867045 216.586993 \r\nL 179.048864 216.586993 \r\nz\r\n\" style=\"fill:#1f77b4;\"/>\r\n   </g>\r\n   <g id=\"patch_16\">\r\n    <path clip-path=\"url(#peaccb2b5a3)\" d=\"M 188.867045 224.64 \r\nL 198.685227 224.64 \r\nL 198.685227 208.569255 \r\nL 188.867045 208.569255 \r\nz\r\n\" style=\"fill:#1f77b4;\"/>\r\n   </g>\r\n   <g id=\"patch_17\">\r\n    <path clip-path=\"url(#peaccb2b5a3)\" d=\"M 198.685227 224.64 \r\nL 208.503409 224.64 \r\nL 208.503409 221.395285 \r\nL 198.685227 221.395285 \r\nz\r\n\" style=\"fill:#1f77b4;\"/>\r\n   </g>\r\n   <g id=\"patch_18\">\r\n    <path clip-path=\"url(#peaccb2b5a3)\" d=\"M 208.503409 224.64 \r\nL 218.321591 224.64 \r\nL 218.321591 218.949992 \r\nL 208.503409 218.949992 \r\nz\r\n\" style=\"fill:#1f77b4;\"/>\r\n   </g>\r\n   <g id=\"patch_19\">\r\n    <path clip-path=\"url(#peaccb2b5a3)\" d=\"M 218.321591 224.64 \r\nL 228.139773 224.64 \r\nL 228.139773 213.542134 \r\nL 218.321591 213.542134 \r\nz\r\n\" style=\"fill:#1f77b4;\"/>\r\n   </g>\r\n   <g id=\"patch_20\">\r\n    <path clip-path=\"url(#peaccb2b5a3)\" d=\"M 228.139773 224.64 \r\nL 237.957955 224.64 \r\nL 237.957955 17.554286 \r\nL 228.139773 17.554286 \r\nz\r\n\" style=\"fill:#1f77b4;\"/>\r\n   </g>\r\n   <g id=\"patch_21\">\r\n    <path clip-path=\"url(#peaccb2b5a3)\" d=\"M 237.957955 224.64 \r\nL 247.776136 224.64 \r\nL 247.776136 197.236265 \r\nL 237.957955 197.236265 \r\nz\r\n\" style=\"fill:#1f77b4;\"/>\r\n   </g>\r\n   <g id=\"patch_22\">\r\n    <path clip-path=\"url(#peaccb2b5a3)\" d=\"M 247.776136 224.64 \r\nL 257.594318 224.64 \r\nL 257.594318 223.86409 \r\nL 247.776136 223.86409 \r\nz\r\n\" style=\"fill:#1f77b4;\"/>\r\n   </g>\r\n   <g id=\"patch_23\">\r\n    <path clip-path=\"url(#peaccb2b5a3)\" d=\"M 257.594318 224.64 \r\nL 267.4125 224.64 \r\nL 267.4125 200.598542 \r\nL 257.594318 200.598542 \r\nz\r\n\" style=\"fill:#1f77b4;\"/>\r\n   </g>\r\n   <g id=\"patch_24\">\r\n    <path clip-path=\"url(#peaccb2b5a3)\" d=\"M 267.4125 224.64 \r\nL 277.230682 224.64 \r\nL 277.230682 122.243374 \r\nL 267.4125 122.243374 \r\nz\r\n\" style=\"fill:#1f77b4;\"/>\r\n   </g>\r\n   <g id=\"patch_25\">\r\n    <path clip-path=\"url(#peaccb2b5a3)\" d=\"M 277.230682 224.64 \r\nL 287.048864 224.64 \r\nL 287.048864 215.035173 \r\nL 277.230682 215.035173 \r\nz\r\n\" style=\"fill:#1f77b4;\"/>\r\n   </g>\r\n   <g id=\"patch_26\">\r\n    <path clip-path=\"url(#peaccb2b5a3)\" d=\"M 287.048864 224.64 \r\nL 296.867045 224.64 \r\nL 296.867045 221.336504 \r\nL 287.048864 221.336504 \r\nz\r\n\" style=\"fill:#1f77b4;\"/>\r\n   </g>\r\n   <g id=\"patch_27\">\r\n    <path clip-path=\"url(#peaccb2b5a3)\" d=\"M 296.867045 224.64 \r\nL 306.685227 224.64 \r\nL 306.685227 223.08818 \r\nL 296.867045 223.08818 \r\nz\r\n\" style=\"fill:#1f77b4;\"/>\r\n   </g>\r\n   <g id=\"patch_28\">\r\n    <path clip-path=\"url(#peaccb2b5a3)\" d=\"M 306.685227 224.64 \r\nL 316.503409 224.64 \r\nL 316.503409 215.564203 \r\nL 306.685227 215.564203 \r\nz\r\n\" style=\"fill:#1f77b4;\"/>\r\n   </g>\r\n   <g id=\"patch_29\">\r\n    <path clip-path=\"url(#peaccb2b5a3)\" d=\"M 316.503409 224.64 \r\nL 326.321591 224.64 \r\nL 326.321591 201.809432 \r\nL 316.503409 201.809432 \r\nz\r\n\" style=\"fill:#1f77b4;\"/>\r\n   </g>\r\n   <g id=\"patch_30\">\r\n    <path clip-path=\"url(#peaccb2b5a3)\" d=\"M 326.321591 224.64 \r\nL 336.139773 224.64 \r\nL 336.139773 153.397342 \r\nL 326.321591 153.397342 \r\nz\r\n\" style=\"fill:#1f77b4;\"/>\r\n   </g>\r\n   <g id=\"patch_31\">\r\n    <path clip-path=\"url(#peaccb2b5a3)\" d=\"M 336.139773 224.64 \r\nL 345.957955 224.64 \r\nL 345.957955 223.370329 \r\nL 336.139773 223.370329 \r\nz\r\n\" style=\"fill:#1f77b4;\"/>\r\n   </g>\r\n   <g id=\"patch_32\">\r\n    <path clip-path=\"url(#peaccb2b5a3)\" d=\"M 345.957955 224.64 \r\nL 355.776136 224.64 \r\nL 355.776136 224.181508 \r\nL 345.957955 224.181508 \r\nz\r\n\" style=\"fill:#1f77b4;\"/>\r\n   </g>\r\n   <g id=\"patch_33\">\r\n    <path clip-path=\"url(#peaccb2b5a3)\" d=\"M 355.776136 224.64 \r\nL 365.594318 224.64 \r\nL 365.594318 221.030842 \r\nL 355.776136 221.030842 \r\nz\r\n\" style=\"fill:#1f77b4;\"/>\r\n   </g>\r\n   <g id=\"matplotlib.axis_1\">\r\n    <g id=\"xtick_1\">\r\n     <g id=\"line2d_1\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 0 3.5 \r\n\" id=\"m90301eb827\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"61.230682\" xlink:href=\"#m90301eb827\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_1\">\r\n      <!-- 0 -->\r\n      <g transform=\"translate(58.049432 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 31.78125 66.40625 \r\nQ 24.171875 66.40625 20.328125 58.90625 \r\nQ 16.5 51.421875 16.5 36.375 \r\nQ 16.5 21.390625 20.328125 13.890625 \r\nQ 24.171875 6.390625 31.78125 6.390625 \r\nQ 39.453125 6.390625 43.28125 13.890625 \r\nQ 47.125 21.390625 47.125 36.375 \r\nQ 47.125 51.421875 43.28125 58.90625 \r\nQ 39.453125 66.40625 31.78125 66.40625 \r\nz\r\nM 31.78125 74.21875 \r\nQ 44.046875 74.21875 50.515625 64.515625 \r\nQ 56.984375 54.828125 56.984375 36.375 \r\nQ 56.984375 17.96875 50.515625 8.265625 \r\nQ 44.046875 -1.421875 31.78125 -1.421875 \r\nQ 19.53125 -1.421875 13.0625 8.265625 \r\nQ 6.59375 17.96875 6.59375 36.375 \r\nQ 6.59375 54.828125 13.0625 64.515625 \r\nQ 19.53125 74.21875 31.78125 74.21875 \r\nz\r\n\" id=\"DejaVuSans-48\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_2\">\r\n     <g id=\"line2d_2\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"111.957955\" xlink:href=\"#m90301eb827\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_2\">\r\n      <!-- 5 -->\r\n      <g transform=\"translate(108.776705 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 10.796875 72.90625 \r\nL 49.515625 72.90625 \r\nL 49.515625 64.59375 \r\nL 19.828125 64.59375 \r\nL 19.828125 46.734375 \r\nQ 21.96875 47.46875 24.109375 47.828125 \r\nQ 26.265625 48.1875 28.421875 48.1875 \r\nQ 40.625 48.1875 47.75 41.5 \r\nQ 54.890625 34.8125 54.890625 23.390625 \r\nQ 54.890625 11.625 47.5625 5.09375 \r\nQ 40.234375 -1.421875 26.90625 -1.421875 \r\nQ 22.3125 -1.421875 17.546875 -0.640625 \r\nQ 12.796875 0.140625 7.71875 1.703125 \r\nL 7.71875 11.625 \r\nQ 12.109375 9.234375 16.796875 8.0625 \r\nQ 21.484375 6.890625 26.703125 6.890625 \r\nQ 35.15625 6.890625 40.078125 11.328125 \r\nQ 45.015625 15.765625 45.015625 23.390625 \r\nQ 45.015625 31 40.078125 35.4375 \r\nQ 35.15625 39.890625 26.703125 39.890625 \r\nQ 22.75 39.890625 18.8125 39.015625 \r\nQ 14.890625 38.140625 10.796875 36.28125 \r\nz\r\n\" id=\"DejaVuSans-53\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_3\">\r\n     <g id=\"line2d_3\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"162.685227\" xlink:href=\"#m90301eb827\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_3\">\r\n      <!-- 10 -->\r\n      <g transform=\"translate(156.322727 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 12.40625 8.296875 \r\nL 28.515625 8.296875 \r\nL 28.515625 63.921875 \r\nL 10.984375 60.40625 \r\nL 10.984375 69.390625 \r\nL 28.421875 72.90625 \r\nL 38.28125 72.90625 \r\nL 38.28125 8.296875 \r\nL 54.390625 8.296875 \r\nL 54.390625 0 \r\nL 12.40625 0 \r\nz\r\n\" id=\"DejaVuSans-49\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_4\">\r\n     <g id=\"line2d_4\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"213.4125\" xlink:href=\"#m90301eb827\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_4\">\r\n      <!-- 15 -->\r\n      <g transform=\"translate(207.05 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_5\">\r\n     <g id=\"line2d_5\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"264.139773\" xlink:href=\"#m90301eb827\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_5\">\r\n      <!-- 20 -->\r\n      <g transform=\"translate(257.777273 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 19.1875 8.296875 \r\nL 53.609375 8.296875 \r\nL 53.609375 0 \r\nL 7.328125 0 \r\nL 7.328125 8.296875 \r\nQ 12.9375 14.109375 22.625 23.890625 \r\nQ 32.328125 33.6875 34.8125 36.53125 \r\nQ 39.546875 41.84375 41.421875 45.53125 \r\nQ 43.3125 49.21875 43.3125 52.78125 \r\nQ 43.3125 58.59375 39.234375 62.25 \r\nQ 35.15625 65.921875 28.609375 65.921875 \r\nQ 23.96875 65.921875 18.8125 64.3125 \r\nQ 13.671875 62.703125 7.8125 59.421875 \r\nL 7.8125 69.390625 \r\nQ 13.765625 71.78125 18.9375 73 \r\nQ 24.125 74.21875 28.421875 74.21875 \r\nQ 39.75 74.21875 46.484375 68.546875 \r\nQ 53.21875 62.890625 53.21875 53.421875 \r\nQ 53.21875 48.921875 51.53125 44.890625 \r\nQ 49.859375 40.875 45.40625 35.40625 \r\nQ 44.1875 33.984375 37.640625 27.21875 \r\nQ 31.109375 20.453125 19.1875 8.296875 \r\nz\r\n\" id=\"DejaVuSans-50\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_6\">\r\n     <g id=\"line2d_6\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"314.867045\" xlink:href=\"#m90301eb827\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_6\">\r\n      <!-- 25 -->\r\n      <g transform=\"translate(308.504545 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_7\">\r\n     <g id=\"line2d_7\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"365.594318\" xlink:href=\"#m90301eb827\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_7\">\r\n      <!-- 30 -->\r\n      <g transform=\"translate(359.231818 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 40.578125 39.3125 \r\nQ 47.65625 37.796875 51.625 33 \r\nQ 55.609375 28.21875 55.609375 21.1875 \r\nQ 55.609375 10.40625 48.1875 4.484375 \r\nQ 40.765625 -1.421875 27.09375 -1.421875 \r\nQ 22.515625 -1.421875 17.65625 -0.515625 \r\nQ 12.796875 0.390625 7.625 2.203125 \r\nL 7.625 11.71875 \r\nQ 11.71875 9.328125 16.59375 8.109375 \r\nQ 21.484375 6.890625 26.8125 6.890625 \r\nQ 36.078125 6.890625 40.9375 10.546875 \r\nQ 45.796875 14.203125 45.796875 21.1875 \r\nQ 45.796875 27.640625 41.28125 31.265625 \r\nQ 36.765625 34.90625 28.71875 34.90625 \r\nL 20.21875 34.90625 \r\nL 20.21875 43.015625 \r\nL 29.109375 43.015625 \r\nQ 36.375 43.015625 40.234375 45.921875 \r\nQ 44.09375 48.828125 44.09375 54.296875 \r\nQ 44.09375 59.90625 40.109375 62.90625 \r\nQ 36.140625 65.921875 28.71875 65.921875 \r\nQ 24.65625 65.921875 20.015625 65.03125 \r\nQ 15.375 64.15625 9.8125 62.3125 \r\nL 9.8125 71.09375 \r\nQ 15.4375 72.65625 20.34375 73.4375 \r\nQ 25.25 74.21875 29.59375 74.21875 \r\nQ 40.828125 74.21875 47.359375 69.109375 \r\nQ 53.90625 64.015625 53.90625 55.328125 \r\nQ 53.90625 49.265625 50.4375 45.09375 \r\nQ 46.96875 40.921875 40.578125 39.3125 \r\nz\r\n\" id=\"DejaVuSans-51\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-51\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_2\">\r\n    <g id=\"ytick_1\">\r\n     <g id=\"line2d_8\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL -3.5 0 \r\n\" id=\"me487a278da\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"46.0125\" xlink:href=\"#me487a278da\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_8\">\r\n      <!-- 0 -->\r\n      <g transform=\"translate(32.65 228.439219)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_2\">\r\n     <g id=\"line2d_9\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"46.0125\" xlink:href=\"#me487a278da\" y=\"195.249464\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_9\">\r\n      <!-- 2500 -->\r\n      <g transform=\"translate(13.5625 199.048683)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_3\">\r\n     <g id=\"line2d_10\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"46.0125\" xlink:href=\"#me487a278da\" y=\"165.858929\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_10\">\r\n      <!-- 5000 -->\r\n      <g transform=\"translate(13.5625 169.658147)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-53\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_4\">\r\n     <g id=\"line2d_11\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"46.0125\" xlink:href=\"#me487a278da\" y=\"136.468393\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_11\">\r\n      <!-- 7500 -->\r\n      <g transform=\"translate(13.5625 140.267612)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 8.203125 72.90625 \r\nL 55.078125 72.90625 \r\nL 55.078125 68.703125 \r\nL 28.609375 0 \r\nL 18.3125 0 \r\nL 43.21875 64.59375 \r\nL 8.203125 64.59375 \r\nz\r\n\" id=\"DejaVuSans-55\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-55\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_5\">\r\n     <g id=\"line2d_12\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"46.0125\" xlink:href=\"#me487a278da\" y=\"107.077857\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_12\">\r\n      <!-- 10000 -->\r\n      <g transform=\"translate(7.2 110.877076)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_6\">\r\n     <g id=\"line2d_13\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"46.0125\" xlink:href=\"#me487a278da\" y=\"77.687322\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_13\">\r\n      <!-- 12500 -->\r\n      <g transform=\"translate(7.2 81.48654)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-53\"/>\r\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_7\">\r\n     <g id=\"line2d_14\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"46.0125\" xlink:href=\"#me487a278da\" y=\"48.296786\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_14\">\r\n      <!-- 15000 -->\r\n      <g transform=\"translate(7.2 52.096005)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_8\">\r\n     <g id=\"line2d_15\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"46.0125\" xlink:href=\"#me487a278da\" y=\"18.90625\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_15\">\r\n      <!-- 17500 -->\r\n      <g transform=\"translate(7.2 22.705469)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-55\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-53\"/>\r\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"patch_34\">\r\n    <path d=\"M 46.0125 224.64 \r\nL 46.0125 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_35\">\r\n    <path d=\"M 380.8125 224.64 \r\nL 380.8125 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_36\">\r\n    <path d=\"M 46.0125 224.64 \r\nL 380.8125 224.64 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_37\">\r\n    <path d=\"M 46.0125 7.2 \r\nL 380.8125 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"peaccb2b5a3\">\r\n   <rect height=\"217.44\" width=\"334.8\" x=\"46.0125\" y=\"7.2\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "a=plt.hist(added.geo_level_1_id.where(added.damage_grade==3),bins=31)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "31"
            ]
          },
          "metadata": {},
          "execution_count": 127
        }
      ],
      "source": [
        "len(a[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {},
      "outputs": [],
      "source": [
        "quake_effect={}\n",
        "maxi=max(a[0])\n",
        "for inx,val in enumerate(a[0]):\n",
        "    quake_effect[inx]=1-(maxi-val)/maxi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {},
      "outputs": [],
      "source": [
        "eartquakeDist=[]\n",
        "for idx,i in enumerate(X_train.geo_level_1_id):\n",
        "    eartquakeDist.append(quake_effect[X_train.geo_level_1_id[idx]])\n",
        "\n",
        "eartquakeDist2=[]\n",
        "for idx,i in enumerate(X_pred.geo_level_1_id):\n",
        "    eartquakeDist2.append(quake_effect[X_pred.geo_level_1_id[idx]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 130
        }
      ],
      "source": [
        "eartquakeDist2[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {},
      "outputs": [],
      "source": [
        "added2=pd.concat([X_train.geo_level_1_id,X_train.geo_level_2_id,X_train.geo_level_3_id,y_train.damage_grade,pd.DataFrame(eartquakeDist,columns=[\"QuakeForce\"])], axis=1)\n",
        "\n",
        "X_train_geolevel=pd.concat([X_train,pd.DataFrame(eartquakeDist,columns=[\"QuakeForce\"])], axis=1)\n",
        "X_pred_geolevel=pd.concat([X_pred,pd.DataFrame(eartquakeDist2,columns=[\"QuakeForce\"])], axis=1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "land_surface_condition cseréje megy\n",
            "foundation_type cseréje megy\n",
            "roof_type cseréje megy\n",
            "ground_floor_type cseréje megy\n",
            "other_floor_type cseréje megy\n",
            "position cseréje megy\n",
            "plan_configuration cseréje megy\n",
            "legal_ownership_status cseréje megy\n",
            "land_surface_condition cseréje megy\n",
            "foundation_type cseréje megy\n",
            "roof_type cseréje megy\n",
            "ground_floor_type cseréje megy\n",
            "other_floor_type cseréje megy\n",
            "position cseréje megy\n",
            "plan_configuration cseréje megy\n",
            "legal_ownership_status cseréje megy\n"
          ]
        }
      ],
      "source": [
        "X_pred_conv=create_base_data(X_pred_geolevel)\n",
        "X_train_conv=create_base_data(X_train_geolevel)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {},
      "outputs": [],
      "source": [
        "#--------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   count_floors_pre_eq  age  area_percentage  height_percentage  \\\n",
              "0                    3   20                7                  6   \n",
              "1                    2   25               13                  5   \n",
              "2                    2    5                4                  5   \n",
              "3                    1    0               19                  3   \n",
              "4                    3   15                8                  7   \n",
              "\n",
              "   has_superstructure_adobe_mud  has_superstructure_mud_mortar_stone  \\\n",
              "0                             0                                    1   \n",
              "1                             0                                    1   \n",
              "2                             0                                    1   \n",
              "3                             0                                    0   \n",
              "4                             0                                    1   \n",
              "\n",
              "   has_superstructure_stone_flag  has_superstructure_cement_mortar_stone  \\\n",
              "0                              0                                       0   \n",
              "1                              0                                       0   \n",
              "2                              0                                       0   \n",
              "3                              0                                       0   \n",
              "4                              0                                       0   \n",
              "\n",
              "   has_superstructure_mud_mortar_brick  \\\n",
              "0                                    0   \n",
              "1                                    0   \n",
              "2                                    0   \n",
              "3                                    0   \n",
              "4                                    0   \n",
              "\n",
              "   has_superstructure_cement_mortar_brick  ...  plan_configuration_a  \\\n",
              "0                                       0  ...                   0.0   \n",
              "1                                       0  ...                   0.0   \n",
              "2                                       0  ...                   0.0   \n",
              "3                                       1  ...                   0.0   \n",
              "4                                       0  ...                   0.0   \n",
              "\n",
              "   plan_configuration_q  plan_configuration_u  plan_configuration_n  \\\n",
              "0                   0.0                   0.0                   0.0   \n",
              "1                   0.0                   0.0                   0.0   \n",
              "2                   0.0                   0.0                   0.0   \n",
              "3                   0.0                   0.0                   0.0   \n",
              "4                   0.0                   0.0                   0.0   \n",
              "\n",
              "   plan_configuration_o  legal_ownership_status_a  legal_ownership_status_w  \\\n",
              "0                   0.0                       0.0                       0.0   \n",
              "1                   0.0                       0.0                       0.0   \n",
              "2                   0.0                       0.0                       0.0   \n",
              "3                   0.0                       0.0                       0.0   \n",
              "4                   0.0                       0.0                       0.0   \n",
              "\n",
              "   legal_ownership_status_r  legal_ownership_status_v     geopos  \n",
              "0                       0.0                       1.0  17.417622  \n",
              "1                       0.0                       1.0   6.098800  \n",
              "2                       0.0                       1.0  22.013313  \n",
              "3                       0.0                       1.0  26.027328  \n",
              "4                       0.0                       1.0  17.202505  \n",
              "\n",
              "[5 rows x 67 columns]"
            ],
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>count_floors_pre_eq</th>\n      <th>age</th>\n      <th>area_percentage</th>\n      <th>height_percentage</th>\n      <th>has_superstructure_adobe_mud</th>\n      <th>has_superstructure_mud_mortar_stone</th>\n      <th>has_superstructure_stone_flag</th>\n      <th>has_superstructure_cement_mortar_stone</th>\n      <th>has_superstructure_mud_mortar_brick</th>\n      <th>has_superstructure_cement_mortar_brick</th>\n      <th>...</th>\n      <th>plan_configuration_a</th>\n      <th>plan_configuration_q</th>\n      <th>plan_configuration_u</th>\n      <th>plan_configuration_n</th>\n      <th>plan_configuration_o</th>\n      <th>legal_ownership_status_a</th>\n      <th>legal_ownership_status_w</th>\n      <th>legal_ownership_status_r</th>\n      <th>legal_ownership_status_v</th>\n      <th>geopos</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>3</td>\n      <td>20</td>\n      <td>7</td>\n      <td>6</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>17.417622</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>25</td>\n      <td>13</td>\n      <td>5</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>6.098800</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>5</td>\n      <td>4</td>\n      <td>5</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>22.013313</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>0</td>\n      <td>19</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>26.027328</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>3</td>\n      <td>15</td>\n      <td>8</td>\n      <td>7</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>17.202505</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 67 columns</p>\n</div>"
          },
          "metadata": {},
          "execution_count": 134
        }
      ],
      "source": [
        "#\n",
        "X_train_ok=kill_columns(X_train_conv)\n",
        "X_pred_ok=kill_columns(X_pred_conv)\n",
        "y_train_ok=kill_columns(y_train)\n",
        "\n",
        "\n",
        "\n",
        "X_train_ok.to_csv(basedir+\"/tmp/X_tran_ok.csv\",index=False)\n",
        "X_pred_ok.to_csv(basedir+\"/tmp/X_pred_ok.csv\",index=False)\n",
        "y_train_ok.to_csv(basedir+\"/tmp/y_train_ok.csv\",index=False)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#f=open(basedir+\"/tmp/similarity.csv\",\"a\")\n",
        "X_train_ok=pd.read_csv(basedir+\"/tmp/X_tran_ok.csv\",)\n",
        "X_pred_ok=pd.read_csv(basedir+\"/tmp/X_pred_ok.csv\")\n",
        "y_train_ok=pd.read_csv(basedir+\"/tmp/y_train_ok.csv\")\n",
        "\n",
        "X_train_ok.head()\n",
        "X_pred_ok.head()\n",
        "#print(\"Ready\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZkluH7RwmL_M"
      },
      "source": [
        "X_pred_ok=kill_columns(X_pred_ok)\n",
        "X_train_ok=kill_columns(X_train_ok)\n",
        "y_train_ok=kill_columns(y_train_ok)\n",
        "\n",
        "t=[1, 2, 3]\n",
        "columnname=\"damage_grade\"\n",
        "key=create_dict(columnname,t)\n",
        "\n",
        "\n",
        "checkvalues(y_train_ok,columnname,key)\n",
        "createcolumn(y_train_ok,columnname,key)"
      ],
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "damage_grade ellenőrzése !\n",
            "\n",
            "260601 mintából 0 db nem volt megfelelő\n",
            "damage_grade cseréje megy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        damage_grade  damage_grade_1  damage_grade_2  damage_grade_3\n",
              "0                  3             0.0             0.0             1.0\n",
              "1                  2             0.0             1.0             0.0\n",
              "2                  3             0.0             0.0             1.0\n",
              "3                  2             0.0             1.0             0.0\n",
              "4                  3             0.0             0.0             1.0\n",
              "...              ...             ...             ...             ...\n",
              "260596             2             0.0             1.0             0.0\n",
              "260597             3             0.0             0.0             1.0\n",
              "260598             3             0.0             0.0             1.0\n",
              "260599             2             0.0             1.0             0.0\n",
              "260600             3             0.0             0.0             1.0\n",
              "\n",
              "[260601 rows x 4 columns]"
            ],
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>damage_grade</th>\n      <th>damage_grade_1</th>\n      <th>damage_grade_2</th>\n      <th>damage_grade_3</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>3</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>3</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>260596</th>\n      <td>2</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>260597</th>\n      <td>3</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>260598</th>\n      <td>3</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>260599</th>\n      <td>2</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>260600</th>\n      <td>3</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>260601 rows × 4 columns</p>\n</div>"
          },
          "metadata": {},
          "execution_count": 136
        }
      ],
      "source": [
        "y_train_ok"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q0rn-7TBQfRp"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "\n",
        "scaler2=MinMaxScaler()\n",
        "X_train_scale=scaler2.fit_transform(X_train_ok)\n",
        "\n",
        "#scaler1=StandardScaler()\n",
        "X_pred_scale=scaler2.fit_transform(X_pred_ok)\n",
        "\"\"\"\n",
        "scaler3=StandardScaler()\n",
        "y_train_scale=scaler3.fit_transform(y_train_ok)\n",
        "\"\"\"\n",
        "y_train_np=y_train_ok.to_numpy()\n",
        "\n",
        "# szétszedjük a train és test részekre\n",
        "from sklearn.model_selection import train_test_split\n",
        "#X_train_train, X_train_test,y_train_train, y_train_test  = train_test_split( X_train_scale, y_train_scale, test_size=0.10, random_state=0)\n",
        "X_train_train, X_train_test,y_train_train, y_train_test  = train_test_split( X_train_scale, y_train_np, test_size=0.01, random_state=0)\n",
        "\n",
        "\n"
      ],
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {},
      "outputs": [],
      "source": [
        "def outlierDropfrom_df(df,inxlist):\n",
        "    a=df\n",
        "    out=a.drop(inxlist,axis=0)\n",
        "    return(out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {},
      "outputs": [],
      "source": [
        "def outlierDropfrom_numpyarray(na,inxlist):\n",
        "    \n",
        "    out=np.delete (na ,inxlist, axis=0)\n",
        "    return(out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calc_the_range(y_train,hist):    \n",
        "    num1=sum(1 for val in y_train if val==1)\n",
        "    #print(num1)\n",
        "    num2=sum(1 for val in y_train if val==2)\n",
        "    #print(num2)\n",
        "    num3=sum(1 for val in y_train if val==3)\n",
        "    #print(num3)\n",
        "    \n",
        "    out=(num1,num1+num2,num1+num2+num3)\n",
        "    print(\"out: \" ,out)\n",
        "    steps=[]\n",
        "    for i in range(len(hist[0])):\n",
        "        print(hist[0][i], end=\"\")\n",
        "        if hist[0][i]>out[0]:\n",
        "            steps.append(i)\n",
        "            break\n",
        "    for i in range(len(hist[0])):\n",
        "        #print(hist[0][i])\n",
        "        if hist[0][i]>out[1]:\n",
        "            steps.append(i)\n",
        "            break\n",
        "    print(\"step:\",steps)\n",
        "    limits_out=(hist[1][steps[0]],hist[1][steps[1]])\n",
        "    print(\"Limits:\",limits_out)\n",
        "    return (limits_out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {},
      "outputs": [],
      "source": [
        "def conv_a_floatlist(alist, range_x):\n",
        "    o=[]\n",
        "    print(\"conv:\" ,range_x)\n",
        "    for i in alist:\n",
        "        if i<range_x[0]:\n",
        "            o.append(1)\n",
        "        if range_x[0]<= i <range_x[1]:\n",
        "            o.append(2)\n",
        "        if range_x[1]<=i:\n",
        "            o.append(3)\n",
        "    return(o)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "\n",
        "# explicitly require this experimental feature\n",
        "from xgboost import XGBClassifier  \n",
        "from sklearn.experimental import enable_hist_gradient_boosting  # noqa\n",
        "# now you can import normally from ensemble\n",
        "from sklearn.ensemble import HistGradientBoostingClassifier\n",
        "from sklearn.ensemble import HistGradientBoostingRegressor\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "#clf = GradientBoostingClassifier(random_state=0,n_estimators=525, max_depth=43, verbose=3, tol=0.0001)\n",
        "#clf = HistGradientBoostingClassifier(random_state=1, max_depth=53, verbose=3, tol=0.000001, learning_rate=0.6, max_iter=24000,early_stopping=False,min_samples_leaf=5000, scoring=\"balanced_accuracy\")\n",
        "clf=HistGradientBoostingRegressor(random_state=1, max_depth=113, verbose=3, tol=0.00000001, learning_rate=0.1, max_iter=4000,early_stopping=True,min_samples_leaf=3, loss=\"poisson\",n_iter_no_change=80, warm_start=True)\n",
        "#clf.fit(X_train_train, y_train_train)\n",
        "\n",
        "#clf1= MLPClassifier(random_state=1, max_iter=1,verbose=True, tol=1e-10, hidden_layer_sizes=(60,), warm_start=True)\n",
        "#clf2= MLPClassifier(random_state=1, max_iter=1,verbose=True, tol=1e-10, hidden_layer_sizes=(100,20), warm_start=True)\n",
        "#clf3= MLPClassifier(random_state=1, max_iter=1,verbose=True, tol=1e-10, hidden_layer_sizes=(100,30), warm_start=True)\n",
        "\n",
        "'''\n",
        "clf1=GradientBoostingRegressor(random_state=1, max_depth=113, verbose=1, tol=0.00000001, learning_rate=0.1, n_estimators=20,min_samples_leaf=3, loss=\"quantile\",n_iter_no_change=80, warm_start=True,)\n",
        "clf2=GradientBoostingRegressor(random_state=1, max_depth=113, verbose=1, tol=0.00000001, learning_rate=0.1, n_estimators=2,min_samples_leaf=3, loss=\"quantile\",n_iter_no_change=80, warm_start=True)\n",
        "clf3=GradientBoostingRegressor(random_state=1, max_depth=113, verbose=1, tol=0.00000001, learning_rate=0.1, n_estimators=2,min_samples_leaf=3, loss=\"quantile\",n_iter_no_change=80, warm_start=True)\n",
        "'''\n",
        "clf1=GradientBoostingRegressor(verbose=3,warm_start=True,n_estimators=5)\n",
        "clf2=GradientBoostingRegressor(verbose=3,warm_start=True,n_estimators=5)\n",
        "clf3=GradientBoostingRegressor(verbose=3,warm_start=True,n_estimators=5)\n",
        "\n",
        "\n",
        "# clf1= GradientBoostingClassifier(verbose=3, tol=1e-10, warm_start=True,n_estimators=10)\n",
        "# clf2= GradientBoostingClassifier(verbose=3, tol=1e-10, warm_start=True,n_estimators=10)\n",
        "# clf3= GradientBoostingClassifier(verbose=3, tol=1e-10, warm_start=True,n_estimators=10)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "yhiba=[]\n",
        "for inx,i in enumerate(y_train_train):\n",
        "    yhiba.append(0)\n",
        "yhiba[0]=1\n"
      ]
    },
    {
      "source": [
        "n=20\n",
        "i=0\n",
        "yt0=y_train_train[:,0]\n",
        "yt1=y_train_train[:,1]\n",
        "yt2=y_train_train[:,2]\n",
        "yt3=y_train_train[:,3]\n",
        "clf1.fit(X_train_train,yt1)\n",
        "clf2.fit(X_train_train,yt2)\n",
        "clf3.fit(X_train_train,yt3)\n"
      ],
      "cell_type": "code",
      "metadata": {},
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Iter       Train Loss   Remaining Time \n",
            "         1           0.0833            3.47s\n",
            "         2           0.0802            2.57s\n",
            "         3           0.0776            1.69s\n",
            "         4           0.0755            0.84s\n",
            "         5           0.0736            0.00s\n",
            "      Iter       Train Loss   Remaining Time \n",
            "         1           0.2407            3.42s\n",
            "         2           0.2370            2.55s\n",
            "         3           0.2339            1.69s\n",
            "         4           0.2314            0.84s\n",
            "         5           0.2291            0.00s\n",
            "      Iter       Train Loss   Remaining Time \n",
            "         1           0.2137            3.32s\n",
            "         2           0.2064            2.49s\n",
            "         3           0.2005            1.66s\n",
            "         4           0.1956            0.83s\n",
            "         5           0.1915            0.00s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GradientBoostingRegressor(n_estimators=5, verbose=3, warm_start=True)"
            ]
          },
          "metadata": {},
          "execution_count": 144
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 0.00295379\n",
            "Iteration 2, loss = 0.00010094\n",
            "Iteration 3, loss = 0.00007857\n",
            "Iteration 4, loss = 0.00007376\n",
            "Iteration 5, loss = 0.00006352\n",
            "Iteration 6, loss = 0.00005093\n",
            "Iteration 7, loss = 0.00004983\n",
            "Iteration 8, loss = 0.00004696\n",
            "Iteration 9, loss = 0.00004742\n",
            "Iteration 10, loss = 0.00004567\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPClassifier(hidden_layer_sizes=(60, 500, 5), max_iter=10, random_state=1,\n",
              "              verbose=True, warm_start=True)"
            ]
          },
          "metadata": {},
          "execution_count": 145
        }
      ],
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "clfhiba3=MLPClassifier(random_state=1, max_iter=10, warm_start=True, verbose=True,hidden_layer_sizes=(60,500,5))\n",
        "clfhiba3.fit(X_train_train,yhiba)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ITT kezdődik az okítási ciklus IDe jön vissza"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 162,
      "metadata": {},
      "outputs": [],
      "source": [
        "def reconvert (X):\n",
        "    yo=[]\n",
        "    y1=clf1.predict(X)\n",
        "    y2=clf2.predict(X)\n",
        "    y3=clf3.predict(X)\n",
        "    yerror=clfhiba3.predict(X)\n",
        "    o=1\n",
        "    \n",
        "    for idx,_ in enumerate(X):\n",
        "       if y1[idx]==max(y1[idx],y2[idx],y3[idx]):\n",
        "           o=1\n",
        "       if y2[idx]==max(y1[idx],y2[idx],y3[idx]):\n",
        "           o=2\n",
        "       if y3[idx]==max(y1[idx],y2[idx],y3[idx]):\n",
        "           o=3\n",
        "       '''\n",
        "       ox=o\n",
        "       if ox==3 and yerror[idx]:\n",
        "           o=2\n",
        "       if ox==1 and yerror[idx]:\n",
        "           o=2\n",
        "       if ox==2 and yerror[idx]:\n",
        "           # 2 eset van \n",
        "           if y3[idx]>y1[idx]:\n",
        "               o=3\n",
        "           else:\n",
        "               o=1\n",
        "\n",
        "       '''\n",
        "       \n",
        "       yo.append(o)\n",
        "    return(yo)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 193,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "n_hiba:5505\n",
            "damage grade in     5515 nextsave=6005. step: 0.046946,0.170270,0.132384\n",
            "n_hiba:5515\n",
            "damage grade in     5525 nextsave=6005. step: 0.046943,0.170253,0.132375\n",
            "n_hiba:5525\n",
            "damage grade in     5535 nextsave=6005. step: 0.046935,0.170238,0.132365\n",
            "n_hiba:5535\n",
            "damage grade in     5545 nextsave=6005. step: 0.046929,0.170219,0.132353\n",
            "n_hiba:5545\n",
            "damage grade in     5555 nextsave=6005. step: 0.046921,0.170206,0.132343\n",
            "n_hiba:5555\n",
            "damage grade in     5565 nextsave=6005. step: 0.046913,0.170194,0.132334\n",
            "n_hiba:5565\n",
            "damage grade in     5575 nextsave=6005. step: 0.046906,0.170181,0.132324\n",
            "n_hiba:5575\n",
            "damage grade in     5585 nextsave=6005. step: 0.046898,0.170167,0.132314\n",
            "n_hiba:5585\n",
            "damage grade in     5595 nextsave=6005. step: 0.046888,0.170152,0.132306\n",
            "n_hiba:5595\n",
            "damage grade in     5605 nextsave=6005. step: 0.046882,0.170134,0.132299\n",
            "n_hiba:5605\n",
            "damage grade in     5615 nextsave=6005. step: 0.046878,0.170121,0.132291\n",
            "n_hiba:5615\n",
            "damage grade in     5625 nextsave=6005. step: 0.046872,0.170100,0.132280\n",
            "n_hiba:5625\n",
            "damage grade in     5635 nextsave=6005. step: 0.046869,0.170079,0.132269\n",
            "n_hiba:5635\n",
            "damage grade in     5645 nextsave=6005. step: 0.046865,0.170062,0.132256\n",
            "n_hiba:5645\n",
            "damage grade in     5655 nextsave=6005. step: 0.046857,0.170047,0.132241\n",
            "n_hiba:5655\n",
            "damage grade in     5665 nextsave=6005. step: 0.046854,0.170032,0.132233\n",
            "n_hiba:5665\n",
            "damage grade in     5675 nextsave=6005. step: 0.046842,0.170014,0.132226\n",
            "n_hiba:5675\n",
            "damage grade in     5685 nextsave=6005. step: 0.046836,0.170001,0.132208\n",
            "n_hiba:5685\n",
            "damage grade in     5695 nextsave=6005. step: 0.046825,0.169990,0.132198\n",
            "n_hiba:5695\n",
            "damage grade in     5705 nextsave=6005. step: 0.046820,0.169972,0.132183\n",
            "n_hiba:5705\n",
            "damage grade in     5715 nextsave=6005. step: 0.046814,0.169957,0.132175\n",
            "n_hiba:5715\n",
            "damage grade in     5725 nextsave=6005. step: 0.046807,0.169940,0.132165\n",
            "n_hiba:5725\n",
            "damage grade in     5735 nextsave=6005. step: 0.046799,0.169921,0.132155\n",
            "n_hiba:5735\n",
            "damage grade in     5745 nextsave=6005. step: 0.046793,0.169905,0.132145\n",
            "n_hiba:5745\n",
            "damage grade in     5755 nextsave=6005. step: 0.046786,0.169892,0.132133\n",
            "n_hiba:5755\n",
            "damage grade in     5765 nextsave=6005. step: 0.046778,0.169868,0.132120\n",
            "n_hiba:5765\n",
            "damage grade in     5775 nextsave=6005. step: 0.046771,0.169853,0.132108\n",
            "n_hiba:5775\n",
            "damage grade in     5785 nextsave=6005. step: 0.046766,0.169837,0.132094\n",
            "n_hiba:5785\n",
            "damage grade in     5795 nextsave=6005. step: 0.046761,0.169823,0.132079\n",
            "n_hiba:5795\n",
            "damage grade in     5805 nextsave=6005. step: 0.046754,0.169811,0.132065\n",
            "n_hiba:5805\n",
            "damage grade in     5815 nextsave=6005. step: 0.046747,0.169800,0.132053\n",
            "n_hiba:5815\n",
            "damage grade in     5825 nextsave=6005. step: 0.046742,0.169789,0.132041\n",
            "n_hiba:5825\n",
            "damage grade in     5835 nextsave=6005. step: 0.046737,0.169781,0.132030\n",
            "n_hiba:5835\n",
            "damage grade in     5845 nextsave=6005. step: 0.046733,0.169766,0.132016\n",
            "n_hiba:5845\n",
            "damage grade in     5855 nextsave=6005. step: 0.046726,0.169753,0.132000\n",
            "n_hiba:5855\n",
            "damage grade in     5865 nextsave=6005. step: 0.046718,0.169737,0.131991\n",
            "n_hiba:5865\n",
            "damage grade in     5875 nextsave=6005. step: 0.046714,0.169722,0.131984\n",
            "n_hiba:5875\n",
            "damage grade in     5885 nextsave=6005. step: 0.046710,0.169700,0.131975\n",
            "n_hiba:5885\n",
            "damage grade in     5895 nextsave=6005. step: 0.046704,0.169675,0.131959\n",
            "n_hiba:5895\n",
            "damage grade in     5905 nextsave=6005. step: 0.046699,0.169656,0.131951\n",
            "n_hiba:5905\n",
            "damage grade in     5915 nextsave=6005. step: 0.046695,0.169638,0.131942\n",
            "n_hiba:5915\n",
            "damage grade in     5925 nextsave=6005. step: 0.046690,0.169624,0.131935\n",
            "n_hiba:5925\n",
            "damage grade in     5935 nextsave=6005. step: 0.046682,0.169608,0.131927\n",
            "n_hiba:5935\n",
            "damage grade in     5945 nextsave=6005. step: 0.046676,0.169590,0.131918\n",
            "n_hiba:5945\n",
            "damage grade in     5955 nextsave=6005. step: 0.046671,0.169577,0.131912\n",
            "n_hiba:5955\n",
            "damage grade in     5965 nextsave=6005. step: 0.046665,0.169562,0.131904\n",
            "n_hiba:5965\n",
            "damage grade in     5975 nextsave=6005. step: 0.046661,0.169548,0.131892\n",
            "n_hiba:5975\n",
            "damage grade in     5985 nextsave=6005. step: 0.046658,0.169535,0.131878\n",
            "n_hiba:5985\n",
            "damage grade in     5995 nextsave=6005. step: 0.046653,0.169525,0.131863\n",
            "n_hiba:5995\n",
            "damage grade in     6005 nextsave=6005. step: 0.046648,0.169506,0.131852\n",
            "n_hiba:6005\n",
            "damage grade in     6015 nextsave=6515. step: 0.046642,0.169494,0.131840\n",
            "n_hiba:6015\n",
            "damage grade in     6025 nextsave=6515. step: 0.046636,0.169485,0.131821\n",
            "n_hiba:6025\n",
            "damage grade in     6035 nextsave=6515. step: 0.046629,0.169477,0.131802\n",
            "n_hiba:6035\n",
            "damage grade in     6045 nextsave=6515. step: 0.046620,0.169464,0.131787\n",
            "n_hiba:6045\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "n=len(clf1.train_score_)\n",
        "nextsave=n+500\n",
        "while 1==1:\n",
        "    \n",
        "    n=n+1    \n",
        "    nhiba=len(clf1.train_score_)\n",
        "    print(f\"n_hiba:{nhiba}\")\n",
        "    nhiba=nhiba+10    \n",
        "    \n",
        "    clf1.set_params(alpha=0.99,n_estimators=nhiba,warm_start=True,verbose=0)\n",
        "    clf2.set_params(alpha=0.99,n_estimators=nhiba,warm_start=True,verbose=0)\n",
        "    clf3.set_params(alpha=0.99,n_estimators=nhiba,warm_start=True,verbose=0)\n",
        "   \n",
        "    clf1.fit(X_train_train,yt1)\n",
        "    clf2.fit(X_train_train,yt2)\n",
        "    clf3.fit(X_train_train,yt3)\n",
        "   \n",
        "  \n",
        "    o1=clf1.train_score_[-1]\n",
        "    o2=clf2.train_score_[-1]\n",
        "    o3=clf3.train_score_[-1]\n",
        "    #\n",
        "    # Save the model regularly!\n",
        "    #\n",
        "    if nhiba>nextsave:\n",
        "        nextsave=nhiba+500\n",
        "        import pickle\n",
        "        nhibastr=str(nhiba)\n",
        "        fname1=basedir+\"/clf1_\"+nhibastr+\"_data.pickled\"\n",
        "        with open(fname1, 'wb') as file:\n",
        "            pickle.dump(clf1, file)\n",
        "\n",
        "        fname2=basedir+\"/clf2_\"+nhibastr+\"_data.pickled\"\n",
        "        with open(fname2, 'wb') as file:\n",
        "            pickle.dump(clf2, file)\n",
        "\n",
        "        fname3=basedir+\"/clf3_\"+nhibastr+\"_data.pickled\"\n",
        "        with open(fname3, 'wb') as file:\n",
        "            pickle.dump(clf3, file)\n",
        "\n",
        "\n",
        "    if nhiba>12070:\n",
        "        break    \n",
        "        \n",
        "    print(f\"damage grade in {nhiba:8} nextsave={nextsave}. step: {o1:8.6f},{o2:8.6f},{o3:8.6f}\" )\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 186,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "import pickle\n",
        "fname1=basedir+\"/clf1_3500_data.pickle\"\n",
        "with open(fname1, 'wb') as file:\n",
        "    pickle.dump(clf1, file)\n",
        "\n",
        "fname2=basedir+\"/clf2_3500_data.pickle\"\n",
        "with open(fname2, 'wb') as file:\n",
        "    pickle.dump(clf2, file)\n",
        "\n",
        "fname3=basedir+\"/clf3_3500_data.pickle\"\n",
        "with open(fname3, 'wb') as file:\n",
        "    pickle.dump(clf3, file)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2. 0. 1. 0.] : 0.009 _  0.225 _  0.620 _ False_ 0.000\n",
            "[1. 1. 0. 0.] : 0.593 _  0.363 _  0.126 _ True_ 0.000\n",
            "[2. 0. 1. 0.] : 0.476 _  0.494 _  0.048 _ True_ 0.000\n",
            "[2. 0. 1. 0.] :-0.003 _  0.432 _  0.602 _ False_ 0.000\n",
            "[2. 0. 1. 0.] : 0.177 _  0.727 _  0.109 _ True_ 0.000\n",
            "[3. 0. 0. 1.] :-0.007 _  0.429 _  0.575 _ True_ 0.000\n",
            "[2. 0. 1. 0.] : 0.140 _  0.759 _  0.128 _ True_ 0.000\n",
            "[2. 0. 1. 0.] : 0.154 _  0.659 _  0.166 _ True_ 0.000\n",
            "[3. 0. 0. 1.] :-0.005 _  0.120 _  0.877 _ True_ 0.000\n",
            "[1. 1. 0. 0.] : 0.147 _  0.615 _  0.228 _ False_ 0.000\n",
            "[2. 0. 1. 0.] :-0.010 _  0.148 _  0.862 _ False_ 0.000\n",
            "[1. 1. 0. 0.] : 0.670 _  0.397 _ -0.040 _ True_ 0.000\n",
            "[2. 0. 1. 0.] : 0.004 _  0.193 _  0.829 _ False_ 0.000\n",
            "[3. 0. 0. 1.] : 0.002 _  0.458 _  0.531 _ True_ 0.000\n",
            "[3. 0. 0. 1.] :-0.009 _  0.104 _  0.893 _ True_ 0.000\n",
            "[3. 0. 0. 1.] : 0.004 _  0.452 _  0.556 _ True_ 0.000\n",
            "[1. 1. 0. 0.] : 0.320 _  0.561 _  0.132 _ False_ 0.000\n",
            "[2. 0. 1. 0.] : 0.013 _  0.816 _  0.161 _ True_ 0.000\n",
            "[3. 0. 0. 1.] :-0.009 _  0.659 _  0.395 _ False_ 1.000\n",
            "[3. 0. 0. 1.] :-0.004 _  0.502 _  0.550 _ True_ 0.000\n",
            "[3. 0. 0. 1.] : 0.050 _  0.338 _  0.449 _ True_ 0.000\n",
            "[2. 0. 1. 0.] : 0.325 _  0.395 _  0.441 _ False_ 0.000\n",
            "[2. 0. 1. 0.] : 0.344 _  0.585 _  0.076 _ True_ 0.000\n",
            "[2. 0. 1. 0.] : 0.016 _  0.728 _  0.260 _ True_ 0.000\n",
            "[2. 0. 1. 0.] : 0.011 _  0.818 _  0.241 _ True_ 0.000\n",
            "[2. 0. 1. 0.] : 0.007 _  0.752 _  0.255 _ True_ 0.000\n",
            "257994 177400 0.6876128902222532\n"
          ]
        }
      ],
      "source": [
        "\n",
        "yhibaold=[]\n",
        "maxlen=len(y_train_train)\n",
        "#maxlen=1000\n",
        "stimm=0\n",
        "for idx,_ in enumerate(y1):\n",
        "    if idx in range(0,maxlen):\n",
        "        ou1=y1[idx]\n",
        "        ou2=y2[idx]\n",
        "        ou3=y3[idx]\n",
        "        ouh=yhiba_pred[idx]\n",
        "        match=(y_train_train[idx][0]==1 and ou1==max(ou1,ou2,ou3)) or (y_train_train[idx][0]==2 and ou2==max(ou1,ou2,ou3)) or (y_train_train[idx][0]==3 and ou3==max(ou1,ou2,ou3)) \n",
        "        if match:\n",
        "            yhibaold.append(0)\n",
        "            stimm+=1\n",
        "        else:\n",
        "            yhibaold.append(1)\n",
        "        if idx%10000==1:\n",
        "            print(f\"{y_train_train[idx]} :{ou1:6.3f} _ {ou2:6.3f} _ {ou3:6.3f} _ {match}_{ouh:6.3f}\")\n",
        "\n",
        "print(maxlen,stimm,stimm/maxlen) \n",
        "#print(yhiba)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {},
      "outputs": [],
      "source": [
        "yhiba=yhibaold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "metadata": {
        "tags": [
          "outputPrepend"
        ]
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rade in      429. step: 0.707381\n",
            "Iteration 421, loss = 0.56447394\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      430. step: 0.707040\n",
            "Iteration 422, loss = 0.56448940\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      431. step: 0.707214\n",
            "Iteration 423, loss = 0.56449091\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      432. step: 0.707075\n",
            "Iteration 424, loss = 0.56446701\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      433. step: 0.707137\n",
            "Iteration 425, loss = 0.56446139\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      434. step: 0.707361\n",
            "Iteration 426, loss = 0.56441907\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      435. step: 0.707435\n",
            "Iteration 427, loss = 0.56442155\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      436. step: 0.707129\n",
            "Iteration 428, loss = 0.56436299\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      437. step: 0.707311\n",
            "Iteration 429, loss = 0.56436408\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      438. step: 0.707319\n",
            "Iteration 430, loss = 0.56433492\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      439. step: 0.707168\n",
            "Iteration 431, loss = 0.56434023\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      440. step: 0.707086\n",
            "Iteration 432, loss = 0.56434439\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      441. step: 0.707563\n",
            "Iteration 433, loss = 0.56431722\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      442. step: 0.707148\n",
            "Iteration 434, loss = 0.56428421\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      443. step: 0.707206\n",
            "Iteration 435, loss = 0.56428551\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      444. step: 0.707447\n",
            "Iteration 436, loss = 0.56426005\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      445. step: 0.707501\n",
            "Iteration 437, loss = 0.56428511\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      446. step: 0.707315\n",
            "Iteration 438, loss = 0.56425722\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      447. step: 0.707342\n",
            "Iteration 439, loss = 0.56421680\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      448. step: 0.707617\n",
            "Iteration 440, loss = 0.56424147\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      449. step: 0.707544\n",
            "Iteration 441, loss = 0.56419197\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      450. step: 0.707451\n",
            "Iteration 442, loss = 0.56423825\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      451. step: 0.707741\n",
            "Iteration 443, loss = 0.56422514\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      452. step: 0.707741\n",
            "Iteration 444, loss = 0.56417038\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      453. step: 0.707734\n",
            "Iteration 445, loss = 0.56418429\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      454. step: 0.707528\n",
            "Iteration 446, loss = 0.56415396\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      455. step: 0.707745\n",
            "Iteration 447, loss = 0.56411894\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      456. step: 0.707629\n",
            "Iteration 448, loss = 0.56408139\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      457. step: 0.707908\n",
            "Iteration 449, loss = 0.56413224\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      458. step: 0.707691\n",
            "Iteration 450, loss = 0.56412384\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      459. step: 0.707675\n",
            "Iteration 451, loss = 0.56409592\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      460. step: 0.707850\n",
            "Iteration 452, loss = 0.56413318\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      461. step: 0.707625\n",
            "Iteration 453, loss = 0.56413555\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      462. step: 0.707885\n",
            "Iteration 454, loss = 0.56408605\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      463. step: 0.708009\n",
            "Iteration 455, loss = 0.56410666\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      464. step: 0.708315\n",
            "Iteration 456, loss = 0.56416302\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      465. step: 0.708373\n",
            "Iteration 457, loss = 0.56406866\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      466. step: 0.708059\n",
            "Iteration 458, loss = 0.56413890\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      467. step: 0.708078\n",
            "Iteration 459, loss = 0.56406298\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      468. step: 0.708055\n",
            "Iteration 460, loss = 0.56407571\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      469. step: 0.708412\n",
            "Iteration 461, loss = 0.56403355\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      470. step: 0.707985\n",
            "Iteration 462, loss = 0.56406183\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      471. step: 0.708280\n",
            "Iteration 463, loss = 0.56397712\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      472. step: 0.708175\n",
            "Iteration 464, loss = 0.56397746\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      473. step: 0.708024\n",
            "Iteration 465, loss = 0.56399785\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      474. step: 0.707858\n",
            "Iteration 466, loss = 0.56397041\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      475. step: 0.707989\n",
            "Iteration 467, loss = 0.56399413\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      476. step: 0.707989\n",
            "Iteration 468, loss = 0.56394023\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      477. step: 0.708051\n",
            "Iteration 469, loss = 0.56396862\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      478. step: 0.707799\n",
            "Iteration 470, loss = 0.56394707\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      479. step: 0.707978\n",
            "Iteration 471, loss = 0.56393472\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      480. step: 0.707788\n",
            "Iteration 472, loss = 0.56397065\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      481. step: 0.707819\n",
            "Iteration 473, loss = 0.56392727\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      482. step: 0.707609\n",
            "Iteration 474, loss = 0.56383622\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      483. step: 0.707726\n",
            "Iteration 475, loss = 0.56389399\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      484. step: 0.707749\n",
            "Iteration 476, loss = 0.56386356\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      485. step: 0.707695\n",
            "Iteration 477, loss = 0.56385090\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      486. step: 0.707683\n",
            "Iteration 478, loss = 0.56385059\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      487. step: 0.707571\n",
            "Iteration 479, loss = 0.56380338\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      488. step: 0.707540\n",
            "Iteration 480, loss = 0.56383064\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      489. step: 0.707078\n",
            "Iteration 481, loss = 0.56382773\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      490. step: 0.707280\n",
            "Iteration 482, loss = 0.56379624\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      491. step: 0.707703\n",
            "Iteration 483, loss = 0.56373660\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      492. step: 0.707354\n",
            "Iteration 484, loss = 0.56371104\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      493. step: 0.707726\n",
            "Iteration 485, loss = 0.56374022\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      494. step: 0.707578\n",
            "Iteration 486, loss = 0.56370839\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      495. step: 0.707520\n",
            "Iteration 487, loss = 0.56372882\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      496. step: 0.707947\n",
            "Iteration 488, loss = 0.56370714\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      497. step: 0.707761\n",
            "Iteration 489, loss = 0.56366768\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      498. step: 0.707683\n",
            "Iteration 490, loss = 0.56368081\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      499. step: 0.708086\n",
            "Iteration 491, loss = 0.56366035\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      500. step: 0.707699\n",
            "Iteration 492, loss = 0.56367236\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      501. step: 0.707861\n",
            "Iteration 493, loss = 0.56366508\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      502. step: 0.707691\n",
            "Iteration 494, loss = 0.56361856\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      503. step: 0.707854\n",
            "Iteration 495, loss = 0.56361852\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      504. step: 0.708055\n",
            "Iteration 496, loss = 0.56353619\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      505. step: 0.707838\n",
            "Iteration 497, loss = 0.56356360\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      506. step: 0.707896\n",
            "Iteration 498, loss = 0.56355298\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      507. step: 0.707447\n",
            "Iteration 499, loss = 0.56353790\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      508. step: 0.707586\n",
            "Iteration 500, loss = 0.56355202\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      509. step: 0.708001\n",
            "Iteration 501, loss = 0.56350169\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      510. step: 0.707563\n",
            "Iteration 502, loss = 0.56350002\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      511. step: 0.707780\n",
            "Iteration 503, loss = 0.56349411\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      512. step: 0.707753\n",
            "Iteration 504, loss = 0.56346504\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      513. step: 0.707606\n",
            "Iteration 505, loss = 0.56344738\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      514. step: 0.707416\n",
            "Iteration 506, loss = 0.56348362\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      515. step: 0.707931\n",
            "Iteration 507, loss = 0.56346894\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      516. step: 0.707796\n",
            "Iteration 508, loss = 0.56346407\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      517. step: 0.707640\n",
            "Iteration 509, loss = 0.56348542\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      518. step: 0.707931\n",
            "Iteration 510, loss = 0.56340559\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      519. step: 0.707854\n",
            "Iteration 511, loss = 0.56341206\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      520. step: 0.707602\n",
            "Iteration 512, loss = 0.56346494\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      521. step: 0.707823\n",
            "Iteration 513, loss = 0.56344935\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      522. step: 0.708106\n",
            "Iteration 514, loss = 0.56341177\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      523. step: 0.708009\n",
            "Iteration 515, loss = 0.56342456\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      524. step: 0.707892\n",
            "Iteration 516, loss = 0.56342011\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      525. step: 0.708109\n",
            "Iteration 517, loss = 0.56341131\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      526. step: 0.708063\n",
            "Iteration 518, loss = 0.56338695\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      527. step: 0.708020\n",
            "Iteration 519, loss = 0.56338215\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      528. step: 0.707978\n",
            "Iteration 520, loss = 0.56337460\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      529. step: 0.707989\n",
            "Iteration 521, loss = 0.56337108\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      530. step: 0.708016\n",
            "Iteration 522, loss = 0.56335635\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      531. step: 0.707889\n",
            "Iteration 523, loss = 0.56329447\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      532. step: 0.708222\n",
            "Iteration 524, loss = 0.56322191\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      533. step: 0.708094\n",
            "Iteration 525, loss = 0.56329243\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      534. step: 0.708028\n",
            "Iteration 526, loss = 0.56325032\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      535. step: 0.708334\n",
            "Iteration 527, loss = 0.56324535\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      536. step: 0.708016\n",
            "Iteration 528, loss = 0.56320332\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      537. step: 0.708183\n",
            "Iteration 529, loss = 0.56318179\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      538. step: 0.708113\n",
            "Iteration 530, loss = 0.56319047\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      539. step: 0.708303\n",
            "Iteration 531, loss = 0.56322947\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      540. step: 0.708493\n",
            "Iteration 532, loss = 0.56320622\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      541. step: 0.708365\n",
            "Iteration 533, loss = 0.56319409\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      542. step: 0.708245\n",
            "Iteration 534, loss = 0.56313984\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      543. step: 0.708109\n",
            "Iteration 535, loss = 0.56318681\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      544. step: 0.708613\n",
            "Iteration 536, loss = 0.56315073\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      545. step: 0.708047\n",
            "Iteration 537, loss = 0.56309893\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      546. step: 0.708253\n",
            "Iteration 538, loss = 0.56317837\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      547. step: 0.708261\n",
            "Iteration 539, loss = 0.56312105\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      548. step: 0.708218\n",
            "Iteration 540, loss = 0.56310173\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      549. step: 0.708179\n",
            "Iteration 541, loss = 0.56309864\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      550. step: 0.708330\n",
            "Iteration 542, loss = 0.56314369\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
          ]
        }
      ],
      "source": [
        "i=0\n",
        "\n",
        "n=n+10\n",
        "\n",
        "while 1==1:\n",
        "   \n",
        "    nhiba=clfhiba3.n_iter_\n",
        "    nhiba=nhiba+10    \n",
        "    \n",
        "\n",
        "    clfhiba3.set_params(warm_start=True,verbose=3,learning_rate=\"adaptive\",max_iter=nhiba)\n",
        " \n",
        "    clfhiba3.fit(X_train_train,yhiba)\n",
        "  \n",
        "\n",
        "    ohiba=clfhiba3.score(X_train_train,yhiba)\n",
        "    if nhiba>550:\n",
        "        break    \n",
        "    print(f\"damage grade in {nhiba:8}. step: {ohiba:8.6f}\" )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 176,
      "metadata": {},
      "outputs": [],
      "source": [
        "y1=clf1.predict(X_train_train)\n",
        "y2=clf2.predict(X_train_train)\n",
        "y3=clf3.predict(X_train_train)\n",
        "yhiba_pred=clfhiba3.predict(X_train_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 177,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.13535196806372593"
            ]
          },
          "metadata": {},
          "execution_count": 177
        }
      ],
      "source": [
        "clf3.train_score_[-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "metadata": {},
      "outputs": [],
      "source": [
        "def show_similarity(a,b):\n",
        "    for i in range(min(len(a),1000)):\n",
        "        print(f\"{a[i]}-{b[i]} \",end=\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 155,
      "metadata": {},
      "outputs": [],
      "source": [
        "def search_outliers(y_t1,y_pred,X_orig,yrealpred):\n",
        "    y1=[] # y orig \n",
        "    x1=[] # X orig\n",
        "    yp=[] # y predikt\n",
        "    yrp=[] # y predicted real value\n",
        "    for inx,i in enumerate(y_t1):\n",
        "        if y_t1[inx][0]==y_pred[inx]:\n",
        "            pass\n",
        "        else:\n",
        "            y1.append(y_t1[inx][0])\n",
        "            x1.append(X_orig[inx])\n",
        "            yp.append(y_pred[inx])\n",
        "            yrp.append(yrealpred[inx])\n",
        "    out=(y1,yp,x1,yrp)\n",
        "    return out\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "metadata": {},
      "outputs": [],
      "source": [
        "# futtass ez felett !!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.25      , 0.02512563, 0.1010101 , ..., 0.        , 1.        ,\n",
              "        0.22846051],\n",
              "       [0.125     , 0.00502513, 0.05050505, ..., 0.        , 1.        ,\n",
              "        0.59586796],\n",
              "       [0.125     , 0.        , 0.1010101 , ..., 0.        , 1.        ,\n",
              "        0.87090745],\n",
              "       ...,\n",
              "       [0.        , 0.        , 0.04040404, ..., 0.        , 1.        ,\n",
              "        0.47198577],\n",
              "       [0.25      , 0.02512563, 0.1010101 , ..., 0.        , 1.        ,\n",
              "        0.70982493],\n",
              "       [0.125     , 0.0201005 , 0.07070707, ..., 0.        , 1.        ,\n",
              "        0.67640778]])"
            ]
          },
          "metadata": {},
          "execution_count": 157
        }
      ],
      "source": [
        "X_train_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 194,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hiba:64795 max:257994 -- error: 25.1149 good %: 74.8851 %\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7488507484670186"
            ]
          },
          "metadata": {},
          "execution_count": 194
        }
      ],
      "source": [
        "ypred1=reconvert(X_train_train)\n",
        "check_similarity_np(ypred1,y_train_train[:,0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 195,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hiba:700 max:2607 -- error: 26.8508 good %: 73.1492 %\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7314921365554277"
            ]
          },
          "metadata": {},
          "execution_count": 195
        }
      ],
      "source": [
        "ypred2=reconvert(X_train_test)\n",
        "check_similarity_np(ypred2,y_train_test[:,0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 180,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction End\n"
          ]
        }
      ],
      "source": [
        "#outfile generation\n",
        "y_pred_ok=reconvert(X_pred_scale)\n",
        "print(\"Prediction End\")\n",
        "\n",
        "X_pred_bd=pd.read_csv(basedir+\"/orig/test_values.csv\")\n",
        "\n",
        "#y_pred_ok_int=conv_a_floatlist(y_pred_ok,range_x)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wD9rzAN4pBR5",
        "outputId": "ab2363f0-11eb-4e5a-c1a4-a71d12168323"
      },
      "source": [
        "buildingid=X_pred_bd[\"building_id\"]\n",
        "\n",
        "head2=y_pred_ok\n",
        "\n",
        "\n",
        "outdf=pd.DataFrame(data={\"damage_grade\":y_pred_ok} ,index=buildingid)\n",
        "outdf.index.name=\"building_id\"\n",
        "\n",
        "\n",
        "outdf.head()\n",
        "st=40\n",
        "sts=str(st)\n",
        "outdf.to_csv(basedir+\"/out/submission_\"+sts+\"_xgboost.csv\")\n",
        "print()\n",
        "print(basedir+\"/out/submission_\"+sts+\"_xgboost.csv\")"
      ],
      "execution_count": 181,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\nC:/Users/sipocz/OneDrive/Dokumentumok/GitHub/_EarthQuake/gpos_lin/out/submission_40_xgboost.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PjkOA2B1Pys6",
        "outputId": "e76ee986-189a-401e-9df9-722850ff2e5b"
      },
      "source": [
        "if not( _PCVERSION_):\r\n",
        "    !head \"/content/drive/My Drive/001_AI/_EarthQuake/gpos_lin/out/submission_24_xgboost.csv\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 167,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.28571429, 0.0201005 , 0.06593407, ..., 0.        , 1.        ,\n",
              "        0.56170323],\n",
              "       [0.14285714, 0.02512563, 0.13186813, ..., 0.        , 1.        ,\n",
              "        0.19576828],\n",
              "       [0.14285714, 0.00502513, 0.03296703, ..., 0.        , 1.        ,\n",
              "        0.71028087],\n",
              "       ...,\n",
              "       [0.        , 0.05025126, 0.02197802, ..., 0.        , 1.        ,\n",
              "        0.73558512],\n",
              "       [0.14285714, 0.00502513, 0.08791209, ..., 0.        , 0.        ,\n",
              "        0.21615666],\n",
              "       [0.14285714, 0.01005025, 0.10989011, ..., 0.        , 1.        ,\n",
              "        0.83998508]])"
            ]
          },
          "metadata": {},
          "execution_count": 167
        }
      ],
      "source": [
        "X_pred_scale"
      ]
    },
    {
      "source": [],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "source": [
        "<img src=\"https://www.python.org/static/community_logos/python-logo-master-v3-TM.png\" title=\"Python Logo\"/>"
      ],
      "cell_type": "markdown",
      "metadata": {}
    }
  ]
}