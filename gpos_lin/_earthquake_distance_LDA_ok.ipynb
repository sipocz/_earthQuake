{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "_earthquake_distance_LDA.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "_PCVERSION_=True\n",
        "\n",
        "\n",
        "if _PCVERSION_:\n",
        "    basedir=\"C:/Users/sipocz/OneDrive/Dokumentumok/GitHub/_EarthQuake/gpos_lin\"\n",
        "else:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive',force_remount=True)\n",
        "    basedir=\"/content/drive/My Drive/001_AI/_EarthQuake/gpos_lin\"\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "#--------------scikit import \n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "#--------------\n",
        "\n",
        "def outlierStatistic(X_train_predict):\n",
        "    print(X_train_predict)\n",
        "    maxX=len(X_train_predict)\n",
        "    outlier=0\n",
        "    for i in X_train_predict:\n",
        "        if i==-1:\n",
        "            outlier+=1\n",
        "    print(f\"A összes ({maxX} darabból {outlier} darab outlier van. Az {outlier/maxX*100:5.1f} %.)\")\n",
        "\n",
        "\n",
        "def checkvalues(df,columnname,key):\n",
        "    print(f\"{columnname} ellenőrzése !\")\n",
        "    numok=0\n",
        "    numerr=0\n",
        "    for i in df.index:\n",
        "        if df.at[i,columnname] in key:\n",
        "            #print(df.at[i,columnname])\n",
        "            numok+=1\n",
        "            pass\n",
        "        else:\n",
        "            numerr+=1\n",
        "            print(df.at[i,columnname],end=\", \")\n",
        "    sumall=numok+numerr\n",
        "    print(f\"\\n{sumall} mintából {numerr} db nem volt megfelelő\")\n",
        "\n",
        "\n",
        "def createcolumn(df,columnname,keys):\n",
        "    print(f\"{columnname} cseréje megy\")\n",
        "    for key in keys:\n",
        "        df[keys[key]]=0.0\n",
        "    for key in keys:\n",
        "        for i in df.index:\n",
        "            if df.at[i,columnname]==key:\n",
        "                df.at[i,keys[key]]=1.0\n",
        "\n",
        "\n",
        "def create_dict(idx,list):\n",
        "    o={}\n",
        "    for i in list:\n",
        "        o[i]=idx+\"_\"+i\n",
        "    return o\n",
        "\n",
        "def create_base_data(df):\n",
        "    t=['n', 't', 'o']\n",
        "    columnname=\"land_surface_condition\"\n",
        "    key=create_dict(columnname,t)\n",
        "\n",
        "\n",
        "    #checkvalues(df,columnname,key)\n",
        "    createcolumn(df,columnname,key)\n",
        "\n",
        "    t= ['h', 'w', 'i', 'r', 'u']\n",
        "    columnname=\"foundation_type\"\n",
        "    key=create_dict(columnname,t)\n",
        "\n",
        "    #checkvalues(df,columnname,key)\n",
        "    createcolumn(df,columnname,key)\n",
        "\n",
        "    t=  ['q', 'n', 'x']\n",
        "    columnname=\"roof_type\"\n",
        "    key=create_dict(columnname,t)\n",
        "\n",
        "    #checkvalues(df,columnname,key)\n",
        "    createcolumn(df,columnname,key)\n",
        "\n",
        "    t=  ['z', 'v', 'f', 'm', 'x']\n",
        "    columnname=\"ground_floor_type\"\n",
        "    key=create_dict(columnname,t)\n",
        "\n",
        "    #checkvalues(df,columnname,key)\n",
        "    createcolumn(df,columnname,key)\n",
        "\n",
        "    t=   ['q', 's', 'j', 'x']\n",
        "    columnname=\"other_floor_type\"\n",
        "    key=create_dict(columnname,t)\n",
        "\n",
        "    #checkvalues(df,columnname,key)\n",
        "    createcolumn(df,columnname,key)\n",
        "\n",
        "    t=   ['j', 's', 't', 'o']\n",
        "    columnname=\"position\"\n",
        "    key=create_dict(columnname,t)\n",
        "\n",
        "    #checkvalues(df,columnname,key)\n",
        "    createcolumn(df,columnname,key)\n",
        "\n",
        "    t=   ['c', 's', 'f', 'd', 'm', 'a', 'q', 'u', 'n', 'o']\n",
        "    columnname=\"plan_configuration\"\n",
        "    key=create_dict(columnname,t)\n",
        "\n",
        "    #checkvalues(df,columnname,key)\n",
        "    createcolumn(df,columnname,key)\n",
        "\n",
        "    t=['a', 'w', 'r', 'v']\n",
        "    columnname=\"legal_ownership_status\"\n",
        "    key=create_dict(columnname,t)\n",
        "\n",
        "    #checkvalues(df,columnname,key)\n",
        "    createcolumn(df,columnname,key)\n",
        "\n",
        "    # level 1: 0-30, level 2: 0-1427, level 3: 0-12567.\n",
        "    # level1:0--30\n",
        "    # level2: 0.0000-----------0.9999\n",
        "    # level3: 0.000000000------0.000099999\n",
        "    l1=df.geo_level_1_id\n",
        "    l2=(df.geo_level_2_id/1427*9999)/10000\n",
        "    l3=(df.geo_level_2_id/12567*99999)/1000000000\n",
        "    df[\"geopos\"]=l1+l2+l3\n",
        "    return(df)\n",
        "\n",
        "\n",
        "\n",
        "def kill_columns(df):\n",
        "    notkey=[\"Unnamed: 0\",\"building_id\",\"legal_ownership_status\",\"geo_level_1_id\",\t\"geo_level_2_id\",\t\"geo_level_3_id\", \"land_surface_condition\",\t\"foundation_type\",\t\"roof_type\",\t\"ground_floor_type\",\t\"other_floor_type\",\t\"position\",\t\"plan_configuration\"]\n",
        "    for i in df.columns:\n",
        "        #print(i)\n",
        "        if i in notkey:\n",
        "            df.drop(columns=[i], inplace=True)\n",
        "    return df\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def check_similarity_np(np1,np2):\n",
        "    db=0\n",
        "    maxi=0\n",
        "    for ind, i1 in enumerate(np1):\n",
        "        maxi+=1\n",
        "        i2=np2[ind]\n",
        "        if i1!=i2:\n",
        "            db+=1\n",
        "    #print(f\"{ind}. eset:  {i:3},{i2:3}\")\n",
        "    print(f\"hiba:{db} max:{maxi} -- error:{db/maxi*100.0 : 2.6} good %:{100-db/maxi*100.0 : 2.6} %\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 502
        },
        "id": "HZHYUIgKIlT9",
        "outputId": "3740844a-f48c-4b06-87fd-f1eddb0431ab"
      },
      "source": [
        "'''\n",
        "features_train=basedir+\"/orig/train_values.csv\"\n",
        "labels_train=basedir+\"/orig/train_labels.csv\"\n",
        "features_predict=basedir+\"/orig/test_values.csv\"\n",
        "\n",
        "X_train=pd.read_csv(features_train)\n",
        "y_train=pd.read_csv(labels_train)\n",
        "X_pred=pd.read_csv(features_predict)\n",
        "#\n",
        "X_pred_conv=create_base_data(X_pred)\n",
        "X_train_conv=create_base_data(X_train)\n",
        "#\n",
        "X_train_ok=kill_columns(X_train_conv)\n",
        "X_pred_ok=kill_columns(X_pred_conv)\n",
        "y_train_ok=kill_columns(y_train)\n",
        "\n",
        "X_train_ok.to_csv(basedir+\"/tmp/X_tran_ok.csv\",index=False)\n",
        "X_pred_ok.to_csv(basedir+\"/tmp/X_pred_ok.csv\",index=False)\n",
        "y_train_ok.to_csv(basedir+\"/tmp/y_train_ok.csv\",index=False)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#f=open(basedir+\"/tmp/similarity.csv\",\"a\")\n",
        "X_train_ok=pd.read_csv(basedir+\"/tmp/X_tran_ok.csv\",)\n",
        "X_pred_ok=pd.read_csv(basedir+\"/tmp/X_pred_ok.csv\")\n",
        "y_train_ok=pd.read_csv(basedir+\"/tmp/y_train_ok.csv\")\n",
        "\n",
        "X_train_ok.head()\n",
        "X_pred_ok.head()\n",
        "#print(\"Ready\")\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZkluH7RwmL_M"
      },
      "source": [
        "X_pred_ok=kill_columns(X_pred_ok)\r\n",
        "X_train_ok=kill_columns(X_train_ok)\r\n",
        "y_train_ok=kill_columns(y_train_ok)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oFWkbCwqrI32"
      },
      "source": [
        "print(type(X_pred_ok))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q0rn-7TBQfRp"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler1=StandardScaler()\n",
        "X_pred_scale=scaler1.fit_transform(X_pred_ok)\n",
        "\n",
        "scaler2=StandardScaler()\n",
        "X_train_scale=scaler2.fit_transform(X_train_ok)\n",
        "\"\"\"\n",
        "scaler3=StandardScaler()\n",
        "y_train_scale=scaler3.fit_transform(y_train_ok)\n",
        "\"\"\"\n",
        "y_train_np=y_train_ok.to_numpy()\n",
        "\n",
        "# szétszedjük a train és test részekre\n",
        "from sklearn.model_selection import train_test_split\n",
        "#X_train_train, X_train_test,y_train_train, y_train_test  = train_test_split( X_train_scale, y_train_scale, test_size=0.10, random_state=0)\n",
        "X_train_train, X_train_test,y_train_train, y_train_test  = train_test_split( X_train_scale, y_train_np, test_size=0.10, random_state=0)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(type(y_train_train))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# *************\n",
        "# * LDA       *\n",
        "# *************\n",
        "\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "\n",
        "lda = LinearDiscriminantAnalysis(n_components=2, solver=\"svd\", tol=0.000000001)\n",
        "lda.fit(X_train_train, y_train_train)\n",
        "\n",
        "X_train_lda = lda.transform(X_train_train)\n",
        "#y_train = lda.transform(y_train_train)\n",
        "X_test_lda=lda.transform(X_train_test)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# OTLET:\n",
        "# LDA térben keressünk outliereket... \n",
        "# ha nincs túl sok tanítsuk a modellt az inlierekre\n",
        "# \n",
        "#  \n",
        "from sklearn.ensemble import IsolationForest\n",
        "\n",
        "rng=1\n",
        "#------------------------------\n",
        "outliers_fraction=0.01\n",
        "#------------------------------\n",
        "clf=IsolationForest(contamination=outliers_fraction, random_state=rng)\n",
        "clf.fit(X_train_lda)\n",
        "outlier_predict=clf.predict(X_train_lda)\n",
        "\n",
        "\n",
        "print(\"Outlier Detection ENDED\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(outlier_predict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "outlier_index=[]\n",
        "for inx,i in enumerate(X_train_lda):\n",
        "    if outlier_predict[inx]==-1:\n",
        "        outlier_index.append(inx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "print(type(y_train_train))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def outlierDropfrom_df(df,inxlist):\n",
        "    a=df\n",
        "    out=a.drop(inxlist,axis=0)\n",
        "    return(out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def outlierDropfrom_numpyarray(na,inxlist):\n",
        "    \n",
        "    out=np.delete (na ,inxlist, axis=0)\n",
        "    return(out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train_train_out=X_train_train\n",
        "y_train_train_out=y_train_train\n",
        "#print(X_train_train_out)\n",
        "#print(outlier_predict)\n",
        "\n",
        "print(\"Drop2\")\n",
        "\n",
        "y_train_train_out=outlierDropfrom_numpyarray(y_train_train,outlier_predict)\n",
        "print(len(y_train_train_out),\" .. \",len(outlier_predict))\n",
        "#print(\"Drop1\",len(X_train_lda))\n",
        "X_train_train_out=outlierDropfrom_numpyarray(X_train_lda,outlier_predict)\n",
        "print(len(X_train_train_out),\" .. \",len(outlier_predict))\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(y_train_train)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KpEo_arRSK-Y",
        "outputId": "ed01a5e0-f52e-4b28-afc5-4711ca297ada",
        "tags": [
          "outputPrepend"
        ]
      },
      "source": [
        "# XGBClassifier\n",
        "'''\n",
        "from xgboost import XGBClassifier  # 72.09\n",
        "# max_depth=10 : 72.79857561664441\n",
        "\n",
        "knn = XGBClassifier(verbosity=3,max_depth = 13,n_estimators=100,loss=\"deviance\", criterion=\"mae\")\n",
        "\n",
        "print(\"Fit: Inlier betanitás Start--\")\n",
        "\n",
        "knn.fit(X_train_train_out, y_train_train_out)\n",
        "\n",
        "print(\"Fit End - Prediction Start \")\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 176,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[2]\n [2]\n [2]\n ...\n [3]\n [3]\n [3]]\n"
          ]
        }
      ],
      "source": [
        "print(y_train_train_out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 196,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fit End - Prediction Start \n"
          ]
        }
      ],
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "knn = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(100,), random_state=1, verbose=True, max_iter=1000, tol=0.0000001)\n",
        "#knn.fit(X_train_train_out, y_train_train_out)\n",
        "knn.fit(X_train_train, y_train_train)\n",
        "\n",
        "print(\"Fit End - Prediction Start \")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 197,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-1.07318179 -0.22064278]\n [ 0.36871787 -0.07171722]\n [-1.24793817 -1.35487209]\n ...\n [ 0.4365659  -0.38185691]\n [ 0.25811703  1.4176803 ]\n [ 0.99633896  1.92926339]]\n"
          ]
        }
      ],
      "source": [
        "print(X_train_train_out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 198,
      "metadata": {},
      "outputs": [],
      "source": [
        "# test the traing\n",
        "#------------------------------------\n",
        "\n",
        "y_pred_train=knn.predict(X_train_train)\n",
        "\n",
        "#------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 199,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hiba:74285 max:234540 -- error: 31.6726 good %: 68.3274 %\n"
          ]
        }
      ],
      "source": [
        "check_similarity_np(y_pred_train,y_train_train)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ut8Tk6p4tMUr"
      },
      "source": [
        "# the prediction\n",
        "#------------------------------------\n",
        "\n",
        "y_pred_test=knn.predict(X_test_lda)\n",
        "\n",
        "#------------------------------------"
      ],
      "execution_count": 174,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IyQx3aPoyim9",
        "outputId": "785937c2-57e6-4e74-edd6-7e3b81a4748e"
      },
      "source": [
        "check_similarity_np(y_pred_test,y_train_test)"
      ],
      "execution_count": 179,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hiba:10381 max:26061 -- error: 39.8335 good %: 60.1665 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# futtass ez felett !!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QAuqcJ7xke4c",
        "outputId": "cf1018c8-fad5-4bb6-9bfa-788d2c5e5ab9"
      },
      "source": [
        "#outfile generation\r\n",
        "y_pred_ok=knn.predict(X_pred_ok)\r\n",
        "print(\"Prediction End\")\r\n",
        "\r\n",
        "X_pred_bd=pd.read_csv(basedir+\"/orig/test_values.csv\")\r\n",
        "\r\n",
        "y_pred_ok_int=[]\r\n",
        "for i in range(len(y_pred_ok)):\r\n",
        "    y_pred_ok_int.append(int(y_pred_ok[i]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wD9rzAN4pBR5",
        "outputId": "ab2363f0-11eb-4e5a-c1a4-a71d12168323"
      },
      "source": [
        "buildingid=X_pred_bd[\"building_id\"]\r\n",
        "head2=y_pred_ok_int\r\n",
        "\r\n",
        "\r\n",
        "outdf=pd.DataFrame(data={\"damage_grade\":y_pred_ok_int} ,index=buildingid)\r\n",
        "outdf.index.name=\"building_id\"\r\n",
        "\r\n",
        "\r\n",
        "outdf.head()\r\n",
        "st=24\r\n",
        "sts=str(st)\r\n",
        "outdf.to_csv(basedir+\"/out/submission_\"+sts+\"_xgboost.csv\")\r\n",
        "print()\r\n",
        "print(basedir+\"/out/submission_\"+sts+\"_xgboost.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PjkOA2B1Pys6",
        "outputId": "e76ee986-189a-401e-9df9-722850ff2e5b"
      },
      "source": [
        "if not( _PCVERSION_):\r\n",
        "    !head \"/content/drive/My Drive/001_AI/_EarthQuake/gpos_lin/out/submission_24_xgboost.csv\""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}