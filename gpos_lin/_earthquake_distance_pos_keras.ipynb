{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "_earthquake_distance_LDA.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "_PCVERSION_=True\n",
        "\n",
        "\n",
        "if _PCVERSION_:\n",
        "    basedir=\"C:/Users/sipocz/OneDrive/Dokumentumok/GitHub/_EarthQuake/gpos_lin\"\n",
        "else:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive',force_remount=True)\n",
        "    basedir=\"/content/drive/My Drive/001_AI/_EarthQuake/gpos_lin\"\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "#--------------scikit import \n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "#--------------\n",
        "\n",
        "def outlierStatistic(X_train_predict):\n",
        "    print(X_train_predict)\n",
        "    maxX=len(X_train_predict)\n",
        "    outlier=0\n",
        "    for i in X_train_predict:\n",
        "        if i==-1:\n",
        "            outlier+=1\n",
        "    print(f\"A összes ({maxX} darabból {outlier} darab outlier van. Az {outlier/maxX*100:5.1f} %.)\")\n",
        "\n",
        "\n",
        "def checkvalues(df,columnname,key):\n",
        "    print(f\"{columnname} ellenőrzése !\")\n",
        "    numok=0\n",
        "    numerr=0\n",
        "    for i in df.index:\n",
        "        if df.at[i,columnname] in key:\n",
        "            #print(df.at[i,columnname])\n",
        "            numok+=1\n",
        "            pass\n",
        "        else:\n",
        "            numerr+=1\n",
        "            print(df.at[i,columnname],end=\", \")\n",
        "    sumall=numok+numerr\n",
        "    print(f\"\\n{sumall} mintából {numerr} db nem volt megfelelő\")\n",
        "\n",
        "\n",
        "def createcolumn(df,columnname,keys):\n",
        "    print(f\"{columnname} cseréje megy\")\n",
        "    for key in keys:\n",
        "        df[keys[key]]=0.0\n",
        "    for key in keys:\n",
        "        for i in df.index:\n",
        "            if df.at[i,columnname]==key:\n",
        "                df.at[i,keys[key]]=1.0\n",
        "\n",
        "\n",
        "def create_dict(idx,list):\n",
        "    o={}\n",
        "    for i in list:\n",
        "        o[i]=idx+\"_\"+str(i)\n",
        "    return o\n",
        "\n",
        "def create_base_data(df):\n",
        "    t=['n', 't', 'o']\n",
        "    columnname=\"land_surface_condition\"\n",
        "    key=create_dict(columnname,t)\n",
        "\n",
        "\n",
        "    #checkvalues(df,columnname,key)\n",
        "    createcolumn(df,columnname,key)\n",
        "\n",
        "    t= ['h', 'w', 'i', 'r', 'u']\n",
        "    columnname=\"foundation_type\"\n",
        "    key=create_dict(columnname,t)\n",
        "\n",
        "    #checkvalues(df,columnname,key)\n",
        "    createcolumn(df,columnname,key)\n",
        "\n",
        "    t=  ['q', 'n', 'x']\n",
        "    columnname=\"roof_type\"\n",
        "    key=create_dict(columnname,t)\n",
        "\n",
        "    #checkvalues(df,columnname,key)\n",
        "    createcolumn(df,columnname,key)\n",
        "\n",
        "    t=  ['z', 'v', 'f', 'm', 'x']\n",
        "    columnname=\"ground_floor_type\"\n",
        "    key=create_dict(columnname,t)\n",
        "\n",
        "    #checkvalues(df,columnname,key)\n",
        "    createcolumn(df,columnname,key)\n",
        "\n",
        "    t=   ['q', 's', 'j', 'x']\n",
        "    columnname=\"other_floor_type\"\n",
        "    key=create_dict(columnname,t)\n",
        "\n",
        "    #checkvalues(df,columnname,key)\n",
        "    createcolumn(df,columnname,key)\n",
        "\n",
        "    t=   ['j', 's', 't', 'o']\n",
        "    columnname=\"position\"\n",
        "    key=create_dict(columnname,t)\n",
        "\n",
        "    #checkvalues(df,columnname,key)\n",
        "    createcolumn(df,columnname,key)\n",
        "\n",
        "    t=   ['c', 's', 'f', 'd', 'm', 'a', 'q', 'u', 'n', 'o']\n",
        "    columnname=\"plan_configuration\"\n",
        "    key=create_dict(columnname,t)\n",
        "\n",
        "    #checkvalues(df,columnname,key)\n",
        "    createcolumn(df,columnname,key)\n",
        "\n",
        "    t=['a', 'w', 'r', 'v']\n",
        "    columnname=\"legal_ownership_status\"\n",
        "    key=create_dict(columnname,t)\n",
        "\n",
        "    #checkvalues(df,columnname,key)\n",
        "    createcolumn(df,columnname,key)\n",
        "\n",
        "    # level 1: 0-30, level 2: 0-1427, level 3: 0-12567.\n",
        "    # level1:0--30\n",
        "    # level2: 0.0000-----------0.9999\n",
        "    # level3: 0.000000000------0.000099999\n",
        "    '''\n",
        "    l1=df.geo_level_1_id\n",
        "    l2=(df.geo_level_2_id/1427*9999)/10000\n",
        "    l3=(df.geo_level_2_id/12567*99999)/1000000000\n",
        "    df[\"geopos\"]=l1+l2+l3\n",
        "    '''\n",
        "    return(df)\n",
        "\n",
        "\n",
        "\n",
        "def kill_columns(df):\n",
        "    notkey=[\"Unnamed: 0\",\"building_id\",\"legal_ownership_status\", \"land_surface_condition\",\t\"foundation_type\",\t\"roof_type\",\t\"ground_floor_type\",\t\"other_floor_type\",\t\"position\",\t\"plan_configuration\"]\n",
        "    for i in df.columns:\n",
        "        #print(i)\n",
        "        if i in notkey:\n",
        "            df.drop(columns=[i], inplace=True)\n",
        "    return df\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [],
      "source": [
        "def check_similarity_np(np1,np2):\n",
        "    db=0\n",
        "    maxi=0\n",
        "    for ind, i1 in enumerate(np1):\n",
        "        maxi+=1\n",
        "        i2=np2[ind]\n",
        "        if i1!=i2:\n",
        "            db+=1\n",
        "    #print(f\"{ind}. eset:  {i:3},{i2:3}\")\n",
        "    print(f\"hiba:{db} max:{maxi} -- error:{db/maxi*100.0 : 2.6} good %:{100-db/maxi*100.0 : 2.6} %\")\n",
        "    return(1-db/maxi)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 502
        },
        "id": "HZHYUIgKIlT9",
        "outputId": "3740844a-f48c-4b06-87fd-f1eddb0431ab"
      },
      "source": [
        "\n",
        "features_train=basedir+\"/orig/train_values.csv\"\n",
        "labels_train=basedir+\"/orig/train_labels.csv\"\n",
        "features_predict=basedir+\"/orig/test_values.csv\"\n",
        "X_train=pd.read_csv(features_train)\n",
        "X_pred=pd.read_csv(features_predict)\n",
        "y_train=pd.read_csv(labels_train)\n",
        "\n"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['building_id', 'geo_level_1_id', 'geo_level_2_id', 'geo_level_3_id',\n",
              "       'count_floors_pre_eq', 'age', 'area_percentage', 'height_percentage',\n",
              "       'land_surface_condition', 'foundation_type', 'roof_type',\n",
              "       'ground_floor_type', 'other_floor_type', 'position',\n",
              "       'plan_configuration', 'has_superstructure_adobe_mud',\n",
              "       'has_superstructure_mud_mortar_stone', 'has_superstructure_stone_flag',\n",
              "       'has_superstructure_cement_mortar_stone',\n",
              "       'has_superstructure_mud_mortar_brick',\n",
              "       'has_superstructure_cement_mortar_brick', 'has_superstructure_timber',\n",
              "       'has_superstructure_bamboo', 'has_superstructure_rc_non_engineered',\n",
              "       'has_superstructure_rc_engineered', 'has_superstructure_other',\n",
              "       'legal_ownership_status', 'count_families', 'has_secondary_use',\n",
              "       'has_secondary_use_agriculture', 'has_secondary_use_hotel',\n",
              "       'has_secondary_use_rental', 'has_secondary_use_institution',\n",
              "       'has_secondary_use_school', 'has_secondary_use_industry',\n",
              "       'has_secondary_use_health_post', 'has_secondary_use_gov_office',\n",
              "       'has_secondary_use_use_police', 'has_secondary_use_other'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ],
      "source": [
        "X_train.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [],
      "source": [
        "added=pd.concat([X_train.geo_level_1_id,X_train.geo_level_2_id,X_train.geo_level_3_id,y_train.damage_grade], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [],
      "source": [
        "#added"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<Figure size 432x288 with 1 Axes>",
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<!-- Created with matplotlib (https://matplotlib.org/) -->\r\n<svg height=\"248.518125pt\" version=\"1.1\" viewBox=\"0 0 388.0125 248.518125\" width=\"388.0125pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <metadata>\r\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\r\n   <cc:Work>\r\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\r\n    <dc:date>2020-12-31T17:26:56.121971</dc:date>\r\n    <dc:format>image/svg+xml</dc:format>\r\n    <dc:creator>\r\n     <cc:Agent>\r\n      <dc:title>Matplotlib v3.3.3, https://matplotlib.org/</dc:title>\r\n     </cc:Agent>\r\n    </dc:creator>\r\n   </cc:Work>\r\n  </rdf:RDF>\r\n </metadata>\r\n <defs>\r\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M 0 248.518125 \r\nL 388.0125 248.518125 \r\nL 388.0125 0 \r\nL 0 0 \r\nz\r\n\" style=\"fill:none;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g id=\"patch_2\">\r\n    <path d=\"M 46.0125 224.64 \r\nL 380.8125 224.64 \r\nL 380.8125 7.2 \r\nL 46.0125 7.2 \r\nz\r\n\" style=\"fill:#ffffff;\"/>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path clip-path=\"url(#pda7d4a762b)\" d=\"M 61.230682 224.64 \r\nL 71.048864 224.64 \r\nL 71.048864 217.598028 \r\nL 61.230682 217.598028 \r\nz\r\n\" style=\"fill:#1f77b4;\"/>\r\n   </g>\r\n   <g id=\"patch_4\">\r\n    <path clip-path=\"url(#pda7d4a762b)\" d=\"M 71.048864 224.64 \r\nL 80.867045 224.64 \r\nL 80.867045 221.054355 \r\nL 71.048864 221.054355 \r\nz\r\n\" style=\"fill:#1f77b4;\"/>\r\n   </g>\r\n   <g id=\"patch_5\">\r\n    <path clip-path=\"url(#pda7d4a762b)\" d=\"M 80.867045 224.64 \r\nL 90.685227 224.64 \r\nL 90.685227 221.865533 \r\nL 80.867045 221.865533 \r\nz\r\n\" style=\"fill:#1f77b4;\"/>\r\n   </g>\r\n   <g id=\"patch_6\">\r\n    <path clip-path=\"url(#pda7d4a762b)\" d=\"M 90.685227 224.64 \r\nL 100.503409 224.64 \r\nL 100.503409 192.369192 \r\nL 90.685227 192.369192 \r\nz\r\n\" style=\"fill:#1f77b4;\"/>\r\n   </g>\r\n   <g id=\"patch_7\">\r\n    <path clip-path=\"url(#pda7d4a762b)\" d=\"M 100.503409 224.64 \r\nL 110.321591 224.64 \r\nL 110.321591 190.746834 \r\nL 100.503409 190.746834 \r\nz\r\n\" style=\"fill:#1f77b4;\"/>\r\n   </g>\r\n   <g id=\"patch_8\">\r\n    <path clip-path=\"url(#pda7d4a762b)\" d=\"M 110.321591 224.64 \r\nL 120.139773 224.64 \r\nL 120.139773 221.936071 \r\nL 110.321591 221.936071 \r\nz\r\n\" style=\"fill:#1f77b4;\"/>\r\n   </g>\r\n   <g id=\"patch_9\">\r\n    <path clip-path=\"url(#pda7d4a762b)\" d=\"M 120.139773 224.64 \r\nL 129.957955 224.64 \r\nL 129.957955 153.503147 \r\nL 120.139773 153.503147 \r\nz\r\n\" style=\"fill:#1f77b4;\"/>\r\n   </g>\r\n   <g id=\"patch_10\">\r\n    <path clip-path=\"url(#pda7d4a762b)\" d=\"M 129.957955 224.64 \r\nL 139.776136 224.64 \r\nL 139.776136 146.014439 \r\nL 129.957955 146.014439 \r\nz\r\n\" style=\"fill:#1f77b4;\"/>\r\n   </g>\r\n   <g id=\"patch_11\">\r\n    <path clip-path=\"url(#pda7d4a762b)\" d=\"M 139.776136 224.64 \r\nL 149.594318 224.64 \r\nL 149.594318 108.100648 \r\nL 139.776136 108.100648 \r\nz\r\n\" style=\"fill:#1f77b4;\"/>\r\n   </g>\r\n   <g id=\"patch_12\">\r\n    <path clip-path=\"url(#pda7d4a762b)\" d=\"M 149.594318 224.64 \r\nL 159.4125 224.64 \r\nL 159.4125 216.833874 \r\nL 149.594318 216.833874 \r\nz\r\n\" style=\"fill:#1f77b4;\"/>\r\n   </g>\r\n   <g id=\"patch_13\">\r\n    <path clip-path=\"url(#pda7d4a762b)\" d=\"M 159.4125 224.64 \r\nL 169.230682 224.64 \r\nL 169.230682 121.643807 \r\nL 159.4125 121.643807 \r\nz\r\n\" style=\"fill:#1f77b4;\"/>\r\n   </g>\r\n   <g id=\"patch_14\">\r\n    <path clip-path=\"url(#pda7d4a762b)\" d=\"M 169.230682 224.64 \r\nL 179.048864 224.64 \r\nL 179.048864 187.46685 \r\nL 169.230682 187.46685 \r\nz\r\n\" style=\"fill:#1f77b4;\"/>\r\n   </g>\r\n   <g id=\"patch_15\">\r\n    <path clip-path=\"url(#pda7d4a762b)\" d=\"M 179.048864 224.64 \r\nL 188.867045 224.64 \r\nL 188.867045 216.586993 \r\nL 179.048864 216.586993 \r\nz\r\n\" style=\"fill:#1f77b4;\"/>\r\n   </g>\r\n   <g id=\"patch_16\">\r\n    <path clip-path=\"url(#pda7d4a762b)\" d=\"M 188.867045 224.64 \r\nL 198.685227 224.64 \r\nL 198.685227 208.569255 \r\nL 188.867045 208.569255 \r\nz\r\n\" style=\"fill:#1f77b4;\"/>\r\n   </g>\r\n   <g id=\"patch_17\">\r\n    <path clip-path=\"url(#pda7d4a762b)\" d=\"M 198.685227 224.64 \r\nL 208.503409 224.64 \r\nL 208.503409 221.395285 \r\nL 198.685227 221.395285 \r\nz\r\n\" style=\"fill:#1f77b4;\"/>\r\n   </g>\r\n   <g id=\"patch_18\">\r\n    <path clip-path=\"url(#pda7d4a762b)\" d=\"M 208.503409 224.64 \r\nL 218.321591 224.64 \r\nL 218.321591 218.949992 \r\nL 208.503409 218.949992 \r\nz\r\n\" style=\"fill:#1f77b4;\"/>\r\n   </g>\r\n   <g id=\"patch_19\">\r\n    <path clip-path=\"url(#pda7d4a762b)\" d=\"M 218.321591 224.64 \r\nL 228.139773 224.64 \r\nL 228.139773 213.542134 \r\nL 218.321591 213.542134 \r\nz\r\n\" style=\"fill:#1f77b4;\"/>\r\n   </g>\r\n   <g id=\"patch_20\">\r\n    <path clip-path=\"url(#pda7d4a762b)\" d=\"M 228.139773 224.64 \r\nL 237.957955 224.64 \r\nL 237.957955 17.554286 \r\nL 228.139773 17.554286 \r\nz\r\n\" style=\"fill:#1f77b4;\"/>\r\n   </g>\r\n   <g id=\"patch_21\">\r\n    <path clip-path=\"url(#pda7d4a762b)\" d=\"M 237.957955 224.64 \r\nL 247.776136 224.64 \r\nL 247.776136 197.236265 \r\nL 237.957955 197.236265 \r\nz\r\n\" style=\"fill:#1f77b4;\"/>\r\n   </g>\r\n   <g id=\"patch_22\">\r\n    <path clip-path=\"url(#pda7d4a762b)\" d=\"M 247.776136 224.64 \r\nL 257.594318 224.64 \r\nL 257.594318 223.86409 \r\nL 247.776136 223.86409 \r\nz\r\n\" style=\"fill:#1f77b4;\"/>\r\n   </g>\r\n   <g id=\"patch_23\">\r\n    <path clip-path=\"url(#pda7d4a762b)\" d=\"M 257.594318 224.64 \r\nL 267.4125 224.64 \r\nL 267.4125 200.598542 \r\nL 257.594318 200.598542 \r\nz\r\n\" style=\"fill:#1f77b4;\"/>\r\n   </g>\r\n   <g id=\"patch_24\">\r\n    <path clip-path=\"url(#pda7d4a762b)\" d=\"M 267.4125 224.64 \r\nL 277.230682 224.64 \r\nL 277.230682 122.243374 \r\nL 267.4125 122.243374 \r\nz\r\n\" style=\"fill:#1f77b4;\"/>\r\n   </g>\r\n   <g id=\"patch_25\">\r\n    <path clip-path=\"url(#pda7d4a762b)\" d=\"M 277.230682 224.64 \r\nL 287.048864 224.64 \r\nL 287.048864 215.035173 \r\nL 277.230682 215.035173 \r\nz\r\n\" style=\"fill:#1f77b4;\"/>\r\n   </g>\r\n   <g id=\"patch_26\">\r\n    <path clip-path=\"url(#pda7d4a762b)\" d=\"M 287.048864 224.64 \r\nL 296.867045 224.64 \r\nL 296.867045 221.336504 \r\nL 287.048864 221.336504 \r\nz\r\n\" style=\"fill:#1f77b4;\"/>\r\n   </g>\r\n   <g id=\"patch_27\">\r\n    <path clip-path=\"url(#pda7d4a762b)\" d=\"M 296.867045 224.64 \r\nL 306.685227 224.64 \r\nL 306.685227 223.08818 \r\nL 296.867045 223.08818 \r\nz\r\n\" style=\"fill:#1f77b4;\"/>\r\n   </g>\r\n   <g id=\"patch_28\">\r\n    <path clip-path=\"url(#pda7d4a762b)\" d=\"M 306.685227 224.64 \r\nL 316.503409 224.64 \r\nL 316.503409 215.564203 \r\nL 306.685227 215.564203 \r\nz\r\n\" style=\"fill:#1f77b4;\"/>\r\n   </g>\r\n   <g id=\"patch_29\">\r\n    <path clip-path=\"url(#pda7d4a762b)\" d=\"M 316.503409 224.64 \r\nL 326.321591 224.64 \r\nL 326.321591 201.809432 \r\nL 316.503409 201.809432 \r\nz\r\n\" style=\"fill:#1f77b4;\"/>\r\n   </g>\r\n   <g id=\"patch_30\">\r\n    <path clip-path=\"url(#pda7d4a762b)\" d=\"M 326.321591 224.64 \r\nL 336.139773 224.64 \r\nL 336.139773 153.397342 \r\nL 326.321591 153.397342 \r\nz\r\n\" style=\"fill:#1f77b4;\"/>\r\n   </g>\r\n   <g id=\"patch_31\">\r\n    <path clip-path=\"url(#pda7d4a762b)\" d=\"M 336.139773 224.64 \r\nL 345.957955 224.64 \r\nL 345.957955 223.370329 \r\nL 336.139773 223.370329 \r\nz\r\n\" style=\"fill:#1f77b4;\"/>\r\n   </g>\r\n   <g id=\"patch_32\">\r\n    <path clip-path=\"url(#pda7d4a762b)\" d=\"M 345.957955 224.64 \r\nL 355.776136 224.64 \r\nL 355.776136 224.181508 \r\nL 345.957955 224.181508 \r\nz\r\n\" style=\"fill:#1f77b4;\"/>\r\n   </g>\r\n   <g id=\"patch_33\">\r\n    <path clip-path=\"url(#pda7d4a762b)\" d=\"M 355.776136 224.64 \r\nL 365.594318 224.64 \r\nL 365.594318 221.030842 \r\nL 355.776136 221.030842 \r\nz\r\n\" style=\"fill:#1f77b4;\"/>\r\n   </g>\r\n   <g id=\"matplotlib.axis_1\">\r\n    <g id=\"xtick_1\">\r\n     <g id=\"line2d_1\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 0 3.5 \r\n\" id=\"m80692fb8a1\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"61.230682\" xlink:href=\"#m80692fb8a1\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_1\">\r\n      <!-- 0 -->\r\n      <g transform=\"translate(58.049432 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 31.78125 66.40625 \r\nQ 24.171875 66.40625 20.328125 58.90625 \r\nQ 16.5 51.421875 16.5 36.375 \r\nQ 16.5 21.390625 20.328125 13.890625 \r\nQ 24.171875 6.390625 31.78125 6.390625 \r\nQ 39.453125 6.390625 43.28125 13.890625 \r\nQ 47.125 21.390625 47.125 36.375 \r\nQ 47.125 51.421875 43.28125 58.90625 \r\nQ 39.453125 66.40625 31.78125 66.40625 \r\nz\r\nM 31.78125 74.21875 \r\nQ 44.046875 74.21875 50.515625 64.515625 \r\nQ 56.984375 54.828125 56.984375 36.375 \r\nQ 56.984375 17.96875 50.515625 8.265625 \r\nQ 44.046875 -1.421875 31.78125 -1.421875 \r\nQ 19.53125 -1.421875 13.0625 8.265625 \r\nQ 6.59375 17.96875 6.59375 36.375 \r\nQ 6.59375 54.828125 13.0625 64.515625 \r\nQ 19.53125 74.21875 31.78125 74.21875 \r\nz\r\n\" id=\"DejaVuSans-48\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_2\">\r\n     <g id=\"line2d_2\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"111.957955\" xlink:href=\"#m80692fb8a1\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_2\">\r\n      <!-- 5 -->\r\n      <g transform=\"translate(108.776705 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 10.796875 72.90625 \r\nL 49.515625 72.90625 \r\nL 49.515625 64.59375 \r\nL 19.828125 64.59375 \r\nL 19.828125 46.734375 \r\nQ 21.96875 47.46875 24.109375 47.828125 \r\nQ 26.265625 48.1875 28.421875 48.1875 \r\nQ 40.625 48.1875 47.75 41.5 \r\nQ 54.890625 34.8125 54.890625 23.390625 \r\nQ 54.890625 11.625 47.5625 5.09375 \r\nQ 40.234375 -1.421875 26.90625 -1.421875 \r\nQ 22.3125 -1.421875 17.546875 -0.640625 \r\nQ 12.796875 0.140625 7.71875 1.703125 \r\nL 7.71875 11.625 \r\nQ 12.109375 9.234375 16.796875 8.0625 \r\nQ 21.484375 6.890625 26.703125 6.890625 \r\nQ 35.15625 6.890625 40.078125 11.328125 \r\nQ 45.015625 15.765625 45.015625 23.390625 \r\nQ 45.015625 31 40.078125 35.4375 \r\nQ 35.15625 39.890625 26.703125 39.890625 \r\nQ 22.75 39.890625 18.8125 39.015625 \r\nQ 14.890625 38.140625 10.796875 36.28125 \r\nz\r\n\" id=\"DejaVuSans-53\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_3\">\r\n     <g id=\"line2d_3\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"162.685227\" xlink:href=\"#m80692fb8a1\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_3\">\r\n      <!-- 10 -->\r\n      <g transform=\"translate(156.322727 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 12.40625 8.296875 \r\nL 28.515625 8.296875 \r\nL 28.515625 63.921875 \r\nL 10.984375 60.40625 \r\nL 10.984375 69.390625 \r\nL 28.421875 72.90625 \r\nL 38.28125 72.90625 \r\nL 38.28125 8.296875 \r\nL 54.390625 8.296875 \r\nL 54.390625 0 \r\nL 12.40625 0 \r\nz\r\n\" id=\"DejaVuSans-49\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_4\">\r\n     <g id=\"line2d_4\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"213.4125\" xlink:href=\"#m80692fb8a1\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_4\">\r\n      <!-- 15 -->\r\n      <g transform=\"translate(207.05 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_5\">\r\n     <g id=\"line2d_5\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"264.139773\" xlink:href=\"#m80692fb8a1\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_5\">\r\n      <!-- 20 -->\r\n      <g transform=\"translate(257.777273 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 19.1875 8.296875 \r\nL 53.609375 8.296875 \r\nL 53.609375 0 \r\nL 7.328125 0 \r\nL 7.328125 8.296875 \r\nQ 12.9375 14.109375 22.625 23.890625 \r\nQ 32.328125 33.6875 34.8125 36.53125 \r\nQ 39.546875 41.84375 41.421875 45.53125 \r\nQ 43.3125 49.21875 43.3125 52.78125 \r\nQ 43.3125 58.59375 39.234375 62.25 \r\nQ 35.15625 65.921875 28.609375 65.921875 \r\nQ 23.96875 65.921875 18.8125 64.3125 \r\nQ 13.671875 62.703125 7.8125 59.421875 \r\nL 7.8125 69.390625 \r\nQ 13.765625 71.78125 18.9375 73 \r\nQ 24.125 74.21875 28.421875 74.21875 \r\nQ 39.75 74.21875 46.484375 68.546875 \r\nQ 53.21875 62.890625 53.21875 53.421875 \r\nQ 53.21875 48.921875 51.53125 44.890625 \r\nQ 49.859375 40.875 45.40625 35.40625 \r\nQ 44.1875 33.984375 37.640625 27.21875 \r\nQ 31.109375 20.453125 19.1875 8.296875 \r\nz\r\n\" id=\"DejaVuSans-50\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_6\">\r\n     <g id=\"line2d_6\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"314.867045\" xlink:href=\"#m80692fb8a1\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_6\">\r\n      <!-- 25 -->\r\n      <g transform=\"translate(308.504545 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_7\">\r\n     <g id=\"line2d_7\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"365.594318\" xlink:href=\"#m80692fb8a1\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_7\">\r\n      <!-- 30 -->\r\n      <g transform=\"translate(359.231818 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 40.578125 39.3125 \r\nQ 47.65625 37.796875 51.625 33 \r\nQ 55.609375 28.21875 55.609375 21.1875 \r\nQ 55.609375 10.40625 48.1875 4.484375 \r\nQ 40.765625 -1.421875 27.09375 -1.421875 \r\nQ 22.515625 -1.421875 17.65625 -0.515625 \r\nQ 12.796875 0.390625 7.625 2.203125 \r\nL 7.625 11.71875 \r\nQ 11.71875 9.328125 16.59375 8.109375 \r\nQ 21.484375 6.890625 26.8125 6.890625 \r\nQ 36.078125 6.890625 40.9375 10.546875 \r\nQ 45.796875 14.203125 45.796875 21.1875 \r\nQ 45.796875 27.640625 41.28125 31.265625 \r\nQ 36.765625 34.90625 28.71875 34.90625 \r\nL 20.21875 34.90625 \r\nL 20.21875 43.015625 \r\nL 29.109375 43.015625 \r\nQ 36.375 43.015625 40.234375 45.921875 \r\nQ 44.09375 48.828125 44.09375 54.296875 \r\nQ 44.09375 59.90625 40.109375 62.90625 \r\nQ 36.140625 65.921875 28.71875 65.921875 \r\nQ 24.65625 65.921875 20.015625 65.03125 \r\nQ 15.375 64.15625 9.8125 62.3125 \r\nL 9.8125 71.09375 \r\nQ 15.4375 72.65625 20.34375 73.4375 \r\nQ 25.25 74.21875 29.59375 74.21875 \r\nQ 40.828125 74.21875 47.359375 69.109375 \r\nQ 53.90625 64.015625 53.90625 55.328125 \r\nQ 53.90625 49.265625 50.4375 45.09375 \r\nQ 46.96875 40.921875 40.578125 39.3125 \r\nz\r\n\" id=\"DejaVuSans-51\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-51\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_2\">\r\n    <g id=\"ytick_1\">\r\n     <g id=\"line2d_8\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL -3.5 0 \r\n\" id=\"m14fa702124\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"46.0125\" xlink:href=\"#m14fa702124\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_8\">\r\n      <!-- 0 -->\r\n      <g transform=\"translate(32.65 228.439219)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_2\">\r\n     <g id=\"line2d_9\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"46.0125\" xlink:href=\"#m14fa702124\" y=\"195.249464\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_9\">\r\n      <!-- 2500 -->\r\n      <g transform=\"translate(13.5625 199.048683)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_3\">\r\n     <g id=\"line2d_10\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"46.0125\" xlink:href=\"#m14fa702124\" y=\"165.858929\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_10\">\r\n      <!-- 5000 -->\r\n      <g transform=\"translate(13.5625 169.658147)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-53\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_4\">\r\n     <g id=\"line2d_11\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"46.0125\" xlink:href=\"#m14fa702124\" y=\"136.468393\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_11\">\r\n      <!-- 7500 -->\r\n      <g transform=\"translate(13.5625 140.267612)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 8.203125 72.90625 \r\nL 55.078125 72.90625 \r\nL 55.078125 68.703125 \r\nL 28.609375 0 \r\nL 18.3125 0 \r\nL 43.21875 64.59375 \r\nL 8.203125 64.59375 \r\nz\r\n\" id=\"DejaVuSans-55\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-55\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_5\">\r\n     <g id=\"line2d_12\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"46.0125\" xlink:href=\"#m14fa702124\" y=\"107.077857\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_12\">\r\n      <!-- 10000 -->\r\n      <g transform=\"translate(7.2 110.877076)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_6\">\r\n     <g id=\"line2d_13\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"46.0125\" xlink:href=\"#m14fa702124\" y=\"77.687322\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_13\">\r\n      <!-- 12500 -->\r\n      <g transform=\"translate(7.2 81.48654)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-53\"/>\r\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_7\">\r\n     <g id=\"line2d_14\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"46.0125\" xlink:href=\"#m14fa702124\" y=\"48.296786\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_14\">\r\n      <!-- 15000 -->\r\n      <g transform=\"translate(7.2 52.096005)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_8\">\r\n     <g id=\"line2d_15\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"46.0125\" xlink:href=\"#m14fa702124\" y=\"18.90625\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_15\">\r\n      <!-- 17500 -->\r\n      <g transform=\"translate(7.2 22.705469)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-55\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-53\"/>\r\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"patch_34\">\r\n    <path d=\"M 46.0125 224.64 \r\nL 46.0125 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_35\">\r\n    <path d=\"M 380.8125 224.64 \r\nL 380.8125 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_36\">\r\n    <path d=\"M 46.0125 224.64 \r\nL 380.8125 224.64 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_37\">\r\n    <path d=\"M 46.0125 7.2 \r\nL 380.8125 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"pda7d4a762b\">\r\n   <rect height=\"217.44\" width=\"334.8\" x=\"46.0125\" y=\"7.2\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "a=plt.hist(added.geo_level_1_id.where(added.damage_grade==3),bins=31)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "31"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ],
      "source": [
        "len(a[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [],
      "source": [
        "quake_effect={}\n",
        "maxi=max(a[0])\n",
        "for inx,val in enumerate(a[0]):\n",
        "    quake_effect[inx]=1-(maxi-val)/maxi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [],
      "source": [
        "eartquakeDist=[]\n",
        "for idx,i in enumerate(X_train.geo_level_1_id):\n",
        "    eartquakeDist.append(quake_effect[X_train.geo_level_1_id[idx]])\n",
        "\n",
        "eartquakeDist2=[]\n",
        "for idx,i in enumerate(X_pred.geo_level_1_id):\n",
        "    eartquakeDist2.append(quake_effect[X_pred.geo_level_1_id[idx]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ],
      "source": [
        "eartquakeDist2[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [],
      "source": [
        "added2=pd.concat([X_train.geo_level_1_id,X_train.geo_level_2_id,X_train.geo_level_3_id,y_train.damage_grade,pd.DataFrame(eartquakeDist,columns=[\"QuakeForce\"])], axis=1)\n",
        "\n",
        "#X_train_geolevel=pd.concat([X_train,pd.DataFrame(eartquakeDist,columns=[\"QuakeForce\"])], axis=1)\n",
        "#X_pred_geolevel=pd.concat([X_pred,pd.DataFrame(eartquakeDist2,columns=[\"QuakeForce\"])], axis=1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "land_surface_condition cseréje megy\n",
            "foundation_type cseréje megy\n",
            "roof_type cseréje megy\n",
            "ground_floor_type cseréje megy\n",
            "other_floor_type cseréje megy\n",
            "position cseréje megy\n",
            "plan_configuration cseréje megy\n",
            "legal_ownership_status cseréje megy\n",
            "land_surface_condition cseréje megy\n",
            "foundation_type cseréje megy\n",
            "roof_type cseréje megy\n",
            "ground_floor_type cseréje megy\n",
            "other_floor_type cseréje megy\n",
            "position cseréje megy\n",
            "plan_configuration cseréje megy\n",
            "legal_ownership_status cseréje megy\n"
          ]
        }
      ],
      "source": [
        "X_pred_conv=create_base_data(X_pred)\n",
        "X_train_conv=create_base_data(X_train)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [],
      "source": [
        "#--------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   geo_level_1_id  geo_level_2_id  geo_level_3_id  count_floors_pre_eq  age  \\\n",
              "0              17             596           11307                    3   20   \n",
              "1               6             141           11987                    2   25   \n",
              "2              22              19           10044                    2    5   \n",
              "3              26              39             633                    1    0   \n",
              "4              17             289            7970                    3   15   \n",
              "\n",
              "   area_percentage  height_percentage  has_superstructure_adobe_mud  \\\n",
              "0                7                  6                             0   \n",
              "1               13                  5                             0   \n",
              "2                4                  5                             0   \n",
              "3               19                  3                             0   \n",
              "4                8                  7                             0   \n",
              "\n",
              "   has_superstructure_mud_mortar_stone  has_superstructure_stone_flag  ...  \\\n",
              "0                                    1                              0  ...   \n",
              "1                                    1                              0  ...   \n",
              "2                                    1                              0  ...   \n",
              "3                                    0                              0  ...   \n",
              "4                                    1                              0  ...   \n",
              "\n",
              "   plan_configuration_m  plan_configuration_a  plan_configuration_q  \\\n",
              "0                   0.0                   0.0                   0.0   \n",
              "1                   0.0                   0.0                   0.0   \n",
              "2                   0.0                   0.0                   0.0   \n",
              "3                   0.0                   0.0                   0.0   \n",
              "4                   0.0                   0.0                   0.0   \n",
              "\n",
              "   plan_configuration_u  plan_configuration_n  plan_configuration_o  \\\n",
              "0                   0.0                   0.0                   0.0   \n",
              "1                   0.0                   0.0                   0.0   \n",
              "2                   0.0                   0.0                   0.0   \n",
              "3                   0.0                   0.0                   0.0   \n",
              "4                   0.0                   0.0                   0.0   \n",
              "\n",
              "   legal_ownership_status_a  legal_ownership_status_w  \\\n",
              "0                       0.0                       0.0   \n",
              "1                       0.0                       0.0   \n",
              "2                       0.0                       0.0   \n",
              "3                       0.0                       0.0   \n",
              "4                       0.0                       0.0   \n",
              "\n",
              "   legal_ownership_status_r  legal_ownership_status_v  \n",
              "0                       0.0                       1.0  \n",
              "1                       0.0                       1.0  \n",
              "2                       0.0                       1.0  \n",
              "3                       0.0                       1.0  \n",
              "4                       0.0                       1.0  \n",
              "\n",
              "[5 rows x 68 columns]"
            ],
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>geo_level_1_id</th>\n      <th>geo_level_2_id</th>\n      <th>geo_level_3_id</th>\n      <th>count_floors_pre_eq</th>\n      <th>age</th>\n      <th>area_percentage</th>\n      <th>height_percentage</th>\n      <th>has_superstructure_adobe_mud</th>\n      <th>has_superstructure_mud_mortar_stone</th>\n      <th>has_superstructure_stone_flag</th>\n      <th>...</th>\n      <th>plan_configuration_m</th>\n      <th>plan_configuration_a</th>\n      <th>plan_configuration_q</th>\n      <th>plan_configuration_u</th>\n      <th>plan_configuration_n</th>\n      <th>plan_configuration_o</th>\n      <th>legal_ownership_status_a</th>\n      <th>legal_ownership_status_w</th>\n      <th>legal_ownership_status_r</th>\n      <th>legal_ownership_status_v</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>17</td>\n      <td>596</td>\n      <td>11307</td>\n      <td>3</td>\n      <td>20</td>\n      <td>7</td>\n      <td>6</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>6</td>\n      <td>141</td>\n      <td>11987</td>\n      <td>2</td>\n      <td>25</td>\n      <td>13</td>\n      <td>5</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>22</td>\n      <td>19</td>\n      <td>10044</td>\n      <td>2</td>\n      <td>5</td>\n      <td>4</td>\n      <td>5</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>26</td>\n      <td>39</td>\n      <td>633</td>\n      <td>1</td>\n      <td>0</td>\n      <td>19</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>17</td>\n      <td>289</td>\n      <td>7970</td>\n      <td>3</td>\n      <td>15</td>\n      <td>8</td>\n      <td>7</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 68 columns</p>\n</div>"
          },
          "metadata": {},
          "execution_count": 60
        }
      ],
      "source": [
        "#\n",
        "X_train_ok=kill_columns(X_train_conv)\n",
        "X_pred_ok=kill_columns(X_pred_conv)\n",
        "y_train_ok=kill_columns(y_train)\n",
        "\n",
        "\n",
        "\n",
        "X_train_ok.to_csv(basedir+\"/tmp/X_tran_ok.csv\",index=False)\n",
        "X_pred_ok.to_csv(basedir+\"/tmp/X_pred_ok.csv\",index=False)\n",
        "y_train_ok.to_csv(basedir+\"/tmp/y_train_ok.csv\",index=False)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#f=open(basedir+\"/tmp/similarity.csv\",\"a\")\n",
        "X_train_ok=pd.read_csv(basedir+\"/tmp/X_tran_ok.csv\",)\n",
        "X_pred_ok=pd.read_csv(basedir+\"/tmp/X_pred_ok.csv\")\n",
        "y_train_ok=pd.read_csv(basedir+\"/tmp/y_train_ok.csv\")\n",
        "\n",
        "X_train_ok.head()\n",
        "X_pred_ok.head()\n",
        "#print(\"Ready\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZkluH7RwmL_M"
      },
      "source": [
        "X_pred_ok=kill_columns(X_pred_ok)\n",
        "X_train_ok=kill_columns(X_train_ok)\n",
        "y_train_ok=kill_columns(y_train_ok)\n",
        "\n",
        "t=[1, 2, 3]\n",
        "columnname=\"damage_grade\"\n",
        "key=create_dict(columnname,t)\n",
        "\n",
        "\n",
        "checkvalues(y_train_ok,columnname,key)\n",
        "createcolumn(y_train_ok,columnname,key)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "damage_grade ellenőrzése !\n",
            "\n",
            "260601 mintából 0 db nem volt megfelelő\n",
            "damage_grade cseréje megy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        damage_grade  damage_grade_1  damage_grade_2  damage_grade_3\n",
              "0                  3             0.0             0.0             1.0\n",
              "1                  2             0.0             1.0             0.0\n",
              "2                  3             0.0             0.0             1.0\n",
              "3                  2             0.0             1.0             0.0\n",
              "4                  3             0.0             0.0             1.0\n",
              "...              ...             ...             ...             ...\n",
              "260596             2             0.0             1.0             0.0\n",
              "260597             3             0.0             0.0             1.0\n",
              "260598             3             0.0             0.0             1.0\n",
              "260599             2             0.0             1.0             0.0\n",
              "260600             3             0.0             0.0             1.0\n",
              "\n",
              "[260601 rows x 4 columns]"
            ],
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>damage_grade</th>\n      <th>damage_grade_1</th>\n      <th>damage_grade_2</th>\n      <th>damage_grade_3</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>3</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>3</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>260596</th>\n      <td>2</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>260597</th>\n      <td>3</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>260598</th>\n      <td>3</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>260599</th>\n      <td>2</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>260600</th>\n      <td>3</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>260601 rows × 4 columns</p>\n</div>"
          },
          "metadata": {},
          "execution_count": 62
        }
      ],
      "source": [
        "y_train_ok"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q0rn-7TBQfRp"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "\n",
        "scaler2=MinMaxScaler()\n",
        "X_train_scale=scaler2.fit_transform(X_train_ok)\n",
        "\n",
        "#scaler1=StandardScaler()\n",
        "X_pred_scale=scaler2.fit_transform(X_pred_ok)\n",
        "\"\"\"\n",
        "scaler3=StandardScaler()\n",
        "y_train_scale=scaler3.fit_transform(y_train_ok)\n",
        "\"\"\"\n",
        "y_train_np=y_train_ok.to_numpy()\n",
        "\n",
        "# szétszedjük a train és test részekre\n",
        "from sklearn.model_selection import train_test_split\n",
        "#X_train_train, X_train_test,y_train_train, y_train_test  = train_test_split( X_train_scale, y_train_scale, test_size=0.10, random_state=0)\n",
        "X_train_train, X_train_test,y_train_train, y_train_test  = train_test_split( X_train_scale, y_train_np, test_size=0.01, random_state=0)\n",
        "\n",
        "\n"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [],
      "source": [
        "def outlierDropfrom_df(df,inxlist):\n",
        "    a=df\n",
        "    out=a.drop(inxlist,axis=0)\n",
        "    return(out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [],
      "source": [
        "def outlierDropfrom_numpyarray(na,inxlist):\n",
        "    \n",
        "    out=np.delete (na ,inxlist, axis=0)\n",
        "    return(out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calc_the_range(y_train,hist):    \n",
        "    num1=sum(1 for val in y_train if val==1)\n",
        "    #print(num1)\n",
        "    num2=sum(1 for val in y_train if val==2)\n",
        "    #print(num2)\n",
        "    num3=sum(1 for val in y_train if val==3)\n",
        "    #print(num3)\n",
        "    \n",
        "    out=(num1,num1+num2,num1+num2+num3)\n",
        "    print(\"out: \" ,out)\n",
        "    steps=[]\n",
        "    for i in range(len(hist[0])):\n",
        "        print(hist[0][i], end=\"\")\n",
        "        if hist[0][i]>out[0]:\n",
        "            steps.append(i)\n",
        "            break\n",
        "    for i in range(len(hist[0])):\n",
        "        #print(hist[0][i])\n",
        "        if hist[0][i]>out[1]:\n",
        "            steps.append(i)\n",
        "            break\n",
        "    print(\"step:\",steps)\n",
        "    limits_out=(hist[1][steps[0]],hist[1][steps[1]])\n",
        "    print(\"Limits:\",limits_out)\n",
        "    return (limits_out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [],
      "source": [
        "def conv_a_floatlist(alist, range_x):\n",
        "    o=[]\n",
        "    print(\"conv:\" ,range_x)\n",
        "    for i in alist:\n",
        "        if i<range_x[0]:\n",
        "            o.append(1)\n",
        "        if range_x[0]<= i <range_x[1]:\n",
        "            o.append(2)\n",
        "        if range_x[1]<=i:\n",
        "            o.append(3)\n",
        "    return(o)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import datasets, layers, models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(260601, 68)"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ],
      "source": [
        "X_train_scale.shape\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [],
      "source": [
        "ann2 = tf.keras.models.Sequential([\n",
        "    #tf.keras.layers.InputLayer(input_shape=(67,)),\n",
        "    tf.keras.layers.Dense(268, activation='relu',kernel_initializer='glorot_uniform',input_shape=(68,)),\n",
        "    #tf.keras.layers.Dense(1340, activation='relu',kernel_initializer='normal',),\n",
        "    #tf.keras.layers.Dense(130, activation='relu',kernel_initializer='normal',),\n",
        "    #tf.keras.layers.Dense(600, activation='relu',kernel_initializer='normal',),\n",
        "    \n",
        "    # tf.keras.layers.Dense(134, activation='relu',kernel_initializer='normal',),\n",
        "    #tf.keras.layers.Dense(130, activation='relu',kernel_initializer='normal',),\n",
        "    tf.keras.layers.Dense(50, activation='relu',kernel_initializer='glorot_uniform',),\n",
        "    #tf.keras.layers.Dense(130, activation='relu',kernel_initializer='normal',),\n",
        "    #tf.keras.layers.Dense(130, activation='relu',kernel_initializer='normal',),\n",
        "    #tf.keras.layers.Dense(130, activation='relu',kernel_initializer='normal',),\n",
        "    #tf.keras.layers.Dense(130, activation='relu',kernel_initializer='normal',),\n",
        "    #tf.keras.layers.Dense(130, activation='relu',kernel_initializer='normal',),\n",
        "    #tf.keras.layers.Dense(1000, activation='relu',kernel_initializer='glorot_uniform',),\n",
        "    #tf.keras.layers.Dense(270, activation='relu',kernel_initializer='glorot_uniform',),\n",
        "    tf.keras.layers.Dense(268, activation='relu',kernel_initializer='glorot_uniform',),\n",
        "    tf.keras.layers.Dense(3,activation='sigmoid', kernel_initializer='glorot_uniform',bias_initializer='ones')\n",
        "])\n",
        "\n",
        "optimizer=tf.keras.optimizers.RMSprop(0.001)\n",
        "optimizer=tf.keras.optimizers.Adam()\n",
        "\n",
        "Ytt=y_train_np[:,1:]\n",
        "ann2.compile( loss=\"categorical_crossentropy\",  optimizer=optimizer,metrics=[\"accuracy\"])\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 163,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "LR=LogisticRegression(max_iter=2000)\n",
        "LR.fit(X_train_train,y_train_train[:,1])\n",
        "LR_score2=LR.score(X_train_test,y_train_test[:,1] )\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 164,
      "metadata": {},
      "outputs": [],
      "source": [
        "LR_res=[]\n",
        "for i in range(1,4):\n",
        "    LR=LogisticRegression(max_iter=2000)\n",
        "    LR.fit(X_train_train,y_train_train[:,i])\n",
        "    LR_score=LR.score(X_train_test,y_train_test[:,i] )    \n",
        "    LR_res.append(LR_score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 168,
      "metadata": {},
      "outputs": [],
      "source": [
        "DT_res=[]\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "for i in range(1,4):\n",
        "    DT=DecisionTreeClassifier()\n",
        "    DT.fit(X_train_train,y_train_train[:,i])\n",
        "    DT_score=DT.score(X_train_test,y_train_test[:,i] )    \n",
        "    DT_res.append(DT_score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 185,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "\n",
        "LD_res=[]\n",
        "for i in range(1,4):\n",
        "    LD=LinearDiscriminantAnalysis()\n",
        "    LD.fit(X_train_train,y_train_train[:,i])\n",
        "    LD_score=LD.score(X_train_test,y_train_test[:,i] )    \n",
        "    LD_res.append(LD_score)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 189,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "GNB_res=[]\n",
        "for i in range(1,4):\n",
        "    GNB=GaussianNB()\n",
        "    GNB.fit(X_train_train,y_train_train[:,i])\n",
        "    GNB_score=GNB.score(X_train_test,y_train_test[:,i] )    \n",
        "    GNB_res.append(GNB_score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "KN_res=[]\n",
        "for i in range(1,4):\n",
        "    KN=KNeighborsClassifier()\n",
        "    KN.fit(X_train_train,y_train_train[:,i])\n",
        "    KN_score=KN.score(X_train_test,y_train_test[:,i] )    \n",
        "    KN_res.append(KN_score)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 173,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.9071729957805907, 0.5972382048331415, 0.6896816263904871]"
            ]
          },
          "metadata": {},
          "execution_count": 173
        }
      ],
      "source": [
        "LR_res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 188,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<Figure size 432x288 with 1 Axes>",
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<!-- Created with matplotlib (https://matplotlib.org/) -->\r\n<svg height=\"248.518125pt\" version=\"1.1\" viewBox=\"0 0 373.309375 248.518125\" width=\"373.309375pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <metadata>\r\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\r\n   <cc:Work>\r\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\r\n    <dc:date>2020-12-31T21:29:16.832851</dc:date>\r\n    <dc:format>image/svg+xml</dc:format>\r\n    <dc:creator>\r\n     <cc:Agent>\r\n      <dc:title>Matplotlib v3.3.3, https://matplotlib.org/</dc:title>\r\n     </cc:Agent>\r\n    </dc:creator>\r\n   </cc:Work>\r\n  </rdf:RDF>\r\n </metadata>\r\n <defs>\r\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M 0 248.518125 \r\nL 373.309375 248.518125 \r\nL 373.309375 0 \r\nL 0 0 \r\nz\r\n\" style=\"fill:none;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g id=\"patch_2\">\r\n    <path d=\"M 30.103125 224.64 \r\nL 364.903125 224.64 \r\nL 364.903125 7.2 \r\nL 30.103125 7.2 \r\nz\r\n\" style=\"fill:#ffffff;\"/>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path clip-path=\"url(#p37a26a498d)\" d=\"M 58.003125 224.64 \r\nL 71.953125 224.64 \r\nL 71.953125 17.554286 \r\nL 58.003125 17.554286 \r\nz\r\n\" style=\"fill:#1f77b4;\"/>\r\n   </g>\r\n   <g id=\"patch_4\">\r\n    <path clip-path=\"url(#p37a26a498d)\" d=\"M 169.603125 224.64 \r\nL 183.553125 224.64 \r\nL 183.553125 88.304923 \r\nL 169.603125 88.304923 \r\nz\r\n\" style=\"fill:#1f77b4;\"/>\r\n   </g>\r\n   <g id=\"patch_5\">\r\n    <path clip-path=\"url(#p37a26a498d)\" d=\"M 281.203125 224.64 \r\nL 295.153125 224.64 \r\nL 295.153125 67.20232 \r\nL 281.203125 67.20232 \r\nz\r\n\" style=\"fill:#1f77b4;\"/>\r\n   </g>\r\n   <g id=\"patch_6\">\r\n    <path clip-path=\"url(#p37a26a498d)\" d=\"M 71.953125 224.64 \r\nL 85.903125 224.64 \r\nL 85.903125 19.042851 \r\nL 71.953125 19.042851 \r\nz\r\n\" style=\"fill:#ff7f0e;\"/>\r\n   </g>\r\n   <g id=\"patch_7\">\r\n    <path clip-path=\"url(#p37a26a498d)\" d=\"M 183.553125 224.64 \r\nL 197.503125 224.64 \r\nL 197.503125 71.317765 \r\nL 183.553125 71.317765 \r\nz\r\n\" style=\"fill:#ff7f0e;\"/>\r\n   </g>\r\n   <g id=\"patch_8\">\r\n    <path clip-path=\"url(#p37a26a498d)\" d=\"M 295.153125 224.64 \r\nL 309.103125 224.64 \r\nL 309.103125 54.155482 \r\nL 295.153125 54.155482 \r\nz\r\n\" style=\"fill:#ff7f0e;\"/>\r\n   </g>\r\n   <g id=\"patch_9\">\r\n    <path clip-path=\"url(#p37a26a498d)\" d=\"M 85.903125 224.64 \r\nL 99.853125 224.64 \r\nL 99.853125 18.429912 \r\nL 85.903125 18.429912 \r\nz\r\n\" style=\"fill:#2ca02c;\"/>\r\n   </g>\r\n   <g id=\"patch_10\">\r\n    <path clip-path=\"url(#p37a26a498d)\" d=\"M 197.503125 224.64 \r\nL 211.453125 224.64 \r\nL 211.453125 87.954672 \r\nL 197.503125 87.954672 \r\nz\r\n\" style=\"fill:#2ca02c;\"/>\r\n   </g>\r\n   <g id=\"patch_11\">\r\n    <path clip-path=\"url(#p37a26a498d)\" d=\"M 309.103125 224.64 \r\nL 323.053125 224.64 \r\nL 323.053125 67.640133 \r\nL 309.103125 67.640133 \r\nz\r\n\" style=\"fill:#2ca02c;\"/>\r\n   </g>\r\n   <g id=\"patch_12\">\r\n    <path clip-path=\"url(#p37a26a498d)\" d=\"M 99.853125 224.64 \r\nL 113.803125 224.64 \r\nL 113.803125 31.301625 \r\nL 99.853125 31.301625 \r\nz\r\n\" style=\"fill:#d62728;\"/>\r\n   </g>\r\n   <g id=\"patch_13\">\r\n    <path clip-path=\"url(#p37a26a498d)\" d=\"M 211.453125 224.64 \r\nL 225.403125 224.64 \r\nL 225.403125 92.770619 \r\nL 211.453125 92.770619 \r\nz\r\n\" style=\"fill:#d62728;\"/>\r\n   </g>\r\n   <g id=\"patch_14\">\r\n    <path clip-path=\"url(#p37a26a498d)\" d=\"M 323.053125 224.64 \r\nL 337.003125 224.64 \r\nL 337.003125 107.393585 \r\nL 323.053125 107.393585 \r\nz\r\n\" style=\"fill:#d62728;\"/>\r\n   </g>\r\n   <g id=\"matplotlib.axis_1\">\r\n    <g id=\"xtick_1\">\r\n     <g id=\"line2d_1\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 0 3.5 \r\n\" id=\"m02207c869c\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"85.903125\" xlink:href=\"#m02207c869c\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_1\">\r\n      <!-- (LR1, DT1, LD1, GNB1) -->\r\n      <g transform=\"translate(28.896875 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 31 75.875 \r\nQ 24.46875 64.65625 21.28125 53.65625 \r\nQ 18.109375 42.671875 18.109375 31.390625 \r\nQ 18.109375 20.125 21.3125 9.0625 \r\nQ 24.515625 -2 31 -13.1875 \r\nL 23.1875 -13.1875 \r\nQ 15.875 -1.703125 12.234375 9.375 \r\nQ 8.59375 20.453125 8.59375 31.390625 \r\nQ 8.59375 42.28125 12.203125 53.3125 \r\nQ 15.828125 64.359375 23.1875 75.875 \r\nz\r\n\" id=\"DejaVuSans-40\"/>\r\n        <path d=\"M 9.8125 72.90625 \r\nL 19.671875 72.90625 \r\nL 19.671875 8.296875 \r\nL 55.171875 8.296875 \r\nL 55.171875 0 \r\nL 9.8125 0 \r\nz\r\n\" id=\"DejaVuSans-76\"/>\r\n        <path d=\"M 44.390625 34.1875 \r\nQ 47.5625 33.109375 50.5625 29.59375 \r\nQ 53.5625 26.078125 56.59375 19.921875 \r\nL 66.609375 0 \r\nL 56 0 \r\nL 46.6875 18.703125 \r\nQ 43.0625 26.03125 39.671875 28.421875 \r\nQ 36.28125 30.8125 30.421875 30.8125 \r\nL 19.671875 30.8125 \r\nL 19.671875 0 \r\nL 9.8125 0 \r\nL 9.8125 72.90625 \r\nL 32.078125 72.90625 \r\nQ 44.578125 72.90625 50.734375 67.671875 \r\nQ 56.890625 62.453125 56.890625 51.90625 \r\nQ 56.890625 45.015625 53.6875 40.46875 \r\nQ 50.484375 35.9375 44.390625 34.1875 \r\nz\r\nM 19.671875 64.796875 \r\nL 19.671875 38.921875 \r\nL 32.078125 38.921875 \r\nQ 39.203125 38.921875 42.84375 42.21875 \r\nQ 46.484375 45.515625 46.484375 51.90625 \r\nQ 46.484375 58.296875 42.84375 61.546875 \r\nQ 39.203125 64.796875 32.078125 64.796875 \r\nz\r\n\" id=\"DejaVuSans-82\"/>\r\n        <path d=\"M 12.40625 8.296875 \r\nL 28.515625 8.296875 \r\nL 28.515625 63.921875 \r\nL 10.984375 60.40625 \r\nL 10.984375 69.390625 \r\nL 28.421875 72.90625 \r\nL 38.28125 72.90625 \r\nL 38.28125 8.296875 \r\nL 54.390625 8.296875 \r\nL 54.390625 0 \r\nL 12.40625 0 \r\nz\r\n\" id=\"DejaVuSans-49\"/>\r\n        <path d=\"M 11.71875 12.40625 \r\nL 22.015625 12.40625 \r\nL 22.015625 4 \r\nL 14.015625 -11.625 \r\nL 7.71875 -11.625 \r\nL 11.71875 4 \r\nz\r\n\" id=\"DejaVuSans-44\"/>\r\n        <path id=\"DejaVuSans-32\"/>\r\n        <path d=\"M 19.671875 64.796875 \r\nL 19.671875 8.109375 \r\nL 31.59375 8.109375 \r\nQ 46.6875 8.109375 53.6875 14.9375 \r\nQ 60.6875 21.78125 60.6875 36.53125 \r\nQ 60.6875 51.171875 53.6875 57.984375 \r\nQ 46.6875 64.796875 31.59375 64.796875 \r\nz\r\nM 9.8125 72.90625 \r\nL 30.078125 72.90625 \r\nQ 51.265625 72.90625 61.171875 64.09375 \r\nQ 71.09375 55.28125 71.09375 36.53125 \r\nQ 71.09375 17.671875 61.125 8.828125 \r\nQ 51.171875 0 30.078125 0 \r\nL 9.8125 0 \r\nz\r\n\" id=\"DejaVuSans-68\"/>\r\n        <path d=\"M -0.296875 72.90625 \r\nL 61.375 72.90625 \r\nL 61.375 64.59375 \r\nL 35.5 64.59375 \r\nL 35.5 0 \r\nL 25.59375 0 \r\nL 25.59375 64.59375 \r\nL -0.296875 64.59375 \r\nz\r\n\" id=\"DejaVuSans-84\"/>\r\n        <path d=\"M 59.515625 10.40625 \r\nL 59.515625 29.984375 \r\nL 43.40625 29.984375 \r\nL 43.40625 38.09375 \r\nL 69.28125 38.09375 \r\nL 69.28125 6.78125 \r\nQ 63.578125 2.734375 56.6875 0.65625 \r\nQ 49.8125 -1.421875 42 -1.421875 \r\nQ 24.90625 -1.421875 15.25 8.5625 \r\nQ 5.609375 18.5625 5.609375 36.375 \r\nQ 5.609375 54.25 15.25 64.234375 \r\nQ 24.90625 74.21875 42 74.21875 \r\nQ 49.125 74.21875 55.546875 72.453125 \r\nQ 61.96875 70.703125 67.390625 67.28125 \r\nL 67.390625 56.78125 \r\nQ 61.921875 61.421875 55.765625 63.765625 \r\nQ 49.609375 66.109375 42.828125 66.109375 \r\nQ 29.4375 66.109375 22.71875 58.640625 \r\nQ 16.015625 51.171875 16.015625 36.375 \r\nQ 16.015625 21.625 22.71875 14.15625 \r\nQ 29.4375 6.6875 42.828125 6.6875 \r\nQ 48.046875 6.6875 52.140625 7.59375 \r\nQ 56.25 8.5 59.515625 10.40625 \r\nz\r\n\" id=\"DejaVuSans-71\"/>\r\n        <path d=\"M 9.8125 72.90625 \r\nL 23.09375 72.90625 \r\nL 55.421875 11.921875 \r\nL 55.421875 72.90625 \r\nL 64.984375 72.90625 \r\nL 64.984375 0 \r\nL 51.703125 0 \r\nL 19.390625 60.984375 \r\nL 19.390625 0 \r\nL 9.8125 0 \r\nz\r\n\" id=\"DejaVuSans-78\"/>\r\n        <path d=\"M 19.671875 34.8125 \r\nL 19.671875 8.109375 \r\nL 35.5 8.109375 \r\nQ 43.453125 8.109375 47.28125 11.40625 \r\nQ 51.125 14.703125 51.125 21.484375 \r\nQ 51.125 28.328125 47.28125 31.5625 \r\nQ 43.453125 34.8125 35.5 34.8125 \r\nz\r\nM 19.671875 64.796875 \r\nL 19.671875 42.828125 \r\nL 34.28125 42.828125 \r\nQ 41.5 42.828125 45.03125 45.53125 \r\nQ 48.578125 48.25 48.578125 53.8125 \r\nQ 48.578125 59.328125 45.03125 62.0625 \r\nQ 41.5 64.796875 34.28125 64.796875 \r\nz\r\nM 9.8125 72.90625 \r\nL 35.015625 72.90625 \r\nQ 46.296875 72.90625 52.390625 68.21875 \r\nQ 58.5 63.53125 58.5 54.890625 \r\nQ 58.5 48.1875 55.375 44.234375 \r\nQ 52.25 40.28125 46.1875 39.3125 \r\nQ 53.46875 37.75 57.5 32.78125 \r\nQ 61.53125 27.828125 61.53125 20.40625 \r\nQ 61.53125 10.640625 54.890625 5.3125 \r\nQ 48.25 0 35.984375 0 \r\nL 9.8125 0 \r\nz\r\n\" id=\"DejaVuSans-66\"/>\r\n        <path d=\"M 8.015625 75.875 \r\nL 15.828125 75.875 \r\nQ 23.140625 64.359375 26.78125 53.3125 \r\nQ 30.421875 42.28125 30.421875 31.390625 \r\nQ 30.421875 20.453125 26.78125 9.375 \r\nQ 23.140625 -1.703125 15.828125 -13.1875 \r\nL 8.015625 -13.1875 \r\nQ 14.5 -2 17.703125 9.0625 \r\nQ 20.90625 20.125 20.90625 31.390625 \r\nQ 20.90625 42.671875 17.703125 53.65625 \r\nQ 14.5 64.65625 8.015625 75.875 \r\nz\r\n\" id=\"DejaVuSans-41\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-40\"/>\r\n       <use x=\"39.013672\" xlink:href=\"#DejaVuSans-76\"/>\r\n       <use x=\"94.726562\" xlink:href=\"#DejaVuSans-82\"/>\r\n       <use x=\"164.208984\" xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"227.832031\" xlink:href=\"#DejaVuSans-44\"/>\r\n       <use x=\"259.619141\" xlink:href=\"#DejaVuSans-32\"/>\r\n       <use x=\"291.40625\" xlink:href=\"#DejaVuSans-68\"/>\r\n       <use x=\"368.408203\" xlink:href=\"#DejaVuSans-84\"/>\r\n       <use x=\"429.492188\" xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"493.115234\" xlink:href=\"#DejaVuSans-44\"/>\r\n       <use x=\"524.902344\" xlink:href=\"#DejaVuSans-32\"/>\r\n       <use x=\"556.689453\" xlink:href=\"#DejaVuSans-76\"/>\r\n       <use x=\"612.402344\" xlink:href=\"#DejaVuSans-68\"/>\r\n       <use x=\"689.404297\" xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"753.027344\" xlink:href=\"#DejaVuSans-44\"/>\r\n       <use x=\"784.814453\" xlink:href=\"#DejaVuSans-32\"/>\r\n       <use x=\"816.601562\" xlink:href=\"#DejaVuSans-71\"/>\r\n       <use x=\"894.091797\" xlink:href=\"#DejaVuSans-78\"/>\r\n       <use x=\"968.896484\" xlink:href=\"#DejaVuSans-66\"/>\r\n       <use x=\"1037.5\" xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"1101.123047\" xlink:href=\"#DejaVuSans-41\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_2\">\r\n     <g id=\"line2d_2\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"197.503125\" xlink:href=\"#m02207c869c\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_2\">\r\n      <!-- (LR2, DT2, LD2, GNB2) -->\r\n      <g transform=\"translate(140.496875 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 19.1875 8.296875 \r\nL 53.609375 8.296875 \r\nL 53.609375 0 \r\nL 7.328125 0 \r\nL 7.328125 8.296875 \r\nQ 12.9375 14.109375 22.625 23.890625 \r\nQ 32.328125 33.6875 34.8125 36.53125 \r\nQ 39.546875 41.84375 41.421875 45.53125 \r\nQ 43.3125 49.21875 43.3125 52.78125 \r\nQ 43.3125 58.59375 39.234375 62.25 \r\nQ 35.15625 65.921875 28.609375 65.921875 \r\nQ 23.96875 65.921875 18.8125 64.3125 \r\nQ 13.671875 62.703125 7.8125 59.421875 \r\nL 7.8125 69.390625 \r\nQ 13.765625 71.78125 18.9375 73 \r\nQ 24.125 74.21875 28.421875 74.21875 \r\nQ 39.75 74.21875 46.484375 68.546875 \r\nQ 53.21875 62.890625 53.21875 53.421875 \r\nQ 53.21875 48.921875 51.53125 44.890625 \r\nQ 49.859375 40.875 45.40625 35.40625 \r\nQ 44.1875 33.984375 37.640625 27.21875 \r\nQ 31.109375 20.453125 19.1875 8.296875 \r\nz\r\n\" id=\"DejaVuSans-50\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-40\"/>\r\n       <use x=\"39.013672\" xlink:href=\"#DejaVuSans-76\"/>\r\n       <use x=\"94.726562\" xlink:href=\"#DejaVuSans-82\"/>\r\n       <use x=\"164.208984\" xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"227.832031\" xlink:href=\"#DejaVuSans-44\"/>\r\n       <use x=\"259.619141\" xlink:href=\"#DejaVuSans-32\"/>\r\n       <use x=\"291.40625\" xlink:href=\"#DejaVuSans-68\"/>\r\n       <use x=\"368.408203\" xlink:href=\"#DejaVuSans-84\"/>\r\n       <use x=\"429.492188\" xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"493.115234\" xlink:href=\"#DejaVuSans-44\"/>\r\n       <use x=\"524.902344\" xlink:href=\"#DejaVuSans-32\"/>\r\n       <use x=\"556.689453\" xlink:href=\"#DejaVuSans-76\"/>\r\n       <use x=\"612.402344\" xlink:href=\"#DejaVuSans-68\"/>\r\n       <use x=\"689.404297\" xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"753.027344\" xlink:href=\"#DejaVuSans-44\"/>\r\n       <use x=\"784.814453\" xlink:href=\"#DejaVuSans-32\"/>\r\n       <use x=\"816.601562\" xlink:href=\"#DejaVuSans-71\"/>\r\n       <use x=\"894.091797\" xlink:href=\"#DejaVuSans-78\"/>\r\n       <use x=\"968.896484\" xlink:href=\"#DejaVuSans-66\"/>\r\n       <use x=\"1037.5\" xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"1101.123047\" xlink:href=\"#DejaVuSans-41\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_3\">\r\n     <g id=\"line2d_3\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"309.103125\" xlink:href=\"#m02207c869c\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_3\">\r\n      <!-- (LR3, DT3, LD3, GNB3) -->\r\n      <g transform=\"translate(252.096875 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 40.578125 39.3125 \r\nQ 47.65625 37.796875 51.625 33 \r\nQ 55.609375 28.21875 55.609375 21.1875 \r\nQ 55.609375 10.40625 48.1875 4.484375 \r\nQ 40.765625 -1.421875 27.09375 -1.421875 \r\nQ 22.515625 -1.421875 17.65625 -0.515625 \r\nQ 12.796875 0.390625 7.625 2.203125 \r\nL 7.625 11.71875 \r\nQ 11.71875 9.328125 16.59375 8.109375 \r\nQ 21.484375 6.890625 26.8125 6.890625 \r\nQ 36.078125 6.890625 40.9375 10.546875 \r\nQ 45.796875 14.203125 45.796875 21.1875 \r\nQ 45.796875 27.640625 41.28125 31.265625 \r\nQ 36.765625 34.90625 28.71875 34.90625 \r\nL 20.21875 34.90625 \r\nL 20.21875 43.015625 \r\nL 29.109375 43.015625 \r\nQ 36.375 43.015625 40.234375 45.921875 \r\nQ 44.09375 48.828125 44.09375 54.296875 \r\nQ 44.09375 59.90625 40.109375 62.90625 \r\nQ 36.140625 65.921875 28.71875 65.921875 \r\nQ 24.65625 65.921875 20.015625 65.03125 \r\nQ 15.375 64.15625 9.8125 62.3125 \r\nL 9.8125 71.09375 \r\nQ 15.4375 72.65625 20.34375 73.4375 \r\nQ 25.25 74.21875 29.59375 74.21875 \r\nQ 40.828125 74.21875 47.359375 69.109375 \r\nQ 53.90625 64.015625 53.90625 55.328125 \r\nQ 53.90625 49.265625 50.4375 45.09375 \r\nQ 46.96875 40.921875 40.578125 39.3125 \r\nz\r\n\" id=\"DejaVuSans-51\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-40\"/>\r\n       <use x=\"39.013672\" xlink:href=\"#DejaVuSans-76\"/>\r\n       <use x=\"94.726562\" xlink:href=\"#DejaVuSans-82\"/>\r\n       <use x=\"164.208984\" xlink:href=\"#DejaVuSans-51\"/>\r\n       <use x=\"227.832031\" xlink:href=\"#DejaVuSans-44\"/>\r\n       <use x=\"259.619141\" xlink:href=\"#DejaVuSans-32\"/>\r\n       <use x=\"291.40625\" xlink:href=\"#DejaVuSans-68\"/>\r\n       <use x=\"368.408203\" xlink:href=\"#DejaVuSans-84\"/>\r\n       <use x=\"429.492188\" xlink:href=\"#DejaVuSans-51\"/>\r\n       <use x=\"493.115234\" xlink:href=\"#DejaVuSans-44\"/>\r\n       <use x=\"524.902344\" xlink:href=\"#DejaVuSans-32\"/>\r\n       <use x=\"556.689453\" xlink:href=\"#DejaVuSans-76\"/>\r\n       <use x=\"612.402344\" xlink:href=\"#DejaVuSans-68\"/>\r\n       <use x=\"689.404297\" xlink:href=\"#DejaVuSans-51\"/>\r\n       <use x=\"753.027344\" xlink:href=\"#DejaVuSans-44\"/>\r\n       <use x=\"784.814453\" xlink:href=\"#DejaVuSans-32\"/>\r\n       <use x=\"816.601562\" xlink:href=\"#DejaVuSans-71\"/>\r\n       <use x=\"894.091797\" xlink:href=\"#DejaVuSans-78\"/>\r\n       <use x=\"968.896484\" xlink:href=\"#DejaVuSans-66\"/>\r\n       <use x=\"1037.5\" xlink:href=\"#DejaVuSans-51\"/>\r\n       <use x=\"1101.123047\" xlink:href=\"#DejaVuSans-41\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_2\">\r\n    <g id=\"ytick_1\">\r\n     <g id=\"line2d_4\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL -3.5 0 \r\n\" id=\"m4c0ff3d05e\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m4c0ff3d05e\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_4\">\r\n      <!-- 0.0 -->\r\n      <g transform=\"translate(7.2 228.439219)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 31.78125 66.40625 \r\nQ 24.171875 66.40625 20.328125 58.90625 \r\nQ 16.5 51.421875 16.5 36.375 \r\nQ 16.5 21.390625 20.328125 13.890625 \r\nQ 24.171875 6.390625 31.78125 6.390625 \r\nQ 39.453125 6.390625 43.28125 13.890625 \r\nQ 47.125 21.390625 47.125 36.375 \r\nQ 47.125 51.421875 43.28125 58.90625 \r\nQ 39.453125 66.40625 31.78125 66.40625 \r\nz\r\nM 31.78125 74.21875 \r\nQ 44.046875 74.21875 50.515625 64.515625 \r\nQ 56.984375 54.828125 56.984375 36.375 \r\nQ 56.984375 17.96875 50.515625 8.265625 \r\nQ 44.046875 -1.421875 31.78125 -1.421875 \r\nQ 19.53125 -1.421875 13.0625 8.265625 \r\nQ 6.59375 17.96875 6.59375 36.375 \r\nQ 6.59375 54.828125 13.0625 64.515625 \r\nQ 19.53125 74.21875 31.78125 74.21875 \r\nz\r\n\" id=\"DejaVuSans-48\"/>\r\n        <path d=\"M 10.6875 12.40625 \r\nL 21 12.40625 \r\nL 21 0 \r\nL 10.6875 0 \r\nz\r\n\" id=\"DejaVuSans-46\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_2\">\r\n     <g id=\"line2d_5\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m4c0ff3d05e\" y=\"178.984824\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_5\">\r\n      <!-- 0.2 -->\r\n      <g transform=\"translate(7.2 182.784043)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-50\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_3\">\r\n     <g id=\"line2d_6\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m4c0ff3d05e\" y=\"133.329648\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_6\">\r\n      <!-- 0.4 -->\r\n      <g transform=\"translate(7.2 137.128867)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 37.796875 64.3125 \r\nL 12.890625 25.390625 \r\nL 37.796875 25.390625 \r\nz\r\nM 35.203125 72.90625 \r\nL 47.609375 72.90625 \r\nL 47.609375 25.390625 \r\nL 58.015625 25.390625 \r\nL 58.015625 17.1875 \r\nL 47.609375 17.1875 \r\nL 47.609375 0 \r\nL 37.796875 0 \r\nL 37.796875 17.1875 \r\nL 4.890625 17.1875 \r\nL 4.890625 26.703125 \r\nz\r\n\" id=\"DejaVuSans-52\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-52\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_4\">\r\n     <g id=\"line2d_7\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m4c0ff3d05e\" y=\"87.674472\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_7\">\r\n      <!-- 0.6 -->\r\n      <g transform=\"translate(7.2 91.473691)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 33.015625 40.375 \r\nQ 26.375 40.375 22.484375 35.828125 \r\nQ 18.609375 31.296875 18.609375 23.390625 \r\nQ 18.609375 15.53125 22.484375 10.953125 \r\nQ 26.375 6.390625 33.015625 6.390625 \r\nQ 39.65625 6.390625 43.53125 10.953125 \r\nQ 47.40625 15.53125 47.40625 23.390625 \r\nQ 47.40625 31.296875 43.53125 35.828125 \r\nQ 39.65625 40.375 33.015625 40.375 \r\nz\r\nM 52.59375 71.296875 \r\nL 52.59375 62.3125 \r\nQ 48.875 64.0625 45.09375 64.984375 \r\nQ 41.3125 65.921875 37.59375 65.921875 \r\nQ 27.828125 65.921875 22.671875 59.328125 \r\nQ 17.53125 52.734375 16.796875 39.40625 \r\nQ 19.671875 43.65625 24.015625 45.921875 \r\nQ 28.375 48.1875 33.59375 48.1875 \r\nQ 44.578125 48.1875 50.953125 41.515625 \r\nQ 57.328125 34.859375 57.328125 23.390625 \r\nQ 57.328125 12.15625 50.6875 5.359375 \r\nQ 44.046875 -1.421875 33.015625 -1.421875 \r\nQ 20.359375 -1.421875 13.671875 8.265625 \r\nQ 6.984375 17.96875 6.984375 36.375 \r\nQ 6.984375 53.65625 15.1875 63.9375 \r\nQ 23.390625 74.21875 37.203125 74.21875 \r\nQ 40.921875 74.21875 44.703125 73.484375 \r\nQ 48.484375 72.75 52.59375 71.296875 \r\nz\r\n\" id=\"DejaVuSans-54\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-54\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_5\">\r\n     <g id=\"line2d_8\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m4c0ff3d05e\" y=\"42.019296\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_8\">\r\n      <!-- 0.8 -->\r\n      <g transform=\"translate(7.2 45.818514)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 31.78125 34.625 \r\nQ 24.75 34.625 20.71875 30.859375 \r\nQ 16.703125 27.09375 16.703125 20.515625 \r\nQ 16.703125 13.921875 20.71875 10.15625 \r\nQ 24.75 6.390625 31.78125 6.390625 \r\nQ 38.8125 6.390625 42.859375 10.171875 \r\nQ 46.921875 13.96875 46.921875 20.515625 \r\nQ 46.921875 27.09375 42.890625 30.859375 \r\nQ 38.875 34.625 31.78125 34.625 \r\nz\r\nM 21.921875 38.8125 \r\nQ 15.578125 40.375 12.03125 44.71875 \r\nQ 8.5 49.078125 8.5 55.328125 \r\nQ 8.5 64.0625 14.71875 69.140625 \r\nQ 20.953125 74.21875 31.78125 74.21875 \r\nQ 42.671875 74.21875 48.875 69.140625 \r\nQ 55.078125 64.0625 55.078125 55.328125 \r\nQ 55.078125 49.078125 51.53125 44.71875 \r\nQ 48 40.375 41.703125 38.8125 \r\nQ 48.828125 37.15625 52.796875 32.3125 \r\nQ 56.78125 27.484375 56.78125 20.515625 \r\nQ 56.78125 9.90625 50.3125 4.234375 \r\nQ 43.84375 -1.421875 31.78125 -1.421875 \r\nQ 19.734375 -1.421875 13.25 4.234375 \r\nQ 6.78125 9.90625 6.78125 20.515625 \r\nQ 6.78125 27.484375 10.78125 32.3125 \r\nQ 14.796875 37.15625 21.921875 38.8125 \r\nz\r\nM 18.3125 54.390625 \r\nQ 18.3125 48.734375 21.84375 45.5625 \r\nQ 25.390625 42.390625 31.78125 42.390625 \r\nQ 38.140625 42.390625 41.71875 45.5625 \r\nQ 45.3125 48.734375 45.3125 54.390625 \r\nQ 45.3125 60.0625 41.71875 63.234375 \r\nQ 38.140625 66.40625 31.78125 66.40625 \r\nQ 25.390625 66.40625 21.84375 63.234375 \r\nQ 18.3125 60.0625 18.3125 54.390625 \r\nz\r\n\" id=\"DejaVuSans-56\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-56\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"patch_15\">\r\n    <path d=\"M 30.103125 224.64 \r\nL 30.103125 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_16\">\r\n    <path d=\"M 364.903125 224.64 \r\nL 364.903125 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_17\">\r\n    <path d=\"M 30.103125 224.64 \r\nL 364.903125 224.64 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_18\">\r\n    <path d=\"M 30.103125 7.2 \r\nL 364.903125 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"legend_1\">\r\n    <g id=\"patch_19\">\r\n     <path d=\"M 170.457813 73.9125 \r\nL 224.548438 73.9125 \r\nQ 226.548438 73.9125 226.548438 71.9125 \r\nL 226.548438 14.2 \r\nQ 226.548438 12.2 224.548438 12.2 \r\nL 170.457813 12.2 \r\nQ 168.457813 12.2 168.457813 14.2 \r\nL 168.457813 71.9125 \r\nQ 168.457813 73.9125 170.457813 73.9125 \r\nz\r\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\r\n    </g>\r\n    <g id=\"patch_20\">\r\n     <path d=\"M 172.457813 23.798437 \r\nL 192.457813 23.798437 \r\nL 192.457813 16.798437 \r\nL 172.457813 16.798437 \r\nz\r\n\" style=\"fill:#1f77b4;\"/>\r\n    </g>\r\n    <g id=\"text_9\">\r\n     <!-- LR -->\r\n     <g transform=\"translate(200.457813 23.798437)scale(0.1 -0.1)\">\r\n      <use xlink:href=\"#DejaVuSans-76\"/>\r\n      <use x=\"55.712891\" xlink:href=\"#DejaVuSans-82\"/>\r\n     </g>\r\n    </g>\r\n    <g id=\"patch_21\">\r\n     <path d=\"M 172.457813 38.476562 \r\nL 192.457813 38.476562 \r\nL 192.457813 31.476562 \r\nL 172.457813 31.476562 \r\nz\r\n\" style=\"fill:#ff7f0e;\"/>\r\n    </g>\r\n    <g id=\"text_10\">\r\n     <!-- DT -->\r\n     <g transform=\"translate(200.457813 38.476562)scale(0.1 -0.1)\">\r\n      <use xlink:href=\"#DejaVuSans-68\"/>\r\n      <use x=\"77.001953\" xlink:href=\"#DejaVuSans-84\"/>\r\n     </g>\r\n    </g>\r\n    <g id=\"patch_22\">\r\n     <path d=\"M 172.457813 53.154687 \r\nL 192.457813 53.154687 \r\nL 192.457813 46.154687 \r\nL 172.457813 46.154687 \r\nz\r\n\" style=\"fill:#2ca02c;\"/>\r\n    </g>\r\n    <g id=\"text_11\">\r\n     <!-- LD -->\r\n     <g transform=\"translate(200.457813 53.154687)scale(0.1 -0.1)\">\r\n      <use xlink:href=\"#DejaVuSans-76\"/>\r\n      <use x=\"55.712891\" xlink:href=\"#DejaVuSans-68\"/>\r\n     </g>\r\n    </g>\r\n    <g id=\"patch_23\">\r\n     <path d=\"M 172.457813 67.832812 \r\nL 192.457813 67.832812 \r\nL 192.457813 60.832812 \r\nL 172.457813 60.832812 \r\nz\r\n\" style=\"fill:#d62728;\"/>\r\n    </g>\r\n    <g id=\"text_12\">\r\n     <!-- GNB -->\r\n     <g transform=\"translate(200.457813 67.832812)scale(0.1 -0.1)\">\r\n      <use xlink:href=\"#DejaVuSans-71\"/>\r\n      <use x=\"77.490234\" xlink:href=\"#DejaVuSans-78\"/>\r\n      <use x=\"152.294922\" xlink:href=\"#DejaVuSans-66\"/>\r\n     </g>\r\n    </g>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"p37a26a498d\">\r\n   <rect height=\"217.44\" width=\"334.8\" x=\"30.103125\" y=\"7.2\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "df=pd.DataFrame({'LR' : LR_res, 'DT': DT_res,'LD':LD_res,'GNB':GNB_res}, index=[['LR1',\"LR2\",\"LR3\"],[\"DT1\",\"DT2\",\"DT3\"],[\"LD1\",\"LD2\",\"LD3\"],[\"GNB1\",\"GNB2\",\"GNB3\"]])\n",
        "df.plot.bar(rot=0);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "KN=KNeighborsClassifier()\n",
        "KN.fit(X_train_train,y_train_train[:,1])\n",
        "KN_score=KN.score(X_train_train,y_train_train[:,1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.937374512585564"
            ]
          },
          "metadata": {},
          "execution_count": 144
        }
      ],
      "source": [
        "KN_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "yhiba=[]\n",
        "for inx,i in enumerate(y_train_train):\n",
        "    yhiba.append(0)\n",
        "yhiba[0]=1\n"
      ]
    },
    {
      "source": [
        "n=20\n",
        "i=0\n",
        "yt0=y_train_train[:,0]\n",
        "yt1=y_train_train[:,1]\n",
        "yt2=y_train_train[:,2]\n",
        "yt3=y_train_train[:,3]\n",
        "clf1.fit(X_train_train,yt1)\n",
        "clf2.fit(X_train_train,yt2)\n",
        "clf3.fit(X_train_train,yt3)\n"
      ],
      "cell_type": "code",
      "metadata": {},
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Iter       Train Loss   Remaining Time \n",
            "         1           0.0833            3.47s\n",
            "         2           0.0802            2.57s\n",
            "         3           0.0776            1.69s\n",
            "         4           0.0755            0.84s\n",
            "         5           0.0736            0.00s\n",
            "      Iter       Train Loss   Remaining Time \n",
            "         1           0.2407            3.42s\n",
            "         2           0.2370            2.55s\n",
            "         3           0.2339            1.69s\n",
            "         4           0.2314            0.84s\n",
            "         5           0.2291            0.00s\n",
            "      Iter       Train Loss   Remaining Time \n",
            "         1           0.2137            3.32s\n",
            "         2           0.2064            2.49s\n",
            "         3           0.2005            1.66s\n",
            "         4           0.1956            0.83s\n",
            "         5           0.1915            0.00s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GradientBoostingRegressor(n_estimators=5, verbose=3, warm_start=True)"
            ]
          },
          "metadata": {},
          "execution_count": 144
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 0.00295379\n",
            "Iteration 2, loss = 0.00010094\n",
            "Iteration 3, loss = 0.00007857\n",
            "Iteration 4, loss = 0.00007376\n",
            "Iteration 5, loss = 0.00006352\n",
            "Iteration 6, loss = 0.00005093\n",
            "Iteration 7, loss = 0.00004983\n",
            "Iteration 8, loss = 0.00004696\n",
            "Iteration 9, loss = 0.00004742\n",
            "Iteration 10, loss = 0.00004567\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPClassifier(hidden_layer_sizes=(60, 500, 5), max_iter=10, random_state=1,\n",
              "              verbose=True, warm_start=True)"
            ]
          },
          "metadata": {},
          "execution_count": 145
        }
      ],
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "clfhiba3=MLPClassifier(random_state=1, max_iter=10, warm_start=True, verbose=True,hidden_layer_sizes=(60,500,5))\n",
        "clfhiba3.fit(X_train_train,yhiba)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ITT kezdődik az okítási ciklus IDe jön vissza"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "def reconvert (X):\n",
        "    yo=[]\n",
        "    o=1\n",
        "    y1=X\n",
        "    for idx,_ in enumerate(y1):\n",
        "       if y1[idx][0]==max(y1[idx][0],y1[idx][1],y1[idx][2]):\n",
        "           o=1\n",
        "       if y1[idx][1]==max(y1[idx][0],y1[idx][1],y1[idx][2]):\n",
        "           o=2\n",
        "       if y1[idx][2]==max(y1[idx][0],y1[idx][1],y1[idx][2]):\n",
        "           o=3\n",
        "       '''\n",
        "       ox=o\n",
        "       if ox==3 and yerror[idx]:\n",
        "           o=2\n",
        "       if ox==1 and yerror[idx]:\n",
        "           o=2\n",
        "       if ox==2 and yerror[idx]:\n",
        "           # 2 eset van \n",
        "           if y3[idx]>y1[idx]:\n",
        "               o=3\n",
        "           else:\n",
        "               o=1\n",
        "\n",
        "       '''\n",
        "       \n",
        "       yo.append(o)\n",
        "    return(yo)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 196,
      "metadata": {
        "tags": [
          "outputPrepend"
        ]
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "996,0.131453\n",
            "n_hiba:6345\n",
            "damage grade in     6355 nextsave=6555. step: 0.046458,0.168974,0.131439\n",
            "n_hiba:6355\n",
            "damage grade in     6365 nextsave=6555. step: 0.046451,0.168959,0.131426\n",
            "n_hiba:6365\n",
            "damage grade in     6375 nextsave=6555. step: 0.046446,0.168942,0.131416\n",
            "n_hiba:6375\n",
            "damage grade in     6385 nextsave=6555. step: 0.046440,0.168928,0.131410\n",
            "n_hiba:6385\n",
            "damage grade in     6395 nextsave=6555. step: 0.046437,0.168911,0.131399\n",
            "n_hiba:6395\n",
            "damage grade in     6405 nextsave=6555. step: 0.046432,0.168890,0.131388\n",
            "n_hiba:6405\n",
            "damage grade in     6415 nextsave=6555. step: 0.046428,0.168873,0.131379\n",
            "n_hiba:6415\n",
            "damage grade in     6425 nextsave=6555. step: 0.046425,0.168857,0.131368\n",
            "n_hiba:6425\n",
            "damage grade in     6435 nextsave=6555. step: 0.046418,0.168844,0.131359\n",
            "n_hiba:6435\n",
            "damage grade in     6445 nextsave=6555. step: 0.046412,0.168828,0.131340\n",
            "n_hiba:6445\n",
            "damage grade in     6455 nextsave=6555. step: 0.046407,0.168815,0.131330\n",
            "n_hiba:6455\n",
            "damage grade in     6465 nextsave=6555. step: 0.046402,0.168806,0.131323\n",
            "n_hiba:6465\n",
            "damage grade in     6475 nextsave=6555. step: 0.046398,0.168793,0.131312\n",
            "n_hiba:6475\n",
            "damage grade in     6485 nextsave=6555. step: 0.046393,0.168784,0.131293\n",
            "n_hiba:6485\n",
            "damage grade in     6495 nextsave=6555. step: 0.046388,0.168767,0.131281\n",
            "n_hiba:6495\n",
            "damage grade in     6505 nextsave=6555. step: 0.046383,0.168749,0.131267\n",
            "n_hiba:6505\n",
            "damage grade in     6515 nextsave=6555. step: 0.046375,0.168737,0.131255\n",
            "n_hiba:6515\n",
            "damage grade in     6525 nextsave=6555. step: 0.046369,0.168724,0.131246\n",
            "n_hiba:6525\n",
            "damage grade in     6535 nextsave=6555. step: 0.046361,0.168711,0.131235\n",
            "n_hiba:6535\n",
            "damage grade in     6545 nextsave=6555. step: 0.046357,0.168692,0.131222\n",
            "n_hiba:6545\n",
            "damage grade in     6555 nextsave=6555. step: 0.046354,0.168667,0.131210\n",
            "n_hiba:6555\n",
            "damage grade in     6565 nextsave=7065. step: 0.046350,0.168647,0.131196\n",
            "n_hiba:6565\n",
            "damage grade in     6575 nextsave=7065. step: 0.046344,0.168634,0.131183\n",
            "n_hiba:6575\n",
            "damage grade in     6585 nextsave=7065. step: 0.046338,0.168623,0.131172\n",
            "n_hiba:6585\n",
            "damage grade in     6595 nextsave=7065. step: 0.046330,0.168612,0.131166\n",
            "n_hiba:6595\n",
            "damage grade in     6605 nextsave=7065. step: 0.046327,0.168594,0.131154\n",
            "n_hiba:6605\n",
            "damage grade in     6615 nextsave=7065. step: 0.046323,0.168583,0.131147\n",
            "n_hiba:6615\n",
            "damage grade in     6625 nextsave=7065. step: 0.046318,0.168569,0.131136\n",
            "n_hiba:6625\n",
            "damage grade in     6635 nextsave=7065. step: 0.046310,0.168553,0.131125\n",
            "n_hiba:6635\n",
            "damage grade in     6645 nextsave=7065. step: 0.046306,0.168539,0.131114\n",
            "n_hiba:6645\n",
            "damage grade in     6655 nextsave=7065. step: 0.046302,0.168528,0.131107\n",
            "n_hiba:6655\n",
            "damage grade in     6665 nextsave=7065. step: 0.046299,0.168518,0.131093\n",
            "n_hiba:6665\n",
            "damage grade in     6675 nextsave=7065. step: 0.046293,0.168504,0.131085\n",
            "n_hiba:6675\n",
            "damage grade in     6685 nextsave=7065. step: 0.046286,0.168494,0.131078\n",
            "n_hiba:6685\n",
            "damage grade in     6695 nextsave=7065. step: 0.046281,0.168477,0.131068\n",
            "n_hiba:6695\n",
            "damage grade in     6705 nextsave=7065. step: 0.046277,0.168462,0.131061\n",
            "n_hiba:6705\n",
            "damage grade in     6715 nextsave=7065. step: 0.046273,0.168448,0.131043\n",
            "n_hiba:6715\n",
            "damage grade in     6725 nextsave=7065. step: 0.046268,0.168436,0.131035\n",
            "n_hiba:6725\n",
            "damage grade in     6735 nextsave=7065. step: 0.046261,0.168422,0.131026\n",
            "n_hiba:6735\n",
            "damage grade in     6745 nextsave=7065. step: 0.046255,0.168407,0.131018\n",
            "n_hiba:6745\n",
            "damage grade in     6755 nextsave=7065. step: 0.046248,0.168395,0.131011\n",
            "n_hiba:6755\n",
            "damage grade in     6765 nextsave=7065. step: 0.046242,0.168379,0.131002\n",
            "n_hiba:6765\n",
            "damage grade in     6775 nextsave=7065. step: 0.046237,0.168368,0.130996\n",
            "n_hiba:6775\n",
            "damage grade in     6785 nextsave=7065. step: 0.046231,0.168357,0.130990\n",
            "n_hiba:6785\n",
            "damage grade in     6795 nextsave=7065. step: 0.046223,0.168348,0.130981\n",
            "n_hiba:6795\n",
            "damage grade in     6805 nextsave=7065. step: 0.046218,0.168336,0.130964\n",
            "n_hiba:6805\n",
            "damage grade in     6815 nextsave=7065. step: 0.046213,0.168325,0.130954\n",
            "n_hiba:6815\n",
            "damage grade in     6825 nextsave=7065. step: 0.046210,0.168312,0.130944\n",
            "n_hiba:6825\n",
            "damage grade in     6835 nextsave=7065. step: 0.046206,0.168303,0.130933\n",
            "n_hiba:6835\n",
            "damage grade in     6845 nextsave=7065. step: 0.046202,0.168291,0.130920\n",
            "n_hiba:6845\n",
            "damage grade in     6855 nextsave=7065. step: 0.046198,0.168283,0.130908\n",
            "n_hiba:6855\n",
            "damage grade in     6865 nextsave=7065. step: 0.046194,0.168274,0.130898\n",
            "n_hiba:6865\n",
            "damage grade in     6875 nextsave=7065. step: 0.046188,0.168256,0.130891\n",
            "n_hiba:6875\n",
            "damage grade in     6885 nextsave=7065. step: 0.046184,0.168241,0.130881\n",
            "n_hiba:6885\n",
            "damage grade in     6895 nextsave=7065. step: 0.046177,0.168224,0.130865\n",
            "n_hiba:6895\n",
            "damage grade in     6905 nextsave=7065. step: 0.046172,0.168208,0.130850\n",
            "n_hiba:6905\n",
            "damage grade in     6915 nextsave=7065. step: 0.046169,0.168199,0.130837\n",
            "n_hiba:6915\n",
            "damage grade in     6925 nextsave=7065. step: 0.046166,0.168189,0.130828\n",
            "n_hiba:6925\n",
            "damage grade in     6935 nextsave=7065. step: 0.046160,0.168178,0.130815\n",
            "n_hiba:6935\n",
            "damage grade in     6945 nextsave=7065. step: 0.046156,0.168164,0.130801\n",
            "n_hiba:6945\n",
            "damage grade in     6955 nextsave=7065. step: 0.046151,0.168147,0.130791\n",
            "n_hiba:6955\n",
            "damage grade in     6965 nextsave=7065. step: 0.046147,0.168128,0.130780\n",
            "n_hiba:6965\n",
            "damage grade in     6975 nextsave=7065. step: 0.046144,0.168115,0.130773\n",
            "n_hiba:6975\n",
            "damage grade in     6985 nextsave=7065. step: 0.046141,0.168101,0.130759\n",
            "n_hiba:6985\n",
            "damage grade in     6995 nextsave=7065. step: 0.046137,0.168090,0.130743\n",
            "n_hiba:6995\n",
            "damage grade in     7005 nextsave=7065. step: 0.046130,0.168081,0.130734\n",
            "n_hiba:7005\n",
            "damage grade in     7015 nextsave=7065. step: 0.046126,0.168071,0.130727\n",
            "n_hiba:7015\n",
            "damage grade in     7025 nextsave=7065. step: 0.046122,0.168061,0.130718\n",
            "n_hiba:7025\n",
            "damage grade in     7035 nextsave=7065. step: 0.046116,0.168052,0.130708\n",
            "n_hiba:7035\n",
            "damage grade in     7045 nextsave=7065. step: 0.046111,0.168037,0.130701\n",
            "n_hiba:7045\n",
            "damage grade in     7055 nextsave=7065. step: 0.046107,0.168028,0.130686\n",
            "n_hiba:7055\n",
            "damage grade in     7065 nextsave=7065. step: 0.046102,0.168018,0.130668\n",
            "n_hiba:7065\n",
            "damage grade in     7075 nextsave=7575. step: 0.046099,0.168011,0.130657\n",
            "n_hiba:7075\n",
            "damage grade in     7085 nextsave=7575. step: 0.046094,0.168003,0.130645\n",
            "n_hiba:7085\n",
            "damage grade in     7095 nextsave=7575. step: 0.046089,0.167984,0.130636\n",
            "n_hiba:7095\n",
            "damage grade in     7105 nextsave=7575. step: 0.046082,0.167962,0.130628\n",
            "n_hiba:7105\n",
            "damage grade in     7115 nextsave=7575. step: 0.046080,0.167947,0.130616\n",
            "n_hiba:7115\n",
            "damage grade in     7125 nextsave=7575. step: 0.046077,0.167936,0.130609\n",
            "n_hiba:7125\n",
            "damage grade in     7135 nextsave=7575. step: 0.046070,0.167926,0.130602\n",
            "n_hiba:7135\n",
            "damage grade in     7145 nextsave=7575. step: 0.046065,0.167915,0.130594\n",
            "n_hiba:7145\n",
            "damage grade in     7155 nextsave=7575. step: 0.046060,0.167903,0.130578\n",
            "n_hiba:7155\n",
            "damage grade in     7165 nextsave=7575. step: 0.046056,0.167892,0.130568\n",
            "n_hiba:7165\n",
            "damage grade in     7175 nextsave=7575. step: 0.046052,0.167884,0.130557\n",
            "n_hiba:7175\n",
            "damage grade in     7185 nextsave=7575. step: 0.046046,0.167872,0.130549\n",
            "n_hiba:7185\n",
            "damage grade in     7195 nextsave=7575. step: 0.046042,0.167863,0.130539\n",
            "n_hiba:7195\n",
            "damage grade in     7205 nextsave=7575. step: 0.046036,0.167853,0.130534\n",
            "n_hiba:7205\n",
            "damage grade in     7215 nextsave=7575. step: 0.046030,0.167845,0.130519\n",
            "n_hiba:7215\n",
            "damage grade in     7225 nextsave=7575. step: 0.046026,0.167835,0.130507\n",
            "n_hiba:7225\n",
            "damage grade in     7235 nextsave=7575. step: 0.046023,0.167825,0.130495\n",
            "n_hiba:7235\n",
            "damage grade in     7245 nextsave=7575. step: 0.046019,0.167810,0.130483\n",
            "n_hiba:7245\n",
            "damage grade in     7255 nextsave=7575. step: 0.046014,0.167797,0.130474\n",
            "n_hiba:7255\n",
            "damage grade in     7265 nextsave=7575. step: 0.046006,0.167782,0.130463\n",
            "n_hiba:7265\n",
            "damage grade in     7275 nextsave=7575. step: 0.046002,0.167769,0.130457\n",
            "n_hiba:7275\n",
            "damage grade in     7285 nextsave=7575. step: 0.045997,0.167758,0.130450\n",
            "n_hiba:7285\n",
            "damage grade in     7295 nextsave=7575. step: 0.045992,0.167746,0.130442\n",
            "n_hiba:7295\n",
            "damage grade in     7305 nextsave=7575. step: 0.045987,0.167737,0.130436\n",
            "n_hiba:7305\n",
            "damage grade in     7315 nextsave=7575. step: 0.045982,0.167725,0.130428\n",
            "n_hiba:7315\n",
            "damage grade in     7325 nextsave=7575. step: 0.045977,0.167710,0.130422\n",
            "n_hiba:7325\n",
            "damage grade in     7335 nextsave=7575. step: 0.045974,0.167699,0.130409\n",
            "n_hiba:7335\n",
            "damage grade in     7345 nextsave=7575. step: 0.045969,0.167682,0.130402\n",
            "n_hiba:7345\n",
            "damage grade in     7355 nextsave=7575. step: 0.045965,0.167673,0.130393\n",
            "n_hiba:7355\n",
            "damage grade in     7365 nextsave=7575. step: 0.045960,0.167661,0.130385\n",
            "n_hiba:7365\n",
            "damage grade in     7375 nextsave=7575. step: 0.045954,0.167649,0.130377\n",
            "n_hiba:7375\n",
            "damage grade in     7385 nextsave=7575. step: 0.045950,0.167636,0.130366\n",
            "n_hiba:7385\n",
            "damage grade in     7395 nextsave=7575. step: 0.045947,0.167617,0.130359\n",
            "n_hiba:7395\n",
            "damage grade in     7405 nextsave=7575. step: 0.045942,0.167607,0.130350\n",
            "n_hiba:7405\n",
            "damage grade in     7415 nextsave=7575. step: 0.045938,0.167594,0.130336\n",
            "n_hiba:7415\n",
            "damage grade in     7425 nextsave=7575. step: 0.045932,0.167578,0.130321\n",
            "n_hiba:7425\n",
            "damage grade in     7435 nextsave=7575. step: 0.045927,0.167563,0.130307\n",
            "n_hiba:7435\n",
            "damage grade in     7445 nextsave=7575. step: 0.045923,0.167546,0.130296\n",
            "n_hiba:7445\n",
            "damage grade in     7455 nextsave=7575. step: 0.045918,0.167534,0.130287\n",
            "n_hiba:7455\n",
            "damage grade in     7465 nextsave=7575. step: 0.045914,0.167521,0.130281\n",
            "n_hiba:7465\n",
            "damage grade in     7475 nextsave=7575. step: 0.045907,0.167510,0.130273\n",
            "n_hiba:7475\n",
            "damage grade in     7485 nextsave=7575. step: 0.045903,0.167497,0.130266\n",
            "n_hiba:7485\n",
            "damage grade in     7495 nextsave=7575. step: 0.045899,0.167480,0.130260\n",
            "n_hiba:7495\n",
            "damage grade in     7505 nextsave=7575. step: 0.045893,0.167472,0.130253\n",
            "n_hiba:7505\n",
            "damage grade in     7515 nextsave=7575. step: 0.045889,0.167463,0.130248\n",
            "n_hiba:7515\n",
            "damage grade in     7525 nextsave=7575. step: 0.045883,0.167452,0.130241\n",
            "n_hiba:7525\n",
            "damage grade in     7535 nextsave=7575. step: 0.045879,0.167442,0.130235\n",
            "n_hiba:7535\n",
            "damage grade in     7545 nextsave=7575. step: 0.045875,0.167434,0.130228\n",
            "n_hiba:7545\n",
            "damage grade in     7555 nextsave=7575. step: 0.045871,0.167422,0.130221\n",
            "n_hiba:7555\n",
            "damage grade in     7565 nextsave=7575. step: 0.045865,0.167411,0.130215\n",
            "n_hiba:7565\n",
            "damage grade in     7575 nextsave=7575. step: 0.045862,0.167399,0.130210\n",
            "n_hiba:7575\n",
            "damage grade in     7585 nextsave=8085. step: 0.045856,0.167381,0.130207\n",
            "n_hiba:7585\n",
            "damage grade in     7595 nextsave=8085. step: 0.045850,0.167368,0.130191\n",
            "n_hiba:7595\n",
            "damage grade in     7605 nextsave=8085. step: 0.045846,0.167355,0.130183\n",
            "n_hiba:7605\n",
            "damage grade in     7615 nextsave=8085. step: 0.045840,0.167341,0.130176\n",
            "n_hiba:7615\n",
            "damage grade in     7625 nextsave=8085. step: 0.045835,0.167331,0.130167\n",
            "n_hiba:7625\n",
            "damage grade in     7635 nextsave=8085. step: 0.045830,0.167318,0.130158\n",
            "n_hiba:7635\n",
            "damage grade in     7645 nextsave=8085. step: 0.045826,0.167302,0.130149\n",
            "n_hiba:7645\n",
            "damage grade in     7655 nextsave=8085. step: 0.045823,0.167290,0.130138\n",
            "n_hiba:7655\n",
            "damage grade in     7665 nextsave=8085. step: 0.045817,0.167279,0.130130\n",
            "n_hiba:7665\n",
            "damage grade in     7675 nextsave=8085. step: 0.045814,0.167267,0.130122\n",
            "n_hiba:7675\n",
            "damage grade in     7685 nextsave=8085. step: 0.045809,0.167255,0.130111\n",
            "n_hiba:7685\n",
            "damage grade in     7695 nextsave=8085. step: 0.045803,0.167241,0.130098\n",
            "n_hiba:7695\n",
            "damage grade in     7705 nextsave=8085. step: 0.045800,0.167230,0.130086\n",
            "n_hiba:7705\n",
            "damage grade in     7715 nextsave=8085. step: 0.045796,0.167213,0.130079\n",
            "n_hiba:7715\n",
            "damage grade in     7725 nextsave=8085. step: 0.045791,0.167198,0.130069\n",
            "n_hiba:7725\n",
            "damage grade in     7735 nextsave=8085. step: 0.045785,0.167184,0.130060\n",
            "n_hiba:7735\n",
            "damage grade in     7745 nextsave=8085. step: 0.045778,0.167173,0.130049\n",
            "n_hiba:7745\n",
            "damage grade in     7755 nextsave=8085. step: 0.045774,0.167158,0.130040\n",
            "n_hiba:7755\n",
            "damage grade in     7765 nextsave=8085. step: 0.045769,0.167151,0.130031\n",
            "n_hiba:7765\n",
            "damage grade in     7775 nextsave=8085. step: 0.045762,0.167141,0.130022\n",
            "n_hiba:7775\n",
            "damage grade in     7785 nextsave=8085. step: 0.045758,0.167129,0.130014\n",
            "n_hiba:7785\n",
            "damage grade in     7795 nextsave=8085. step: 0.045751,0.167121,0.130004\n",
            "n_hiba:7795\n",
            "damage grade in     7805 nextsave=8085. step: 0.045746,0.167107,0.129997\n",
            "n_hiba:7805\n",
            "damage grade in     7815 nextsave=8085. step: 0.045741,0.167093,0.129988\n",
            "n_hiba:7815\n",
            "damage grade in     7825 nextsave=8085. step: 0.045736,0.167079,0.129979\n",
            "n_hiba:7825\n",
            "damage grade in     7835 nextsave=8085. step: 0.045730,0.167072,0.129971\n",
            "n_hiba:7835\n",
            "damage grade in     7845 nextsave=8085. step: 0.045727,0.167062,0.129961\n",
            "n_hiba:7845\n",
            "damage grade in     7855 nextsave=8085. step: 0.045724,0.167050,0.129952\n",
            "n_hiba:7855\n",
            "damage grade in     7865 nextsave=8085. step: 0.045721,0.167041,0.129944\n",
            "n_hiba:7865\n",
            "damage grade in     7875 nextsave=8085. step: 0.045718,0.167022,0.129936\n",
            "n_hiba:7875\n",
            "damage grade in     7885 nextsave=8085. step: 0.045715,0.167009,0.129930\n",
            "n_hiba:7885\n",
            "damage grade in     7895 nextsave=8085. step: 0.045713,0.166995,0.129925\n",
            "n_hiba:7895\n",
            "damage grade in     7905 nextsave=8085. step: 0.045710,0.166987,0.129917\n",
            "n_hiba:7905\n",
            "damage grade in     7915 nextsave=8085. step: 0.045706,0.166976,0.129906\n",
            "n_hiba:7915\n",
            "damage grade in     7925 nextsave=8085. step: 0.045702,0.166966,0.129899\n",
            "n_hiba:7925\n",
            "damage grade in     7935 nextsave=8085. step: 0.045699,0.166957,0.129891\n",
            "n_hiba:7935\n",
            "damage grade in     7945 nextsave=8085. step: 0.045695,0.166950,0.129879\n",
            "n_hiba:7945\n",
            "damage grade in     7955 nextsave=8085. step: 0.045692,0.166934,0.129870\n",
            "n_hiba:7955\n",
            "damage grade in     7965 nextsave=8085. step: 0.045690,0.166923,0.129863\n",
            "n_hiba:7965\n",
            "damage grade in     7975 nextsave=8085. step: 0.045685,0.166912,0.129855\n",
            "n_hiba:7975\n",
            "damage grade in     7985 nextsave=8085. step: 0.045681,0.166903,0.129848\n",
            "n_hiba:7985\n",
            "damage grade in     7995 nextsave=8085. step: 0.045676,0.166891,0.129841\n",
            "n_hiba:7995\n",
            "damage grade in     8005 nextsave=8085. step: 0.045673,0.166879,0.129833\n",
            "n_hiba:8005\n",
            "damage grade in     8015 nextsave=8085. step: 0.045670,0.166870,0.129827\n",
            "n_hiba:8015\n",
            "damage grade in     8025 nextsave=8085. step: 0.045667,0.166857,0.129820\n",
            "n_hiba:8025\n",
            "damage grade in     8035 nextsave=8085. step: 0.045664,0.166842,0.129813\n",
            "n_hiba:8035\n",
            "damage grade in     8045 nextsave=8085. step: 0.045662,0.166831,0.129808\n",
            "n_hiba:8045\n",
            "damage grade in     8055 nextsave=8085. step: 0.045658,0.166823,0.129801\n",
            "n_hiba:8055\n",
            "damage grade in     8065 nextsave=8085. step: 0.045654,0.166810,0.129794\n",
            "n_hiba:8065\n",
            "damage grade in     8075 nextsave=8085. step: 0.045651,0.166796,0.129785\n",
            "n_hiba:8075\n",
            "damage grade in     8085 nextsave=8085. step: 0.045648,0.166778,0.129777\n",
            "n_hiba:8085\n",
            "damage grade in     8095 nextsave=8595. step: 0.045645,0.166769,0.129771\n",
            "n_hiba:8095\n",
            "damage grade in     8105 nextsave=8595. step: 0.045641,0.166756,0.129759\n",
            "n_hiba:8105\n",
            "damage grade in     8115 nextsave=8595. step: 0.045636,0.166746,0.129749\n",
            "n_hiba:8115\n",
            "damage grade in     8125 nextsave=8595. step: 0.045630,0.166738,0.129738\n",
            "n_hiba:8125\n",
            "damage grade in     8135 nextsave=8595. step: 0.045625,0.166728,0.129727\n",
            "n_hiba:8135\n",
            "damage grade in     8145 nextsave=8595. step: 0.045617,0.166712,0.129720\n",
            "n_hiba:8145\n",
            "damage grade in     8155 nextsave=8595. step: 0.045609,0.166695,0.129712\n",
            "n_hiba:8155\n",
            "damage grade in     8165 nextsave=8595. step: 0.045601,0.166683,0.129707\n",
            "n_hiba:8165\n",
            "damage grade in     8175 nextsave=8595. step: 0.045598,0.166666,0.129698\n",
            "n_hiba:8175\n",
            "damage grade in     8185 nextsave=8595. step: 0.045593,0.166648,0.129690\n",
            "n_hiba:8185\n",
            "damage grade in     8195 nextsave=8595. step: 0.045590,0.166629,0.129683\n",
            "n_hiba:8195\n",
            "damage grade in     8205 nextsave=8595. step: 0.045585,0.166619,0.129677\n",
            "n_hiba:8205\n",
            "damage grade in     8215 nextsave=8595. step: 0.045582,0.166608,0.129669\n",
            "n_hiba:8215\n",
            "damage grade in     8225 nextsave=8595. step: 0.045578,0.166593,0.129654\n",
            "n_hiba:8225\n",
            "damage grade in     8235 nextsave=8595. step: 0.045573,0.166579,0.129645\n",
            "n_hiba:8235\n",
            "damage grade in     8245 nextsave=8595. step: 0.045567,0.166565,0.129637\n",
            "n_hiba:8245\n",
            "damage grade in     8255 nextsave=8595. step: 0.045564,0.166553,0.129630\n",
            "n_hiba:8255\n",
            "damage grade in     8265 nextsave=8595. step: 0.045561,0.166542,0.129620\n",
            "n_hiba:8265\n",
            "damage grade in     8275 nextsave=8595. step: 0.045557,0.166528,0.129612\n",
            "n_hiba:8275\n",
            "damage grade in     8285 nextsave=8595. step: 0.045550,0.166519,0.129604\n",
            "n_hiba:8285\n",
            "damage grade in     8295 nextsave=8595. step: 0.045545,0.166505,0.129596\n",
            "n_hiba:8295\n",
            "damage grade in     8305 nextsave=8595. step: 0.045541,0.166493,0.129589\n",
            "n_hiba:8305\n",
            "damage grade in     8315 nextsave=8595. step: 0.045538,0.166482,0.129581\n",
            "n_hiba:8315\n",
            "damage grade in     8325 nextsave=8595. step: 0.045537,0.166472,0.129574\n",
            "n_hiba:8325\n",
            "damage grade in     8335 nextsave=8595. step: 0.045534,0.166463,0.129566\n",
            "n_hiba:8335\n",
            "damage grade in     8345 nextsave=8595. step: 0.045528,0.166453,0.129559\n",
            "n_hiba:8345\n",
            "damage grade in     8355 nextsave=8595. step: 0.045523,0.166442,0.129552\n",
            "n_hiba:8355\n",
            "damage grade in     8365 nextsave=8595. step: 0.045515,0.166427,0.129545\n",
            "n_hiba:8365\n",
            "damage grade in     8375 nextsave=8595. step: 0.045509,0.166416,0.129537\n",
            "n_hiba:8375\n",
            "damage grade in     8385 nextsave=8595. step: 0.045505,0.166405,0.129529\n",
            "n_hiba:8385\n",
            "damage grade in     8395 nextsave=8595. step: 0.045499,0.166394,0.129522\n",
            "n_hiba:8395\n",
            "damage grade in     8405 nextsave=8595. step: 0.045496,0.166379,0.129515\n",
            "n_hiba:8405\n",
            "damage grade in     8415 nextsave=8595. step: 0.045491,0.166369,0.129505\n",
            "n_hiba:8415\n",
            "damage grade in     8425 nextsave=8595. step: 0.045484,0.166358,0.129499\n",
            "n_hiba:8425\n",
            "damage grade in     8435 nextsave=8595. step: 0.045482,0.166350,0.129491\n",
            "n_hiba:8435\n",
            "damage grade in     8445 nextsave=8595. step: 0.045479,0.166341,0.129483\n",
            "n_hiba:8445\n",
            "damage grade in     8455 nextsave=8595. step: 0.045476,0.166325,0.129476\n",
            "n_hiba:8455\n",
            "damage grade in     8465 nextsave=8595. step: 0.045469,0.166313,0.129466\n",
            "n_hiba:8465\n",
            "damage grade in     8475 nextsave=8595. step: 0.045464,0.166303,0.129459\n",
            "n_hiba:8475\n",
            "damage grade in     8485 nextsave=8595. step: 0.045458,0.166292,0.129452\n",
            "n_hiba:8485\n",
            "damage grade in     8495 nextsave=8595. step: 0.045451,0.166284,0.129446\n",
            "n_hiba:8495\n",
            "damage grade in     8505 nextsave=8595. step: 0.045445,0.166274,0.129438\n",
            "n_hiba:8505\n",
            "damage grade in     8515 nextsave=8595. step: 0.045443,0.166263,0.129429\n",
            "n_hiba:8515\n",
            "damage grade in     8525 nextsave=8595. step: 0.045439,0.166255,0.129422\n",
            "n_hiba:8525\n",
            "damage grade in     8535 nextsave=8595. step: 0.045436,0.166240,0.129414\n",
            "n_hiba:8535\n",
            "damage grade in     8545 nextsave=8595. step: 0.045432,0.166227,0.129407\n",
            "n_hiba:8545\n",
            "damage grade in     8555 nextsave=8595. step: 0.045429,0.166218,0.129401\n",
            "n_hiba:8555\n",
            "damage grade in     8565 nextsave=8595. step: 0.045422,0.166211,0.129393\n",
            "n_hiba:8565\n",
            "damage grade in     8575 nextsave=8595. step: 0.045416,0.166202,0.129383\n",
            "n_hiba:8575\n",
            "damage grade in     8585 nextsave=8595. step: 0.045414,0.166191,0.129376\n",
            "n_hiba:8585\n",
            "damage grade in     8595 nextsave=8595. step: 0.045409,0.166180,0.129372\n",
            "n_hiba:8595\n",
            "damage grade in     8605 nextsave=9105. step: 0.045406,0.166174,0.129366\n",
            "n_hiba:8605\n",
            "damage grade in     8615 nextsave=9105. step: 0.045402,0.166162,0.129354\n",
            "n_hiba:8615\n",
            "damage grade in     8625 nextsave=9105. step: 0.045399,0.166156,0.129342\n",
            "n_hiba:8625\n",
            "damage grade in     8635 nextsave=9105. step: 0.045393,0.166145,0.129334\n",
            "n_hiba:8635\n",
            "damage grade in     8645 nextsave=9105. step: 0.045389,0.166132,0.129327\n",
            "n_hiba:8645\n",
            "damage grade in     8655 nextsave=9105. step: 0.045383,0.166121,0.129320\n",
            "n_hiba:8655\n",
            "damage grade in     8665 nextsave=9105. step: 0.045379,0.166115,0.129311\n",
            "n_hiba:8665\n",
            "damage grade in     8675 nextsave=9105. step: 0.045376,0.166104,0.129301\n",
            "n_hiba:8675\n",
            "damage grade in     8685 nextsave=9105. step: 0.045373,0.166097,0.129292\n",
            "n_hiba:8685\n",
            "damage grade in     8695 nextsave=9105. step: 0.045369,0.166084,0.129286\n",
            "n_hiba:8695\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-196-485e1b58b75f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mclf1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0myt1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[0mclf2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0myt2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m     \u001b[0mclf3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0myt3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[0;32m    496\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    497\u001b[0m         \u001b[1;31m# fit the boosting stages\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 498\u001b[1;33m         n_stages = self._fit_stages(\n\u001b[0m\u001b[0;32m    499\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraw_predictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_rng\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    500\u001b[0m             sample_weight_val, begin_at_stage, monitor, X_idx_sorted)\n",
            "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\u001b[0m in \u001b[0;36m_fit_stages\u001b[1;34m(self, X, y, raw_predictions, sample_weight, random_state, X_val, y_val, sample_weight_val, begin_at_stage, monitor, X_idx_sorted)\u001b[0m\n\u001b[0;32m    553\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    554\u001b[0m             \u001b[1;31m# fit next stage of trees\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 555\u001b[1;33m             raw_predictions = self._fit_stage(\n\u001b[0m\u001b[0;32m    556\u001b[0m                 \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraw_predictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    557\u001b[0m                 random_state, X_idx_sorted, X_csc, X_csr)\n",
            "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\u001b[0m in \u001b[0;36m_fit_stage\u001b[1;34m(self, i, X, y, raw_predictions, sample_weight, sample_mask, random_state, X_idx_sorted, X_csc, X_csr)\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m             \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_csr\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mX_csr\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 211\u001b[1;33m             tree.fit(X, residual, sample_weight=sample_weight,\n\u001b[0m\u001b[0;32m    212\u001b[0m                      check_input=False, X_idx_sorted=X_idx_sorted)\n\u001b[0;32m    213\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\tree\\_classes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m   1240\u001b[0m         \"\"\"\n\u001b[0;32m   1241\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1242\u001b[1;33m         super().fit(\n\u001b[0m\u001b[0;32m   1243\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1244\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\tree\\_classes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    373\u001b[0m                                            min_impurity_split)\n\u001b[0;32m    374\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 375\u001b[1;33m         \u001b[0mbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    376\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    377\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mis_classifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "n=len(clf1.train_score_)\n",
        "nextsave=n+500\n",
        "while 1==1:\n",
        "    \n",
        "    n=n+1    \n",
        "    nhiba=len(clf1.train_score_)\n",
        "    print(f\"n_hiba:{nhiba}\")\n",
        "    nhiba=nhiba+10    \n",
        "    \n",
        "    clf1.set_params(alpha=0.99,n_estimators=nhiba,warm_start=True,verbose=0)\n",
        "    clf2.set_params(alpha=0.99,n_estimators=nhiba,warm_start=True,verbose=0)\n",
        "    clf3.set_params(alpha=0.99,n_estimators=nhiba,warm_start=True,verbose=0)\n",
        "   \n",
        "    clf1.fit(X_train_train,yt1)\n",
        "    clf2.fit(X_train_train,yt2)\n",
        "    clf3.fit(X_train_train,yt3)\n",
        "   \n",
        "  \n",
        "    o1=clf1.train_score_[-1]\n",
        "    o2=clf2.train_score_[-1]\n",
        "    o3=clf3.train_score_[-1]\n",
        "    #\n",
        "    # Save the model regularly!\n",
        "    #\n",
        "    if nhiba>nextsave:\n",
        "        nextsave=nhiba+500\n",
        "        import pickle\n",
        "        nhibastr=str(nhiba)\n",
        "        fname1=basedir+\"/clf1_\"+nhibastr+\"_data.pickled\"\n",
        "        with open(fname1, 'wb') as file:\n",
        "            pickle.dump(clf1, file)\n",
        "\n",
        "        fname2=basedir+\"/clf2_\"+nhibastr+\"_data.pickled\"\n",
        "        with open(fname2, 'wb') as file:\n",
        "            pickle.dump(clf2, file)\n",
        "\n",
        "        fname3=basedir+\"/clf3_\"+nhibastr+\"_data.pickled\"\n",
        "        with open(fname3, 'wb') as file:\n",
        "            pickle.dump(clf3, file)\n",
        "\n",
        "\n",
        "    if nhiba>12070:\n",
        "        break    \n",
        "        \n",
        "    print(f\"damage grade in {nhiba:8} nextsave={nextsave}. step: {o1:8.6f},{o2:8.6f},{o3:8.6f}\" )\n",
        " "
      ]
    },
    {
      "source": [
        "\n",
        "import pickle\n",
        "fname1=basedir+\"/clf1_3500_data.pickle\"\n",
        "with open(fname1, 'wb') as file:\n",
        "    pickle.dump(clf1, file)\n",
        "\n",
        "fname2=basedir+\"/clf2_3500_data.pickle\"\n",
        "with open(fname2, 'wb') as file:\n",
        "    pickle.dump(clf2, file)\n",
        "\n",
        "fname3=basedir+\"/clf3_3500_data.pickle\"\n",
        "with open(fname3, 'wb') as file:\n",
        "    pickle.dump(clf3, file)\n",
        "\n",
        "\n",
        "\n"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2. 0. 1. 0.] : 0.009 _  0.225 _  0.620 _ False_ 0.000\n",
            "[1. 1. 0. 0.] : 0.593 _  0.363 _  0.126 _ True_ 0.000\n",
            "[2. 0. 1. 0.] : 0.476 _  0.494 _  0.048 _ True_ 0.000\n",
            "[2. 0. 1. 0.] :-0.003 _  0.432 _  0.602 _ False_ 0.000\n",
            "[2. 0. 1. 0.] : 0.177 _  0.727 _  0.109 _ True_ 0.000\n",
            "[3. 0. 0. 1.] :-0.007 _  0.429 _  0.575 _ True_ 0.000\n",
            "[2. 0. 1. 0.] : 0.140 _  0.759 _  0.128 _ True_ 0.000\n",
            "[2. 0. 1. 0.] : 0.154 _  0.659 _  0.166 _ True_ 0.000\n",
            "[3. 0. 0. 1.] :-0.005 _  0.120 _  0.877 _ True_ 0.000\n",
            "[1. 1. 0. 0.] : 0.147 _  0.615 _  0.228 _ False_ 0.000\n",
            "[2. 0. 1. 0.] :-0.010 _  0.148 _  0.862 _ False_ 0.000\n",
            "[1. 1. 0. 0.] : 0.670 _  0.397 _ -0.040 _ True_ 0.000\n",
            "[2. 0. 1. 0.] : 0.004 _  0.193 _  0.829 _ False_ 0.000\n",
            "[3. 0. 0. 1.] : 0.002 _  0.458 _  0.531 _ True_ 0.000\n",
            "[3. 0. 0. 1.] :-0.009 _  0.104 _  0.893 _ True_ 0.000\n",
            "[3. 0. 0. 1.] : 0.004 _  0.452 _  0.556 _ True_ 0.000\n",
            "[1. 1. 0. 0.] : 0.320 _  0.561 _  0.132 _ False_ 0.000\n",
            "[2. 0. 1. 0.] : 0.013 _  0.816 _  0.161 _ True_ 0.000\n",
            "[3. 0. 0. 1.] :-0.009 _  0.659 _  0.395 _ False_ 1.000\n",
            "[3. 0. 0. 1.] :-0.004 _  0.502 _  0.550 _ True_ 0.000\n",
            "[3. 0. 0. 1.] : 0.050 _  0.338 _  0.449 _ True_ 0.000\n",
            "[2. 0. 1. 0.] : 0.325 _  0.395 _  0.441 _ False_ 0.000\n",
            "[2. 0. 1. 0.] : 0.344 _  0.585 _  0.076 _ True_ 0.000\n",
            "[2. 0. 1. 0.] : 0.016 _  0.728 _  0.260 _ True_ 0.000\n",
            "[2. 0. 1. 0.] : 0.011 _  0.818 _  0.241 _ True_ 0.000\n",
            "[2. 0. 1. 0.] : 0.007 _  0.752 _  0.255 _ True_ 0.000\n",
            "257994 177400 0.6876128902222532\n"
          ]
        }
      ],
      "source": [
        "\n",
        "yhibaold=[]\n",
        "maxlen=len(y_train_train)\n",
        "#maxlen=1000\n",
        "stimm=0\n",
        "for idx,_ in enumerate(y1):\n",
        "    if idx in range(0,maxlen):\n",
        "        ou1=y1[idx]\n",
        "        ou2=y2[idx]\n",
        "        ou3=y3[idx]\n",
        "        ouh=yhiba_pred[idx]\n",
        "        match=(y_train_train[idx][0]==1 and ou1==max(ou1,ou2,ou3)) or (y_train_train[idx][0]==2 and ou2==max(ou1,ou2,ou3)) or (y_train_train[idx][0]==3 and ou3==max(ou1,ou2,ou3)) \n",
        "        if match:\n",
        "            yhibaold.append(0)\n",
        "            stimm+=1\n",
        "        else:\n",
        "            yhibaold.append(1)\n",
        "        if idx%10000==1:\n",
        "            print(f\"{y_train_train[idx]} :{ou1:6.3f} _ {ou2:6.3f} _ {ou3:6.3f} _ {match}_{ouh:6.3f}\")\n",
        "\n",
        "print(maxlen,stimm,stimm/maxlen) \n",
        "#print(yhiba)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 203,
      "metadata": {},
      "outputs": [],
      "source": [
        "yhiba=yhibaold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 208,
      "metadata": {
        "tags": [
          "outputPrepend"
        ]
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "n 625, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      634. step: 0.713505\n",
            "Iteration 626, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      635. step: 0.713505\n",
            "Iteration 627, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      636. step: 0.713505\n",
            "Iteration 628, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      637. step: 0.713505\n",
            "Iteration 629, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      638. step: 0.713505\n",
            "Iteration 630, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      639. step: 0.713505\n",
            "Iteration 631, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      640. step: 0.713505\n",
            "Iteration 632, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      641. step: 0.713505\n",
            "Iteration 633, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      642. step: 0.713505\n",
            "Iteration 634, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      643. step: 0.713505\n",
            "Iteration 635, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      644. step: 0.713505\n",
            "Iteration 636, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      645. step: 0.713505\n",
            "Iteration 637, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      646. step: 0.713505\n",
            "Iteration 638, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      647. step: 0.713505\n",
            "Iteration 639, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      648. step: 0.713505\n",
            "Iteration 640, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      649. step: 0.713505\n",
            "Iteration 641, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      650. step: 0.713505\n",
            "Iteration 642, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      651. step: 0.713505\n",
            "Iteration 643, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      652. step: 0.713505\n",
            "Iteration 644, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      653. step: 0.713505\n",
            "Iteration 645, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      654. step: 0.713505\n",
            "Iteration 646, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      655. step: 0.713505\n",
            "Iteration 647, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      656. step: 0.713505\n",
            "Iteration 648, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      657. step: 0.713505\n",
            "Iteration 649, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      658. step: 0.713505\n",
            "Iteration 650, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      659. step: 0.713505\n",
            "Iteration 651, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      660. step: 0.713505\n",
            "Iteration 652, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      661. step: 0.713505\n",
            "Iteration 653, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      662. step: 0.713505\n",
            "Iteration 654, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      663. step: 0.713505\n",
            "Iteration 655, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      664. step: 0.713505\n",
            "Iteration 656, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      665. step: 0.713505\n",
            "Iteration 657, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      666. step: 0.713505\n",
            "Iteration 658, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      667. step: 0.713505\n",
            "Iteration 659, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      668. step: 0.713505\n",
            "Iteration 660, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      669. step: 0.713505\n",
            "Iteration 661, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      670. step: 0.713505\n",
            "Iteration 662, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      671. step: 0.713505\n",
            "Iteration 663, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      672. step: 0.713505\n",
            "Iteration 664, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      673. step: 0.713505\n",
            "Iteration 665, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      674. step: 0.713505\n",
            "Iteration 666, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      675. step: 0.713505\n",
            "Iteration 667, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      676. step: 0.713505\n",
            "Iteration 668, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      677. step: 0.713505\n",
            "Iteration 669, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      678. step: 0.713505\n",
            "Iteration 670, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      679. step: 0.713505\n",
            "Iteration 671, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      680. step: 0.713505\n",
            "Iteration 672, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      681. step: 0.713505\n",
            "Iteration 673, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      682. step: 0.713505\n",
            "Iteration 674, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      683. step: 0.713505\n",
            "Iteration 675, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      684. step: 0.713505\n",
            "Iteration 676, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      685. step: 0.713505\n",
            "Iteration 677, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      686. step: 0.713505\n",
            "Iteration 678, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      687. step: 0.713505\n",
            "Iteration 679, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      688. step: 0.713505\n",
            "Iteration 680, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      689. step: 0.713505\n",
            "Iteration 681, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      690. step: 0.713505\n",
            "Iteration 682, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      691. step: 0.713505\n",
            "Iteration 683, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      692. step: 0.713505\n",
            "Iteration 684, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      693. step: 0.713505\n",
            "Iteration 685, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      694. step: 0.713505\n",
            "Iteration 686, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      695. step: 0.713505\n",
            "Iteration 687, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      696. step: 0.713505\n",
            "Iteration 688, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      697. step: 0.713505\n",
            "Iteration 689, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      698. step: 0.713505\n",
            "Iteration 690, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      699. step: 0.713505\n",
            "Iteration 691, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      700. step: 0.713505\n",
            "Iteration 692, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      701. step: 0.713505\n",
            "Iteration 693, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      702. step: 0.713505\n",
            "Iteration 694, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      703. step: 0.713505\n",
            "Iteration 695, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      704. step: 0.713505\n",
            "Iteration 696, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      705. step: 0.713505\n",
            "Iteration 697, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      706. step: 0.713505\n",
            "Iteration 698, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      707. step: 0.713505\n",
            "Iteration 699, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      708. step: 0.713505\n",
            "Iteration 700, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      709. step: 0.713505\n",
            "Iteration 701, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      710. step: 0.713505\n",
            "Iteration 702, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      711. step: 0.713505\n",
            "Iteration 703, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      712. step: 0.713505\n",
            "Iteration 704, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      713. step: 0.713505\n",
            "Iteration 705, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      714. step: 0.713505\n",
            "Iteration 706, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      715. step: 0.713505\n",
            "Iteration 707, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      716. step: 0.713505\n",
            "Iteration 708, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      717. step: 0.713505\n",
            "Iteration 709, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      718. step: 0.713505\n",
            "Iteration 710, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      719. step: 0.713505\n",
            "Iteration 711, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      720. step: 0.713505\n",
            "Iteration 712, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      721. step: 0.713505\n",
            "Iteration 713, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      722. step: 0.713505\n",
            "Iteration 714, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      723. step: 0.713505\n",
            "Iteration 715, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      724. step: 0.713505\n",
            "Iteration 716, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      725. step: 0.713505\n",
            "Iteration 717, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      726. step: 0.713505\n",
            "Iteration 718, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      727. step: 0.713505\n",
            "Iteration 719, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      728. step: 0.713505\n",
            "Iteration 720, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      729. step: 0.713505\n",
            "Iteration 721, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      730. step: 0.713505\n",
            "Iteration 722, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      731. step: 0.713505\n",
            "Iteration 723, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      732. step: 0.713505\n",
            "Iteration 724, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      733. step: 0.713505\n",
            "Iteration 725, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      734. step: 0.713505\n",
            "Iteration 726, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      735. step: 0.713505\n",
            "Iteration 727, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      736. step: 0.713505\n",
            "Iteration 728, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      737. step: 0.713505\n",
            "Iteration 729, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      738. step: 0.713505\n",
            "Iteration 730, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      739. step: 0.713505\n",
            "Iteration 731, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      740. step: 0.713505\n",
            "Iteration 732, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      741. step: 0.713505\n",
            "Iteration 733, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      742. step: 0.713505\n",
            "Iteration 734, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      743. step: 0.713505\n",
            "Iteration 735, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      744. step: 0.713505\n",
            "Iteration 736, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      745. step: 0.713505\n",
            "Iteration 737, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      746. step: 0.713505\n",
            "Iteration 738, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      747. step: 0.713505\n",
            "Iteration 739, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      748. step: 0.713505\n",
            "Iteration 740, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      749. step: 0.713505\n",
            "Iteration 741, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      750. step: 0.713505\n",
            "Iteration 742, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      751. step: 0.713505\n",
            "Iteration 743, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      752. step: 0.713505\n",
            "Iteration 744, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      753. step: 0.713505\n",
            "Iteration 745, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      754. step: 0.713505\n",
            "Iteration 746, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      755. step: 0.713505\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-208-93a8f0108563>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[0mohiba\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclfhiba3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0myhiba\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mnhiba\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;36m950\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36mscore\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    497\u001b[0m         \"\"\"\n\u001b[0;32m    498\u001b[0m         \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 499\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    500\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    501\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_more_tags\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m   1002\u001b[0m         \"\"\"\n\u001b[0;32m   1003\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1004\u001b[1;33m         \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1005\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1006\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\u001b[0m in \u001b[0;36m_predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    697\u001b[0m                                          layer_units[i + 1])))\n\u001b[0;32m    698\u001b[0m         \u001b[1;31m# forward propagate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 699\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_pass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactivations\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    700\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mactivations\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    701\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\u001b[0m in \u001b[0;36m_forward_pass\u001b[1;34m(self, activations)\u001b[0m\n\u001b[0;32m    102\u001b[0m         \u001b[1;31m# Iterate over the hidden layers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_layers_\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 104\u001b[1;33m             activations[i + 1] = safe_sparse_dot(activations[i],\n\u001b[0m\u001b[0;32m    105\u001b[0m                                                  self.coefs_[i])\n\u001b[0;32m    106\u001b[0m             \u001b[0mactivations\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mintercepts_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m                           FutureWarning)\n\u001b[0;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\utils\\extmath.py\u001b[0m in \u001b[0;36msafe_sparse_dot\u001b[1;34m(a, b, dense_output)\u001b[0m\n\u001b[0;32m    151\u001b[0m             \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 153\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ma\u001b[0m \u001b[1;33m@\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    154\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m     if (sparse.issparse(a) and sparse.issparse(b)\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "i=0\n",
        "\n",
        "n=n+10\n",
        "\n",
        "while 1==1:\n",
        "   \n",
        "    nhiba=clfhiba3.n_iter_\n",
        "    nhiba=nhiba+10    \n",
        "    \n",
        "\n",
        "    clfhiba3.set_params(warm_start=True,verbose=3,learning_rate=\"adaptive\",max_iter=nhiba, tol=0.00000001, alpha=0.000001, solver=\"adam\")\n",
        " \n",
        "    clfhiba3.fit(X_train_train,yhiba)\n",
        "  \n",
        "\n",
        "    ohiba=clfhiba3.score(X_train_train,yhiba)\n",
        "    if nhiba>950:\n",
        "        break    \n",
        "    print(f\"damage grade in {nhiba:8}. step: {ohiba:8.6f}\" )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 201,
      "metadata": {},
      "outputs": [],
      "source": [
        "y1=clf1.predict(X_train_train)\n",
        "y2=clf2.predict(X_train_train)\n",
        "y3=clf3.predict(X_train_train)\n",
        "yhiba_pred=clfhiba3.predict(X_train_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 202,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.12928582902762065"
            ]
          },
          "metadata": {},
          "execution_count": 202
        }
      ],
      "source": [
        "clf3.train_score_[-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "metadata": {},
      "outputs": [],
      "source": [
        "def show_similarity(a,b):\n",
        "    for i in range(min(len(a),1000)):\n",
        "        print(f\"{a[i]}-{b[i]} \",end=\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 155,
      "metadata": {},
      "outputs": [],
      "source": [
        "def search_outliers(y_t1,y_pred,X_orig,yrealpred):\n",
        "    y1=[] # y orig \n",
        "    x1=[] # X orig\n",
        "    yp=[] # y predikt\n",
        "    yrp=[] # y predicted real value\n",
        "    for inx,i in enumerate(y_t1):\n",
        "        if y_t1[inx][0]==y_pred[inx]:\n",
        "            pass\n",
        "        else:\n",
        "            y1.append(y_t1[inx][0])\n",
        "            x1.append(X_orig[inx])\n",
        "            yp.append(y_pred[inx])\n",
        "            yrp.append(yrealpred[inx])\n",
        "    out=(y1,yp,x1,yrp)\n",
        "    return out\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "metadata": {},
      "outputs": [],
      "source": [
        "# futtass ez felett !!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.25      , 0.02512563, 0.1010101 , ..., 0.        , 1.        ,\n",
              "        0.22846051],\n",
              "       [0.125     , 0.00502513, 0.05050505, ..., 0.        , 1.        ,\n",
              "        0.59586796],\n",
              "       [0.125     , 0.        , 0.1010101 , ..., 0.        , 1.        ,\n",
              "        0.87090745],\n",
              "       ...,\n",
              "       [0.        , 0.        , 0.04040404, ..., 0.        , 1.        ,\n",
              "        0.47198577],\n",
              "       [0.25      , 0.02512563, 0.1010101 , ..., 0.        , 1.        ,\n",
              "        0.70982493],\n",
              "       [0.125     , 0.0201005 , 0.07070707, ..., 0.        , 1.        ,\n",
              "        0.67640778]])"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ],
      "source": [
        "X_train_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(257994, 3), dtype=float32, numpy=\n",
              "array([[8.2885650e-28, 3.8502198e-13, 4.8756846e-27],\n",
              "       [5.1160303e-09, 3.4684851e-06, 7.4634139e-05],\n",
              "       [1.2120381e-10, 1.7932983e-08, 2.1243558e-13],\n",
              "       ...,\n",
              "       [6.6566348e-02, 1.9691995e-01, 7.7977031e-02],\n",
              "       [1.9688450e-06, 1.9762903e-02, 4.6188244e-01],\n",
              "       [5.6882501e-03, 4.3086037e-01, 1.7848617e-01]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "ypred1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "only size-1 arrays can be converted to Python scalars",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-35-b02876ecf57d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mypred1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mann2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mio\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mypred1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mcheck_similarity_np\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m<ipython-input-35-b02876ecf57d>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mypred1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mann2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mio\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mypred1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mcheck_similarity_np\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m__int__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    978\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    979\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m__int__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 980\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    981\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    982\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m__long__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mTypeError\u001b[0m: only size-1 arrays can be converted to Python scalars"
          ]
        }
      ],
      "source": [
        "ypred1=ann2(X_train_train)\n",
        "io=[int(i+0.5) for i in ypred1]\n",
        "check_similarity_np(io,y_train_train[:,0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hiba:698 max:2607 -- error: 26.7741 good %: 73.2259 %\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7322593018795551"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "ypred2=ann2(X_train_test)\n",
        "io=reconvert(ypred2)\n",
        "check_similarity_np(io,y_train_test[:,0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hiba:67590 max:257994 -- error: 26.1983 good %: 73.8017 %\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7380171631898416"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "ypred1=ann2.predict(X_train_train)\n",
        "io=reconvert(ypred1)\n",
        "check_similarity_np(io,y_train_train[:,0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.15499848, 0.76536214, 0.0816348 ],\n",
              "       [0.00503527, 0.14800292, 0.8563725 ],\n",
              "       [0.45224142, 0.5335499 , 0.01439941],\n",
              "       ...,\n",
              "       [0.2014128 , 0.5197657 , 0.3004445 ],\n",
              "       [0.01349376, 0.28768444, 0.70185846],\n",
              "       [0.0423561 , 0.74841315, 0.21801093]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ],
      "source": [
        "ypred1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "#outfile generation\n",
        "y_pred_ok=ann2.predict(X_pred_scale)\n",
        "io=reconvert( y_pred_ok)\n",
        "\n",
        "X_pred_bd=pd.read_csv(basedir+\"/orig/test_values.csv\")\n",
        "\n",
        "#y_pred_ok_int=conv_a_floatlist(y_pred_ok,range_x)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wD9rzAN4pBR5",
        "outputId": "ab2363f0-11eb-4e5a-c1a4-a71d12168323"
      },
      "source": [
        "buildingid=X_pred_bd[\"building_id\"]\n",
        "\n",
        "head2=y_pred_ok\n",
        "\n",
        "\n",
        "outdf=pd.DataFrame(data={\"damage_grade\":io} ,index=buildingid)\n",
        "outdf.index.name=\"building_id\"\n",
        "\n",
        "\n",
        "outdf.head()\n",
        "st=43\n",
        "sts=str(st)\n",
        "outdf.to_csv(basedir+\"/out/submission_\"+sts+\"_xgboost.csv\")\n",
        "print()\n",
        "print(basedir+\"/out/submission_\"+sts+\"_xgboost.csv\")"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\nC:/Users/sipocz/OneDrive/Dokumentumok/GitHub/_EarthQuake/gpos_lin/out/submission_43_xgboost.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PjkOA2B1Pys6",
        "outputId": "e76ee986-189a-401e-9df9-722850ff2e5b"
      },
      "source": [
        "if not( _PCVERSION_):\r\n",
        "    !head \"/content/drive/My Drive/001_AI/_EarthQuake/gpos_lin/out/submission_24_xgboost.csv\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 167,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.28571429, 0.0201005 , 0.06593407, ..., 0.        , 1.        ,\n",
              "        0.56170323],\n",
              "       [0.14285714, 0.02512563, 0.13186813, ..., 0.        , 1.        ,\n",
              "        0.19576828],\n",
              "       [0.14285714, 0.00502513, 0.03296703, ..., 0.        , 1.        ,\n",
              "        0.71028087],\n",
              "       ...,\n",
              "       [0.        , 0.05025126, 0.02197802, ..., 0.        , 1.        ,\n",
              "        0.73558512],\n",
              "       [0.14285714, 0.00502513, 0.08791209, ..., 0.        , 0.        ,\n",
              "        0.21615666],\n",
              "       [0.14285714, 0.01005025, 0.10989011, ..., 0.        , 1.        ,\n",
              "        0.83998508]])"
            ]
          },
          "metadata": {},
          "execution_count": 167
        }
      ],
      "source": [
        "X_pred_scale"
      ]
    },
    {
      "source": [],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "source": [
        "<img src=\"https://www.python.org/static/community_logos/python-logo-master-v3-TM.png\" title=\"Python Logo\"/>"
      ],
      "cell_type": "markdown",
      "metadata": {}
    }
  ]
}