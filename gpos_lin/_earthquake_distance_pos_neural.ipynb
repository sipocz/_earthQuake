{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "_earthquake_distance_LDA.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "_PCVERSION_=True\n",
        "\n",
        "\n",
        "if _PCVERSION_:\n",
        "    basedir=\"C:/Users/sipocz/OneDrive/Dokumentumok/GitHub/_EarthQuake/gpos_lin\"\n",
        "else:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive',force_remount=True)\n",
        "    basedir=\"/content/drive/My Drive/001_AI/_EarthQuake/gpos_lin\"\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "#--------------scikit import \n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "#--------------\n",
        "\n",
        "def outlierStatistic(X_train_predict):\n",
        "    print(X_train_predict)\n",
        "    maxX=len(X_train_predict)\n",
        "    outlier=0\n",
        "    for i in X_train_predict:\n",
        "        if i==-1:\n",
        "            outlier+=1\n",
        "    print(f\"A összes ({maxX} darabból {outlier} darab outlier van. Az {outlier/maxX*100:5.1f} %.)\")\n",
        "\n",
        "\n",
        "def checkvalues(df,columnname,key):\n",
        "    print(f\"{columnname} ellenőrzése !\")\n",
        "    numok=0\n",
        "    numerr=0\n",
        "    for i in df.index:\n",
        "        if df.at[i,columnname] in key:\n",
        "            #print(df.at[i,columnname])\n",
        "            numok+=1\n",
        "            pass\n",
        "        else:\n",
        "            numerr+=1\n",
        "            print(df.at[i,columnname],end=\", \")\n",
        "    sumall=numok+numerr\n",
        "    print(f\"\\n{sumall} mintából {numerr} db nem volt megfelelő\")\n",
        "\n",
        "\n",
        "def createcolumn(df,columnname,keys):\n",
        "    print(f\"{columnname} cseréje megy\")\n",
        "    for key in keys:\n",
        "        df[keys[key]]=0.0\n",
        "    for key in keys:\n",
        "        for i in df.index:\n",
        "            if df.at[i,columnname]==key:\n",
        "                df.at[i,keys[key]]=1.0\n",
        "\n",
        "\n",
        "def create_dict(idx,list):\n",
        "    o={}\n",
        "    for i in list:\n",
        "        o[i]=idx+\"_\"+str(i)\n",
        "    return o\n",
        "\n",
        "def create_base_data(df):\n",
        "    t=['n', 't', 'o']\n",
        "    columnname=\"land_surface_condition\"\n",
        "    key=create_dict(columnname,t)\n",
        "\n",
        "\n",
        "    #checkvalues(df,columnname,key)\n",
        "    createcolumn(df,columnname,key)\n",
        "\n",
        "    t= ['h', 'w', 'i', 'r', 'u']\n",
        "    columnname=\"foundation_type\"\n",
        "    key=create_dict(columnname,t)\n",
        "\n",
        "    #checkvalues(df,columnname,key)\n",
        "    createcolumn(df,columnname,key)\n",
        "\n",
        "    t=  ['q', 'n', 'x']\n",
        "    columnname=\"roof_type\"\n",
        "    key=create_dict(columnname,t)\n",
        "\n",
        "    #checkvalues(df,columnname,key)\n",
        "    createcolumn(df,columnname,key)\n",
        "\n",
        "    t=  ['z', 'v', 'f', 'm', 'x']\n",
        "    columnname=\"ground_floor_type\"\n",
        "    key=create_dict(columnname,t)\n",
        "\n",
        "    #checkvalues(df,columnname,key)\n",
        "    createcolumn(df,columnname,key)\n",
        "\n",
        "    t=   ['q', 's', 'j', 'x']\n",
        "    columnname=\"other_floor_type\"\n",
        "    key=create_dict(columnname,t)\n",
        "\n",
        "    #checkvalues(df,columnname,key)\n",
        "    createcolumn(df,columnname,key)\n",
        "\n",
        "    t=   ['j', 's', 't', 'o']\n",
        "    columnname=\"position\"\n",
        "    key=create_dict(columnname,t)\n",
        "\n",
        "    #checkvalues(df,columnname,key)\n",
        "    createcolumn(df,columnname,key)\n",
        "\n",
        "    t=   ['c', 's', 'f', 'd', 'm', 'a', 'q', 'u', 'n', 'o']\n",
        "    columnname=\"plan_configuration\"\n",
        "    key=create_dict(columnname,t)\n",
        "\n",
        "    #checkvalues(df,columnname,key)\n",
        "    createcolumn(df,columnname,key)\n",
        "\n",
        "    t=['a', 'w', 'r', 'v']\n",
        "    columnname=\"legal_ownership_status\"\n",
        "    key=create_dict(columnname,t)\n",
        "\n",
        "    #checkvalues(df,columnname,key)\n",
        "    createcolumn(df,columnname,key)\n",
        "\n",
        "    # level 1: 0-30, level 2: 0-1427, level 3: 0-12567.\n",
        "    # level1:0--30\n",
        "    # level2: 0.0000-----------0.9999\n",
        "    # level3: 0.000000000------0.000099999\n",
        "    l1=df.geo_level_1_id\n",
        "    l2=(df.geo_level_2_id/1427*9999)/10000\n",
        "    l3=(df.geo_level_2_id/12567*99999)/1000000000\n",
        "    df[\"geopos\"]=l1+l2+l3\n",
        "    return(df)\n",
        "\n",
        "\n",
        "\n",
        "def kill_columns(df):\n",
        "    notkey=[\"Unnamed: 0\",\"building_id\",\"legal_ownership_status\",\"geo_level_1_id\",\t\"geo_level_2_id\",\t\"geo_level_3_id\", \"land_surface_condition\",\t\"foundation_type\",\t\"roof_type\",\t\"ground_floor_type\",\t\"other_floor_type\",\t\"position\",\t\"plan_configuration\"]\n",
        "    for i in df.columns:\n",
        "        #print(i)\n",
        "        if i in notkey:\n",
        "            df.drop(columns=[i], inplace=True)\n",
        "    return df\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {},
      "outputs": [],
      "source": [
        "def check_similarity_np(np1,np2):\n",
        "    db=0\n",
        "    maxi=0\n",
        "    for ind, i1 in enumerate(np1):\n",
        "        maxi+=1\n",
        "        i2=np2[ind]\n",
        "        if i1!=i2:\n",
        "            db+=1\n",
        "    #print(f\"{ind}. eset:  {i:3},{i2:3}\")\n",
        "    print(f\"hiba:{db} max:{maxi} -- error:{db/maxi*100.0 : 2.6} good %:{100-db/maxi*100.0 : 2.6} %\")\n",
        "    return(1-db/maxi)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 502
        },
        "id": "HZHYUIgKIlT9",
        "outputId": "3740844a-f48c-4b06-87fd-f1eddb0431ab"
      },
      "source": [
        "\n",
        "features_train=basedir+\"/orig/train_values.csv\"\n",
        "labels_train=basedir+\"/orig/train_labels.csv\"\n",
        "features_predict=basedir+\"/orig/test_values.csv\"\n",
        "X_train=pd.read_csv(features_train)\n",
        "X_pred=pd.read_csv(features_predict)\n",
        "y_train=pd.read_csv(labels_train)\n",
        "\n"
      ],
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['building_id', 'geo_level_1_id', 'geo_level_2_id', 'geo_level_3_id',\n",
              "       'count_floors_pre_eq', 'age', 'area_percentage', 'height_percentage',\n",
              "       'land_surface_condition', 'foundation_type', 'roof_type',\n",
              "       'ground_floor_type', 'other_floor_type', 'position',\n",
              "       'plan_configuration', 'has_superstructure_adobe_mud',\n",
              "       'has_superstructure_mud_mortar_stone', 'has_superstructure_stone_flag',\n",
              "       'has_superstructure_cement_mortar_stone',\n",
              "       'has_superstructure_mud_mortar_brick',\n",
              "       'has_superstructure_cement_mortar_brick', 'has_superstructure_timber',\n",
              "       'has_superstructure_bamboo', 'has_superstructure_rc_non_engineered',\n",
              "       'has_superstructure_rc_engineered', 'has_superstructure_other',\n",
              "       'legal_ownership_status', 'count_families', 'has_secondary_use',\n",
              "       'has_secondary_use_agriculture', 'has_secondary_use_hotel',\n",
              "       'has_secondary_use_rental', 'has_secondary_use_institution',\n",
              "       'has_secondary_use_school', 'has_secondary_use_industry',\n",
              "       'has_secondary_use_health_post', 'has_secondary_use_gov_office',\n",
              "       'has_secondary_use_use_police', 'has_secondary_use_other'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 123
        }
      ],
      "source": [
        "X_train.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {},
      "outputs": [],
      "source": [
        "added=pd.concat([X_train.geo_level_1_id,X_train.geo_level_2_id,X_train.geo_level_3_id,y_train.damage_grade], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {},
      "outputs": [],
      "source": [
        "#added"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<Figure size 432x288 with 1 Axes>",
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<!-- Created with matplotlib (https://matplotlib.org/) -->\r\n<svg height=\"248.518125pt\" version=\"1.1\" viewBox=\"0 0 388.0125 248.518125\" width=\"388.0125pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <metadata>\r\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\r\n   <cc:Work>\r\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\r\n    <dc:date>2020-12-20T00:17:26.508830</dc:date>\r\n    <dc:format>image/svg+xml</dc:format>\r\n    <dc:creator>\r\n     <cc:Agent>\r\n      <dc:title>Matplotlib v3.3.3, https://matplotlib.org/</dc:title>\r\n     </cc:Agent>\r\n    </dc:creator>\r\n   </cc:Work>\r\n  </rdf:RDF>\r\n </metadata>\r\n <defs>\r\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M 0 248.518125 \r\nL 388.0125 248.518125 \r\nL 388.0125 0 \r\nL 0 0 \r\nz\r\n\" style=\"fill:none;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g id=\"patch_2\">\r\n    <path d=\"M 46.0125 224.64 \r\nL 380.8125 224.64 \r\nL 380.8125 7.2 \r\nL 46.0125 7.2 \r\nz\r\n\" style=\"fill:#ffffff;\"/>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path clip-path=\"url(#peaccb2b5a3)\" d=\"M 61.230682 224.64 \r\nL 71.048864 224.64 \r\nL 71.048864 217.598028 \r\nL 61.230682 217.598028 \r\nz\r\n\" style=\"fill:#1f77b4;\"/>\r\n   </g>\r\n   <g id=\"patch_4\">\r\n    <path clip-path=\"url(#peaccb2b5a3)\" d=\"M 71.048864 224.64 \r\nL 80.867045 224.64 \r\nL 80.867045 221.054355 \r\nL 71.048864 221.054355 \r\nz\r\n\" style=\"fill:#1f77b4;\"/>\r\n   </g>\r\n   <g id=\"patch_5\">\r\n    <path clip-path=\"url(#peaccb2b5a3)\" d=\"M 80.867045 224.64 \r\nL 90.685227 224.64 \r\nL 90.685227 221.865533 \r\nL 80.867045 221.865533 \r\nz\r\n\" style=\"fill:#1f77b4;\"/>\r\n   </g>\r\n   <g id=\"patch_6\">\r\n    <path clip-path=\"url(#peaccb2b5a3)\" d=\"M 90.685227 224.64 \r\nL 100.503409 224.64 \r\nL 100.503409 192.369192 \r\nL 90.685227 192.369192 \r\nz\r\n\" style=\"fill:#1f77b4;\"/>\r\n   </g>\r\n   <g id=\"patch_7\">\r\n    <path clip-path=\"url(#peaccb2b5a3)\" d=\"M 100.503409 224.64 \r\nL 110.321591 224.64 \r\nL 110.321591 190.746834 \r\nL 100.503409 190.746834 \r\nz\r\n\" style=\"fill:#1f77b4;\"/>\r\n   </g>\r\n   <g id=\"patch_8\">\r\n    <path clip-path=\"url(#peaccb2b5a3)\" d=\"M 110.321591 224.64 \r\nL 120.139773 224.64 \r\nL 120.139773 221.936071 \r\nL 110.321591 221.936071 \r\nz\r\n\" style=\"fill:#1f77b4;\"/>\r\n   </g>\r\n   <g id=\"patch_9\">\r\n    <path clip-path=\"url(#peaccb2b5a3)\" d=\"M 120.139773 224.64 \r\nL 129.957955 224.64 \r\nL 129.957955 153.503147 \r\nL 120.139773 153.503147 \r\nz\r\n\" style=\"fill:#1f77b4;\"/>\r\n   </g>\r\n   <g id=\"patch_10\">\r\n    <path clip-path=\"url(#peaccb2b5a3)\" d=\"M 129.957955 224.64 \r\nL 139.776136 224.64 \r\nL 139.776136 146.014439 \r\nL 129.957955 146.014439 \r\nz\r\n\" style=\"fill:#1f77b4;\"/>\r\n   </g>\r\n   <g id=\"patch_11\">\r\n    <path clip-path=\"url(#peaccb2b5a3)\" d=\"M 139.776136 224.64 \r\nL 149.594318 224.64 \r\nL 149.594318 108.100648 \r\nL 139.776136 108.100648 \r\nz\r\n\" style=\"fill:#1f77b4;\"/>\r\n   </g>\r\n   <g id=\"patch_12\">\r\n    <path clip-path=\"url(#peaccb2b5a3)\" d=\"M 149.594318 224.64 \r\nL 159.4125 224.64 \r\nL 159.4125 216.833874 \r\nL 149.594318 216.833874 \r\nz\r\n\" style=\"fill:#1f77b4;\"/>\r\n   </g>\r\n   <g id=\"patch_13\">\r\n    <path clip-path=\"url(#peaccb2b5a3)\" d=\"M 159.4125 224.64 \r\nL 169.230682 224.64 \r\nL 169.230682 121.643807 \r\nL 159.4125 121.643807 \r\nz\r\n\" style=\"fill:#1f77b4;\"/>\r\n   </g>\r\n   <g id=\"patch_14\">\r\n    <path clip-path=\"url(#peaccb2b5a3)\" d=\"M 169.230682 224.64 \r\nL 179.048864 224.64 \r\nL 179.048864 187.46685 \r\nL 169.230682 187.46685 \r\nz\r\n\" style=\"fill:#1f77b4;\"/>\r\n   </g>\r\n   <g id=\"patch_15\">\r\n    <path clip-path=\"url(#peaccb2b5a3)\" d=\"M 179.048864 224.64 \r\nL 188.867045 224.64 \r\nL 188.867045 216.586993 \r\nL 179.048864 216.586993 \r\nz\r\n\" style=\"fill:#1f77b4;\"/>\r\n   </g>\r\n   <g id=\"patch_16\">\r\n    <path clip-path=\"url(#peaccb2b5a3)\" d=\"M 188.867045 224.64 \r\nL 198.685227 224.64 \r\nL 198.685227 208.569255 \r\nL 188.867045 208.569255 \r\nz\r\n\" style=\"fill:#1f77b4;\"/>\r\n   </g>\r\n   <g id=\"patch_17\">\r\n    <path clip-path=\"url(#peaccb2b5a3)\" d=\"M 198.685227 224.64 \r\nL 208.503409 224.64 \r\nL 208.503409 221.395285 \r\nL 198.685227 221.395285 \r\nz\r\n\" style=\"fill:#1f77b4;\"/>\r\n   </g>\r\n   <g id=\"patch_18\">\r\n    <path clip-path=\"url(#peaccb2b5a3)\" d=\"M 208.503409 224.64 \r\nL 218.321591 224.64 \r\nL 218.321591 218.949992 \r\nL 208.503409 218.949992 \r\nz\r\n\" style=\"fill:#1f77b4;\"/>\r\n   </g>\r\n   <g id=\"patch_19\">\r\n    <path clip-path=\"url(#peaccb2b5a3)\" d=\"M 218.321591 224.64 \r\nL 228.139773 224.64 \r\nL 228.139773 213.542134 \r\nL 218.321591 213.542134 \r\nz\r\n\" style=\"fill:#1f77b4;\"/>\r\n   </g>\r\n   <g id=\"patch_20\">\r\n    <path clip-path=\"url(#peaccb2b5a3)\" d=\"M 228.139773 224.64 \r\nL 237.957955 224.64 \r\nL 237.957955 17.554286 \r\nL 228.139773 17.554286 \r\nz\r\n\" style=\"fill:#1f77b4;\"/>\r\n   </g>\r\n   <g id=\"patch_21\">\r\n    <path clip-path=\"url(#peaccb2b5a3)\" d=\"M 237.957955 224.64 \r\nL 247.776136 224.64 \r\nL 247.776136 197.236265 \r\nL 237.957955 197.236265 \r\nz\r\n\" style=\"fill:#1f77b4;\"/>\r\n   </g>\r\n   <g id=\"patch_22\">\r\n    <path clip-path=\"url(#peaccb2b5a3)\" d=\"M 247.776136 224.64 \r\nL 257.594318 224.64 \r\nL 257.594318 223.86409 \r\nL 247.776136 223.86409 \r\nz\r\n\" style=\"fill:#1f77b4;\"/>\r\n   </g>\r\n   <g id=\"patch_23\">\r\n    <path clip-path=\"url(#peaccb2b5a3)\" d=\"M 257.594318 224.64 \r\nL 267.4125 224.64 \r\nL 267.4125 200.598542 \r\nL 257.594318 200.598542 \r\nz\r\n\" style=\"fill:#1f77b4;\"/>\r\n   </g>\r\n   <g id=\"patch_24\">\r\n    <path clip-path=\"url(#peaccb2b5a3)\" d=\"M 267.4125 224.64 \r\nL 277.230682 224.64 \r\nL 277.230682 122.243374 \r\nL 267.4125 122.243374 \r\nz\r\n\" style=\"fill:#1f77b4;\"/>\r\n   </g>\r\n   <g id=\"patch_25\">\r\n    <path clip-path=\"url(#peaccb2b5a3)\" d=\"M 277.230682 224.64 \r\nL 287.048864 224.64 \r\nL 287.048864 215.035173 \r\nL 277.230682 215.035173 \r\nz\r\n\" style=\"fill:#1f77b4;\"/>\r\n   </g>\r\n   <g id=\"patch_26\">\r\n    <path clip-path=\"url(#peaccb2b5a3)\" d=\"M 287.048864 224.64 \r\nL 296.867045 224.64 \r\nL 296.867045 221.336504 \r\nL 287.048864 221.336504 \r\nz\r\n\" style=\"fill:#1f77b4;\"/>\r\n   </g>\r\n   <g id=\"patch_27\">\r\n    <path clip-path=\"url(#peaccb2b5a3)\" d=\"M 296.867045 224.64 \r\nL 306.685227 224.64 \r\nL 306.685227 223.08818 \r\nL 296.867045 223.08818 \r\nz\r\n\" style=\"fill:#1f77b4;\"/>\r\n   </g>\r\n   <g id=\"patch_28\">\r\n    <path clip-path=\"url(#peaccb2b5a3)\" d=\"M 306.685227 224.64 \r\nL 316.503409 224.64 \r\nL 316.503409 215.564203 \r\nL 306.685227 215.564203 \r\nz\r\n\" style=\"fill:#1f77b4;\"/>\r\n   </g>\r\n   <g id=\"patch_29\">\r\n    <path clip-path=\"url(#peaccb2b5a3)\" d=\"M 316.503409 224.64 \r\nL 326.321591 224.64 \r\nL 326.321591 201.809432 \r\nL 316.503409 201.809432 \r\nz\r\n\" style=\"fill:#1f77b4;\"/>\r\n   </g>\r\n   <g id=\"patch_30\">\r\n    <path clip-path=\"url(#peaccb2b5a3)\" d=\"M 326.321591 224.64 \r\nL 336.139773 224.64 \r\nL 336.139773 153.397342 \r\nL 326.321591 153.397342 \r\nz\r\n\" style=\"fill:#1f77b4;\"/>\r\n   </g>\r\n   <g id=\"patch_31\">\r\n    <path clip-path=\"url(#peaccb2b5a3)\" d=\"M 336.139773 224.64 \r\nL 345.957955 224.64 \r\nL 345.957955 223.370329 \r\nL 336.139773 223.370329 \r\nz\r\n\" style=\"fill:#1f77b4;\"/>\r\n   </g>\r\n   <g id=\"patch_32\">\r\n    <path clip-path=\"url(#peaccb2b5a3)\" d=\"M 345.957955 224.64 \r\nL 355.776136 224.64 \r\nL 355.776136 224.181508 \r\nL 345.957955 224.181508 \r\nz\r\n\" style=\"fill:#1f77b4;\"/>\r\n   </g>\r\n   <g id=\"patch_33\">\r\n    <path clip-path=\"url(#peaccb2b5a3)\" d=\"M 355.776136 224.64 \r\nL 365.594318 224.64 \r\nL 365.594318 221.030842 \r\nL 355.776136 221.030842 \r\nz\r\n\" style=\"fill:#1f77b4;\"/>\r\n   </g>\r\n   <g id=\"matplotlib.axis_1\">\r\n    <g id=\"xtick_1\">\r\n     <g id=\"line2d_1\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 0 3.5 \r\n\" id=\"m90301eb827\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"61.230682\" xlink:href=\"#m90301eb827\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_1\">\r\n      <!-- 0 -->\r\n      <g transform=\"translate(58.049432 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 31.78125 66.40625 \r\nQ 24.171875 66.40625 20.328125 58.90625 \r\nQ 16.5 51.421875 16.5 36.375 \r\nQ 16.5 21.390625 20.328125 13.890625 \r\nQ 24.171875 6.390625 31.78125 6.390625 \r\nQ 39.453125 6.390625 43.28125 13.890625 \r\nQ 47.125 21.390625 47.125 36.375 \r\nQ 47.125 51.421875 43.28125 58.90625 \r\nQ 39.453125 66.40625 31.78125 66.40625 \r\nz\r\nM 31.78125 74.21875 \r\nQ 44.046875 74.21875 50.515625 64.515625 \r\nQ 56.984375 54.828125 56.984375 36.375 \r\nQ 56.984375 17.96875 50.515625 8.265625 \r\nQ 44.046875 -1.421875 31.78125 -1.421875 \r\nQ 19.53125 -1.421875 13.0625 8.265625 \r\nQ 6.59375 17.96875 6.59375 36.375 \r\nQ 6.59375 54.828125 13.0625 64.515625 \r\nQ 19.53125 74.21875 31.78125 74.21875 \r\nz\r\n\" id=\"DejaVuSans-48\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_2\">\r\n     <g id=\"line2d_2\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"111.957955\" xlink:href=\"#m90301eb827\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_2\">\r\n      <!-- 5 -->\r\n      <g transform=\"translate(108.776705 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 10.796875 72.90625 \r\nL 49.515625 72.90625 \r\nL 49.515625 64.59375 \r\nL 19.828125 64.59375 \r\nL 19.828125 46.734375 \r\nQ 21.96875 47.46875 24.109375 47.828125 \r\nQ 26.265625 48.1875 28.421875 48.1875 \r\nQ 40.625 48.1875 47.75 41.5 \r\nQ 54.890625 34.8125 54.890625 23.390625 \r\nQ 54.890625 11.625 47.5625 5.09375 \r\nQ 40.234375 -1.421875 26.90625 -1.421875 \r\nQ 22.3125 -1.421875 17.546875 -0.640625 \r\nQ 12.796875 0.140625 7.71875 1.703125 \r\nL 7.71875 11.625 \r\nQ 12.109375 9.234375 16.796875 8.0625 \r\nQ 21.484375 6.890625 26.703125 6.890625 \r\nQ 35.15625 6.890625 40.078125 11.328125 \r\nQ 45.015625 15.765625 45.015625 23.390625 \r\nQ 45.015625 31 40.078125 35.4375 \r\nQ 35.15625 39.890625 26.703125 39.890625 \r\nQ 22.75 39.890625 18.8125 39.015625 \r\nQ 14.890625 38.140625 10.796875 36.28125 \r\nz\r\n\" id=\"DejaVuSans-53\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_3\">\r\n     <g id=\"line2d_3\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"162.685227\" xlink:href=\"#m90301eb827\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_3\">\r\n      <!-- 10 -->\r\n      <g transform=\"translate(156.322727 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 12.40625 8.296875 \r\nL 28.515625 8.296875 \r\nL 28.515625 63.921875 \r\nL 10.984375 60.40625 \r\nL 10.984375 69.390625 \r\nL 28.421875 72.90625 \r\nL 38.28125 72.90625 \r\nL 38.28125 8.296875 \r\nL 54.390625 8.296875 \r\nL 54.390625 0 \r\nL 12.40625 0 \r\nz\r\n\" id=\"DejaVuSans-49\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_4\">\r\n     <g id=\"line2d_4\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"213.4125\" xlink:href=\"#m90301eb827\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_4\">\r\n      <!-- 15 -->\r\n      <g transform=\"translate(207.05 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_5\">\r\n     <g id=\"line2d_5\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"264.139773\" xlink:href=\"#m90301eb827\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_5\">\r\n      <!-- 20 -->\r\n      <g transform=\"translate(257.777273 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 19.1875 8.296875 \r\nL 53.609375 8.296875 \r\nL 53.609375 0 \r\nL 7.328125 0 \r\nL 7.328125 8.296875 \r\nQ 12.9375 14.109375 22.625 23.890625 \r\nQ 32.328125 33.6875 34.8125 36.53125 \r\nQ 39.546875 41.84375 41.421875 45.53125 \r\nQ 43.3125 49.21875 43.3125 52.78125 \r\nQ 43.3125 58.59375 39.234375 62.25 \r\nQ 35.15625 65.921875 28.609375 65.921875 \r\nQ 23.96875 65.921875 18.8125 64.3125 \r\nQ 13.671875 62.703125 7.8125 59.421875 \r\nL 7.8125 69.390625 \r\nQ 13.765625 71.78125 18.9375 73 \r\nQ 24.125 74.21875 28.421875 74.21875 \r\nQ 39.75 74.21875 46.484375 68.546875 \r\nQ 53.21875 62.890625 53.21875 53.421875 \r\nQ 53.21875 48.921875 51.53125 44.890625 \r\nQ 49.859375 40.875 45.40625 35.40625 \r\nQ 44.1875 33.984375 37.640625 27.21875 \r\nQ 31.109375 20.453125 19.1875 8.296875 \r\nz\r\n\" id=\"DejaVuSans-50\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_6\">\r\n     <g id=\"line2d_6\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"314.867045\" xlink:href=\"#m90301eb827\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_6\">\r\n      <!-- 25 -->\r\n      <g transform=\"translate(308.504545 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_7\">\r\n     <g id=\"line2d_7\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"365.594318\" xlink:href=\"#m90301eb827\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_7\">\r\n      <!-- 30 -->\r\n      <g transform=\"translate(359.231818 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 40.578125 39.3125 \r\nQ 47.65625 37.796875 51.625 33 \r\nQ 55.609375 28.21875 55.609375 21.1875 \r\nQ 55.609375 10.40625 48.1875 4.484375 \r\nQ 40.765625 -1.421875 27.09375 -1.421875 \r\nQ 22.515625 -1.421875 17.65625 -0.515625 \r\nQ 12.796875 0.390625 7.625 2.203125 \r\nL 7.625 11.71875 \r\nQ 11.71875 9.328125 16.59375 8.109375 \r\nQ 21.484375 6.890625 26.8125 6.890625 \r\nQ 36.078125 6.890625 40.9375 10.546875 \r\nQ 45.796875 14.203125 45.796875 21.1875 \r\nQ 45.796875 27.640625 41.28125 31.265625 \r\nQ 36.765625 34.90625 28.71875 34.90625 \r\nL 20.21875 34.90625 \r\nL 20.21875 43.015625 \r\nL 29.109375 43.015625 \r\nQ 36.375 43.015625 40.234375 45.921875 \r\nQ 44.09375 48.828125 44.09375 54.296875 \r\nQ 44.09375 59.90625 40.109375 62.90625 \r\nQ 36.140625 65.921875 28.71875 65.921875 \r\nQ 24.65625 65.921875 20.015625 65.03125 \r\nQ 15.375 64.15625 9.8125 62.3125 \r\nL 9.8125 71.09375 \r\nQ 15.4375 72.65625 20.34375 73.4375 \r\nQ 25.25 74.21875 29.59375 74.21875 \r\nQ 40.828125 74.21875 47.359375 69.109375 \r\nQ 53.90625 64.015625 53.90625 55.328125 \r\nQ 53.90625 49.265625 50.4375 45.09375 \r\nQ 46.96875 40.921875 40.578125 39.3125 \r\nz\r\n\" id=\"DejaVuSans-51\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-51\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_2\">\r\n    <g id=\"ytick_1\">\r\n     <g id=\"line2d_8\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL -3.5 0 \r\n\" id=\"me487a278da\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"46.0125\" xlink:href=\"#me487a278da\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_8\">\r\n      <!-- 0 -->\r\n      <g transform=\"translate(32.65 228.439219)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_2\">\r\n     <g id=\"line2d_9\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"46.0125\" xlink:href=\"#me487a278da\" y=\"195.249464\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_9\">\r\n      <!-- 2500 -->\r\n      <g transform=\"translate(13.5625 199.048683)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_3\">\r\n     <g id=\"line2d_10\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"46.0125\" xlink:href=\"#me487a278da\" y=\"165.858929\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_10\">\r\n      <!-- 5000 -->\r\n      <g transform=\"translate(13.5625 169.658147)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-53\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_4\">\r\n     <g id=\"line2d_11\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"46.0125\" xlink:href=\"#me487a278da\" y=\"136.468393\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_11\">\r\n      <!-- 7500 -->\r\n      <g transform=\"translate(13.5625 140.267612)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 8.203125 72.90625 \r\nL 55.078125 72.90625 \r\nL 55.078125 68.703125 \r\nL 28.609375 0 \r\nL 18.3125 0 \r\nL 43.21875 64.59375 \r\nL 8.203125 64.59375 \r\nz\r\n\" id=\"DejaVuSans-55\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-55\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_5\">\r\n     <g id=\"line2d_12\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"46.0125\" xlink:href=\"#me487a278da\" y=\"107.077857\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_12\">\r\n      <!-- 10000 -->\r\n      <g transform=\"translate(7.2 110.877076)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_6\">\r\n     <g id=\"line2d_13\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"46.0125\" xlink:href=\"#me487a278da\" y=\"77.687322\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_13\">\r\n      <!-- 12500 -->\r\n      <g transform=\"translate(7.2 81.48654)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-53\"/>\r\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_7\">\r\n     <g id=\"line2d_14\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"46.0125\" xlink:href=\"#me487a278da\" y=\"48.296786\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_14\">\r\n      <!-- 15000 -->\r\n      <g transform=\"translate(7.2 52.096005)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_8\">\r\n     <g id=\"line2d_15\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"46.0125\" xlink:href=\"#me487a278da\" y=\"18.90625\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_15\">\r\n      <!-- 17500 -->\r\n      <g transform=\"translate(7.2 22.705469)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-55\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-53\"/>\r\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"patch_34\">\r\n    <path d=\"M 46.0125 224.64 \r\nL 46.0125 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_35\">\r\n    <path d=\"M 380.8125 224.64 \r\nL 380.8125 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_36\">\r\n    <path d=\"M 46.0125 224.64 \r\nL 380.8125 224.64 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_37\">\r\n    <path d=\"M 46.0125 7.2 \r\nL 380.8125 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"peaccb2b5a3\">\r\n   <rect height=\"217.44\" width=\"334.8\" x=\"46.0125\" y=\"7.2\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAD4CAYAAAAO9oqkAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Il7ecAAAACXBIWXMAAAsTAAALEwEAmpwYAAATmklEQVR4nO3df4xdZ53f8fenDqEoSxRn41peO9SGmq0g2hoYhVQFlJKSOKFah2qV2lI3hkYYRCKBqFTM9o+kbCN5t7C0kaiRWSwcCWJSQhprCQ3eiG66UgMegzeJE7KeBEex5dizGDabssrW4ds/7jO7B2dmPJ575/f7JV3Nud/z63l8PPfj85xzj1NVSJKWtr831w2QJM09w0CSZBhIkgwDSRKGgSQJuGCuGzBdl112Wa1du3aumyFJC8rBgwf/oqpWnF1fsGGwdu1ahoeH57oZkrSgJHluvLrDRJIkw0CSZBhIkjAMJEkYBpIkDANJEoaBJIkphEGS3UlOJXmiU/t6kkPtdTTJoVZfm+SvO/O+2FnnHUkeTzKS5K4kafVLk+xPcqT9XD4D/ZQkTWIqZwZfATZ2C1X1r6tqQ1VtAO4DvtmZ/czYvKr6aKe+E/gwsL69xra5HXi4qtYDD7f3kqRZdM5vIFfVI0nWjjev/ev+JuC9k20jySrg4qp6tL2/G7gR+DawCbi6LboH+F/Ap6bSeGmxWbv9W1Na7uiO989wS7TU9HvN4N3Ayao60qmtS/LDJH+S5N2ttho41lnmWKsBrKyqE236BWDlRDtLsi3JcJLh0dHRPpsuSRrTbxhsAe7pvD8BvKGq3gZ8EvhakounurHq/R+cE/4/nFW1q6qGqmpoxYpXPWdJkjRN035QXZILgH8FvGOsVlUvAy+36YNJngHeDBwH1nRWX9NqACeTrKqqE2046dR02yRJmp5+zgz+BfCjqvrb4Z8kK5Isa9NvpHeh+Nk2DPRikqvadYabgQfaavuArW16a6cuSZolU7m19B7g/wC/nuRYklvarM388hARwHuAx9qtpt8APlpVp9u8jwF/CIwAz9C7eAywA3hfkiP0AmbH9LsjSZqOqdxNtGWC+gfHqd1H71bT8ZYfBq4Yp/4T4JpztUOSNHP8BrIkyTCQJBkGkiQMA0kShoEkCcNAkoRhIEnCMJAkYRhIkjAMJEkYBpIkDANJEoaBJAnDQJKEYSBJwjCQJGEYSJIwDCRJGAaSJAwDSRJTCIMku5OcSvJEp3ZHkuNJDrXXDZ15n04ykuTpJNd16htbbSTJ9k59XZLvtfrXk1w4yA5Kks5tKmcGXwE2jlP/fFVtaK8HAZK8BdgMvLWt89+SLEuyDPgCcD3wFmBLWxbg99q2/hHwU+CWfjokSTp/5wyDqnoEOD3F7W0C9lbVy1X1Y2AEuLK9Rqrq2ar6G2AvsClJgPcC32jr7wFuPL8uSJL61c81g9uSPNaGkZa32mrg+c4yx1ptovqvAj+rqjNn1ceVZFuS4STDo6OjfTRdktQ13TDYCbwJ2ACcAD43qAZNpqp2VdVQVQ2tWLFiNnYpSUvCBdNZqapOjk0n+RLwR+3tceDyzqJrWo0J6j8BLklyQTs76C4vSZol0zozSLKq8/YDwNidRvuAzUlem2QdsB74PnAAWN/uHLqQ3kXmfVVVwHeB32rrbwUemE6bJEnTd84zgyT3AFcDlyU5BtwOXJ1kA1DAUeAjAFV1OMm9wJPAGeDWqnqlbec24CFgGbC7qg63XXwK2JvkPwE/BL48qM5JkqbmnGFQVVvGKU/4gV1VdwJ3jlN/EHhwnPqz9O42kiTNEb+BLEkyDCRJhoEkCcNAkoRhIEnCMJAkYRhIkjAMJEkYBpIkDANJEoaBJAnDQJKEYSBJwjCQJGEYSJIwDCRJGAaSJAwDSRKGgSQJw0CShGEgSWIKYZBkd5JTSZ7o1P5zkh8leSzJ/UkuafW1Sf46yaH2+mJnnXckeTzJSJK7kqTVL02yP8mR9nP5DPRTkjSJqZwZfAXYeFZtP3BFVf0G8OfApzvznqmqDe310U59J/BhYH17jW1zO/BwVa0HHm7vJUmz6JxhUFWPAKfPqn2nqs60t48CaybbRpJVwMVV9WhVFXA3cGObvQnY06b3dOqSpFkyiGsG/xb4duf9uiQ/TPInSd7daquBY51ljrUawMqqOtGmXwBWTrSjJNuSDCcZHh0dHUDTJUnQZxgk+Q/AGeCrrXQCeENVvQ34JPC1JBdPdXvtrKEmmb+rqoaqamjFihV9tFyS1HXBdFdM8kHgXwLXtA9xqupl4OU2fTDJM8CbgeP88lDSmlYDOJlkVVWdaMNJp6bbJknS9EzrzCDJRuDfA79ZVT/v1FckWdam30jvQvGzbRjoxSRXtbuIbgYeaKvtA7a26a2duiRplpzzzCDJPcDVwGVJjgG307t76LXA/naH6KPtzqH3AJ9J8v+AXwAfraqxi88fo3dn0uvoXWMYu86wA7g3yS3Ac8BNA+mZJGnKzhkGVbVlnPKXJ1j2PuC+CeYNA1eMU/8JcM252iFJmjl+A1mSZBhIkgwDSRKGgSQJw0CShGEgScIwkCRhGEiSMAwkSRgGkiQMA0kShoEkCcNAkoRhIEnCMJAkYRhIkjAMJEkYBpIkDANJEoaBJIkphkGS3UlOJXmiU7s0yf4kR9rP5a2eJHclGUnyWJK3d9bZ2pY/kmRrp/6OJI+3de5KkkF2UpI0uameGXwF2HhWbTvwcFWtBx5u7wGuB9a31zZgJ/TCA7gdeCdwJXD7WIC0ZT7cWe/sfUmSZtCUwqCqHgFOn1XeBOxp03uAGzv1u6vnUeCSJKuA64D9VXW6qn4K7Ac2tnkXV9WjVVXA3Z1tSZJmQT/XDFZW1Yk2/QKwsk2vBp7vLHes1SarHxun/ipJtiUZTjI8OjraR9MlSV0XDGIjVVVJahDbOsd+dgG7AIaGhmZ8f3q1tdu/dc5lju54/yy0RNIg9XNmcLIN8dB+nmr148DlneXWtNpk9TXj1CVJs6SfMNgHjN0RtBV4oFO/ud1VdBXwl2046SHg2iTL24Xja4GH2rwXk1zV7iK6ubMtSdIsmNIwUZJ7gKuBy5Ico3dX0A7g3iS3AM8BN7XFHwRuAEaAnwMfAqiq00l+FzjQlvtMVY1dlP4YvTuWXgd8u70kSbNkSmFQVVsmmHXNOMsWcOsE29kN7B6nPgxcMZW2SJIGz28gS5IMA0mSYSBJwjCQJGEYSJIwDCRJGAaSJAwDSRKGgSSJAT21VJqOqTwBFXwK6nzh8VrcPDOQJBkGkiTDQJKEYSBJwjCQJGEYSJIwDCRJGAaSJAwDSRKGgSSJPsIgya8nOdR5vZjkE0nuSHK8U7+hs86nk4wkeTrJdZ36xlYbSbK9305Jks7PtJ9NVFVPAxsAkiwDjgP3Ax8CPl9Vn+0un+QtwGbgrcCvAX+c5M1t9heA9wHHgANJ9lXVk9NtmyTp/AzqQXXXAM9U1XNJJlpmE7C3ql4GfpxkBLiyzRupqmcBkuxtyxoGkjRLBnXNYDNwT+f9bUkeS7I7yfJWWw0831nmWKtNVJckzZK+wyDJhcBvAv+9lXYCb6I3hHQC+Fy/++jsa1uS4STDo6Ojg9qsJC15gzgzuB74QVWdBKiqk1X1SlX9AvgSfzcUdBy4vLPemlabqP4qVbWrqoaqamjFihUDaLokCQYTBlvoDBElWdWZ9wHgiTa9D9ic5LVJ1gHrge8DB4D1Sda1s4zNbVlJ0izp6wJykovo3QX0kU7595NsAAo4Ojavqg4nuZfeheEzwK1V9Urbzm3AQ8AyYHdVHe6nXZKk89NXGFTV/wV+9azab0+y/J3AnePUHwQe7KctkqTp8xvIkiTDQJJkGEiSMAwkSQzucRRa4NZu/9ZcN0HSHPLMQJJkGEiSDANJEoaBJAnDQJKEYSBJwjCQJOH3DBY9vz8gnb+p/t4c3fH+GW7J7PHMQJJkGEiSDANJEoaBJAnDQJKEYSBJwjCQJGEYSJIYQBgkOZrk8SSHkgy32qVJ9ic50n4ub/UkuSvJSJLHkry9s52tbfkjSbb22y5J0tQN6szgn1fVhqoaau+3Aw9X1Xrg4fYe4HpgfXttA3ZCLzyA24F3AlcCt48FiCRp5s3UMNEmYE+b3gPc2KnfXT2PApckWQVcB+yvqtNV9VNgP7BxhtomSTrLIMKggO8kOZhkW6utrKoTbfoFYGWbXg0831n3WKtNVP8lSbYlGU4yPDo6OoCmS5JgMA+qe1dVHU/yD4D9SX7UnVlVlaQGsB+qahewC2BoaGgg25QkDeDMoKqOt5+ngPvpjfmfbMM/tJ+n2uLHgcs7q69ptYnqkqRZ0FcYJLkoyevHpoFrgSeAfcDYHUFbgQfa9D7g5nZX0VXAX7bhpIeAa5MsbxeOr201SdIs6HeYaCVwf5KxbX2tqv5nkgPAvUluAZ4DbmrLPwjcAIwAPwc+BFBVp5P8LnCgLfeZqjrdZ9skSVPUVxhU1bPAPxmn/hPgmnHqBdw6wbZ2A7v7aY8kaXr8BrIkyTCQJBkGkiQMA0kShoEkCcNAkoRhIEnCMJAkYRhIkjAMJEkYBpIkDANJEoaBJAnDQJKEYSBJwjCQJNH//3Qmzbi12781peWO7nj/DLdEWrw8M5AkGQaSJIeJ5h2HRCTNhWmfGSS5PMl3kzyZ5HCSj7f6HUmOJznUXjd01vl0kpEkTye5rlPf2GojSbb31yVJ0vnq58zgDPDvquoHSV4PHEyyv837fFV9trtwkrcAm4G3Ar8G/HGSN7fZXwDeBxwDDiTZV1VP9tG2eWeq/+KXpLkw7TCoqhPAiTb9V0meAlZPssomYG9VvQz8OMkIcGWbN1JVzwIk2duWXVRhIEnz2UAuICdZC7wN+F4r3ZbksSS7kyxvtdXA853VjrXaRPXx9rMtyXCS4dHR0UE0XZLEAMIgya8A9wGfqKoXgZ3Am4AN9M4cPtfvPsZU1a6qGqqqoRUrVgxqs5K05PV1N1GS19ALgq9W1TcBqupkZ/6XgD9qb48Dl3dWX9NqTFKXNA7vOtOg9XM3UYAvA09V1R906qs6i30AeKJN7wM2J3ltknXAeuD7wAFgfZJ1SS6kd5F533TbJUk6f/2cGfwz4LeBx5McarXfAbYk2QAUcBT4CEBVHU5yL70Lw2eAW6vqFYAktwEPAcuA3VV1uI92SZLOUz93E/0pkHFmPTjJOncCd45Tf3Cy9SRJM8tvIEtLnN+BERgGkpYQg29iPqhOkmQYSJIMA0kShoEkCcNAkoRhIEnCW0u1BE3l9kKf6aOlxjMDSZJhIEkyDCRJGAaSJLyALM0Kn4mj+c4wkDRQ/i9sC5NhIGnB88yrf4aBBm6ufjH9QJCmb0mGgaexkvTLlmQYSIPi2YgWC28tlSR5ZjAZh5OkmePv16vN5XOz5k0YJNkI/FdgGfCHVbVjjps0ZXMxVODwxMzyz1dLzbwIgyTLgC8A7wOOAQeS7KuqJ+e2ZZLmmsE8O+ZFGABXAiNV9SxAkr3AJsAwkPrgB+nMWkx/vvMlDFYDz3feHwPeefZCSbYB29rbl5I8Pc39XQb8xTTXnW8WS18WSz/AvsxXi6Iv+b2++/EPxyvOlzCYkqraBezqdztJhqtqaABNmnOLpS+LpR9gX+arxdKXmerHfLm19Dhweef9mlaTJM2C+RIGB4D1SdYluRDYDOyb4zZJ0pIxL4aJqupMktuAh+jdWrq7qg7P4C77HmqaRxZLXxZLP8C+zFeLpS8z0o9U1UxsV5K0gMyXYSJJ0hwyDCRJSy8MkmxM8nSSkSTb57o905XkaJLHkxxKMjzX7TkfSXYnOZXkiU7t0iT7kxxpP5fPZRunaoK+3JHkeDs2h5LcMJdtnIoklyf5bpInkxxO8vFWX3DHZZK+LMTj8veTfD/Jn7W+/MdWX5fke+1z7Ovtxpv+9rWUrhm0x178OZ3HXgBbFuJjL5IcBYaqasF9iSbJe4CXgLur6opW+33gdFXtaCG9vKo+NZftnIoJ+nIH8FJVfXYu23Y+kqwCVlXVD5K8HjgI3Ah8kAV2XCbpy00svOMS4KKqeinJa4A/BT4OfBL4ZlXtTfJF4M+qamc/+1pqZwZ/+9iLqvobYOyxF5pFVfUIcPqs8iZgT5veQ++Xd96boC8LTlWdqKoftOm/Ap6i92SABXdcJunLglM9L7W3r2mvAt4LfKPVB3JclloYjPfYiwX5l4TeX4jvJDnYHtOx0K2sqhNt+gVg5Vw2ZgBuS/JYG0aa90MrXUnWAm8DvscCPy5n9QUW4HFJsizJIeAUsB94BvhZVZ1piwzkc2yphcFi8q6qejtwPXBrG65YFKo3drmQxy93Am8CNgAngM/NaWvOQ5JfAe4DPlFVL3bnLbTjMk5fFuRxqapXqmoDvSczXAn845nYz1ILg0Xz2IuqOt5+ngLup/eXZCE72cZ6x8Z8T81xe6atqk62X+BfAF9igRybNiZ9H/DVqvpmKy/I4zJeXxbqcRlTVT8Dvgv8U+CSJGNfGh7I59hSC4NF8diLJBe1C2MkuQi4Fnhi8rXmvX3A1ja9FXhgDtvSl7EPz+YDLIBj0y5Ufhl4qqr+oDNrwR2XifqyQI/LiiSXtOnX0bv55Sl6ofBbbbGBHJcldTcRQLud7L/wd4+9uHNuW3T+kryR3tkA9B4p8rWF1I8k9wBX03uk8EngduB/APcCbwCeA26qqnl/YXaCvlxNbyiigKPARzrj7vNSkncB/xt4HPhFK/8OvbH2BXVcJunLFhbecfkNeheIl9H7x/u9VfWZ9hmwF7gU+CHwb6rq5b72tdTCQJL0akttmEiSNA7DQJJkGEiSDANJEoaBJAnDQJKEYSBJAv4/861N273+f70AAAAASUVORK5CYII=\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "a=plt.hist(added.geo_level_1_id.where(added.damage_grade==3),bins=31)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "31"
            ]
          },
          "metadata": {},
          "execution_count": 127
        }
      ],
      "source": [
        "len(a[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {},
      "outputs": [],
      "source": [
        "quake_effect={}\n",
        "maxi=max(a[0])\n",
        "for inx,val in enumerate(a[0]):\n",
        "    quake_effect[inx]=1-(maxi-val)/maxi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {},
      "outputs": [],
      "source": [
        "eartquakeDist=[]\n",
        "for idx,i in enumerate(X_train.geo_level_1_id):\n",
        "    eartquakeDist.append(quake_effect[X_train.geo_level_1_id[idx]])\n",
        "\n",
        "eartquakeDist2=[]\n",
        "for idx,i in enumerate(X_pred.geo_level_1_id):\n",
        "    eartquakeDist2.append(quake_effect[X_pred.geo_level_1_id[idx]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 130
        }
      ],
      "source": [
        "eartquakeDist2[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {},
      "outputs": [],
      "source": [
        "added2=pd.concat([X_train.geo_level_1_id,X_train.geo_level_2_id,X_train.geo_level_3_id,y_train.damage_grade,pd.DataFrame(eartquakeDist,columns=[\"QuakeForce\"])], axis=1)\n",
        "\n",
        "X_train_geolevel=pd.concat([X_train,pd.DataFrame(eartquakeDist,columns=[\"QuakeForce\"])], axis=1)\n",
        "X_pred_geolevel=pd.concat([X_pred,pd.DataFrame(eartquakeDist2,columns=[\"QuakeForce\"])], axis=1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "land_surface_condition cseréje megy\n",
            "foundation_type cseréje megy\n",
            "roof_type cseréje megy\n",
            "ground_floor_type cseréje megy\n",
            "other_floor_type cseréje megy\n",
            "position cseréje megy\n",
            "plan_configuration cseréje megy\n",
            "legal_ownership_status cseréje megy\n",
            "land_surface_condition cseréje megy\n",
            "foundation_type cseréje megy\n",
            "roof_type cseréje megy\n",
            "ground_floor_type cseréje megy\n",
            "other_floor_type cseréje megy\n",
            "position cseréje megy\n",
            "plan_configuration cseréje megy\n",
            "legal_ownership_status cseréje megy\n"
          ]
        }
      ],
      "source": [
        "X_pred_conv=create_base_data(X_pred_geolevel)\n",
        "X_train_conv=create_base_data(X_train_geolevel)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {},
      "outputs": [],
      "source": [
        "#--------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   count_floors_pre_eq  age  area_percentage  height_percentage  \\\n",
              "0                    3   20                7                  6   \n",
              "1                    2   25               13                  5   \n",
              "2                    2    5                4                  5   \n",
              "3                    1    0               19                  3   \n",
              "4                    3   15                8                  7   \n",
              "\n",
              "   has_superstructure_adobe_mud  has_superstructure_mud_mortar_stone  \\\n",
              "0                             0                                    1   \n",
              "1                             0                                    1   \n",
              "2                             0                                    1   \n",
              "3                             0                                    0   \n",
              "4                             0                                    1   \n",
              "\n",
              "   has_superstructure_stone_flag  has_superstructure_cement_mortar_stone  \\\n",
              "0                              0                                       0   \n",
              "1                              0                                       0   \n",
              "2                              0                                       0   \n",
              "3                              0                                       0   \n",
              "4                              0                                       0   \n",
              "\n",
              "   has_superstructure_mud_mortar_brick  \\\n",
              "0                                    0   \n",
              "1                                    0   \n",
              "2                                    0   \n",
              "3                                    0   \n",
              "4                                    0   \n",
              "\n",
              "   has_superstructure_cement_mortar_brick  ...  plan_configuration_a  \\\n",
              "0                                       0  ...                   0.0   \n",
              "1                                       0  ...                   0.0   \n",
              "2                                       0  ...                   0.0   \n",
              "3                                       1  ...                   0.0   \n",
              "4                                       0  ...                   0.0   \n",
              "\n",
              "   plan_configuration_q  plan_configuration_u  plan_configuration_n  \\\n",
              "0                   0.0                   0.0                   0.0   \n",
              "1                   0.0                   0.0                   0.0   \n",
              "2                   0.0                   0.0                   0.0   \n",
              "3                   0.0                   0.0                   0.0   \n",
              "4                   0.0                   0.0                   0.0   \n",
              "\n",
              "   plan_configuration_o  legal_ownership_status_a  legal_ownership_status_w  \\\n",
              "0                   0.0                       0.0                       0.0   \n",
              "1                   0.0                       0.0                       0.0   \n",
              "2                   0.0                       0.0                       0.0   \n",
              "3                   0.0                       0.0                       0.0   \n",
              "4                   0.0                       0.0                       0.0   \n",
              "\n",
              "   legal_ownership_status_r  legal_ownership_status_v     geopos  \n",
              "0                       0.0                       1.0  17.417622  \n",
              "1                       0.0                       1.0   6.098800  \n",
              "2                       0.0                       1.0  22.013313  \n",
              "3                       0.0                       1.0  26.027328  \n",
              "4                       0.0                       1.0  17.202505  \n",
              "\n",
              "[5 rows x 67 columns]"
            ],
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>count_floors_pre_eq</th>\n      <th>age</th>\n      <th>area_percentage</th>\n      <th>height_percentage</th>\n      <th>has_superstructure_adobe_mud</th>\n      <th>has_superstructure_mud_mortar_stone</th>\n      <th>has_superstructure_stone_flag</th>\n      <th>has_superstructure_cement_mortar_stone</th>\n      <th>has_superstructure_mud_mortar_brick</th>\n      <th>has_superstructure_cement_mortar_brick</th>\n      <th>...</th>\n      <th>plan_configuration_a</th>\n      <th>plan_configuration_q</th>\n      <th>plan_configuration_u</th>\n      <th>plan_configuration_n</th>\n      <th>plan_configuration_o</th>\n      <th>legal_ownership_status_a</th>\n      <th>legal_ownership_status_w</th>\n      <th>legal_ownership_status_r</th>\n      <th>legal_ownership_status_v</th>\n      <th>geopos</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>3</td>\n      <td>20</td>\n      <td>7</td>\n      <td>6</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>17.417622</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>25</td>\n      <td>13</td>\n      <td>5</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>6.098800</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>5</td>\n      <td>4</td>\n      <td>5</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>22.013313</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>0</td>\n      <td>19</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>26.027328</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>3</td>\n      <td>15</td>\n      <td>8</td>\n      <td>7</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>17.202505</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 67 columns</p>\n</div>"
          },
          "metadata": {},
          "execution_count": 134
        }
      ],
      "source": [
        "#\n",
        "X_train_ok=kill_columns(X_train_conv)\n",
        "X_pred_ok=kill_columns(X_pred_conv)\n",
        "y_train_ok=kill_columns(y_train)\n",
        "\n",
        "\n",
        "\n",
        "X_train_ok.to_csv(basedir+\"/tmp/X_tran_ok.csv\",index=False)\n",
        "X_pred_ok.to_csv(basedir+\"/tmp/X_pred_ok.csv\",index=False)\n",
        "y_train_ok.to_csv(basedir+\"/tmp/y_train_ok.csv\",index=False)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#f=open(basedir+\"/tmp/similarity.csv\",\"a\")\n",
        "X_train_ok=pd.read_csv(basedir+\"/tmp/X_tran_ok.csv\",)\n",
        "X_pred_ok=pd.read_csv(basedir+\"/tmp/X_pred_ok.csv\")\n",
        "y_train_ok=pd.read_csv(basedir+\"/tmp/y_train_ok.csv\")\n",
        "\n",
        "X_train_ok.head()\n",
        "X_pred_ok.head()\n",
        "#print(\"Ready\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZkluH7RwmL_M"
      },
      "source": [
        "X_pred_ok=kill_columns(X_pred_ok)\n",
        "X_train_ok=kill_columns(X_train_ok)\n",
        "y_train_ok=kill_columns(y_train_ok)\n",
        "\n",
        "t=[1, 2, 3]\n",
        "columnname=\"damage_grade\"\n",
        "key=create_dict(columnname,t)\n",
        "\n",
        "\n",
        "checkvalues(y_train_ok,columnname,key)\n",
        "createcolumn(y_train_ok,columnname,key)"
      ],
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "damage_grade ellenőrzése !\n",
            "\n",
            "260601 mintából 0 db nem volt megfelelő\n",
            "damage_grade cseréje megy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        damage_grade  damage_grade_1  damage_grade_2  damage_grade_3\n",
              "0                  3             0.0             0.0             1.0\n",
              "1                  2             0.0             1.0             0.0\n",
              "2                  3             0.0             0.0             1.0\n",
              "3                  2             0.0             1.0             0.0\n",
              "4                  3             0.0             0.0             1.0\n",
              "...              ...             ...             ...             ...\n",
              "260596             2             0.0             1.0             0.0\n",
              "260597             3             0.0             0.0             1.0\n",
              "260598             3             0.0             0.0             1.0\n",
              "260599             2             0.0             1.0             0.0\n",
              "260600             3             0.0             0.0             1.0\n",
              "\n",
              "[260601 rows x 4 columns]"
            ],
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>damage_grade</th>\n      <th>damage_grade_1</th>\n      <th>damage_grade_2</th>\n      <th>damage_grade_3</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>3</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>3</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>260596</th>\n      <td>2</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>260597</th>\n      <td>3</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>260598</th>\n      <td>3</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>260599</th>\n      <td>2</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>260600</th>\n      <td>3</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>260601 rows × 4 columns</p>\n</div>"
          },
          "metadata": {},
          "execution_count": 136
        }
      ],
      "source": [
        "y_train_ok"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q0rn-7TBQfRp"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "\n",
        "scaler2=MinMaxScaler()\n",
        "X_train_scale=scaler2.fit_transform(X_train_ok)\n",
        "\n",
        "#scaler1=StandardScaler()\n",
        "X_pred_scale=scaler2.fit_transform(X_pred_ok)\n",
        "\"\"\"\n",
        "scaler3=StandardScaler()\n",
        "y_train_scale=scaler3.fit_transform(y_train_ok)\n",
        "\"\"\"\n",
        "y_train_np=y_train_ok.to_numpy()\n",
        "\n",
        "# szétszedjük a train és test részekre\n",
        "from sklearn.model_selection import train_test_split\n",
        "#X_train_train, X_train_test,y_train_train, y_train_test  = train_test_split( X_train_scale, y_train_scale, test_size=0.10, random_state=0)\n",
        "X_train_train, X_train_test,y_train_train, y_train_test  = train_test_split( X_train_scale, y_train_np, test_size=0.01, random_state=0)\n",
        "\n",
        "\n"
      ],
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {},
      "outputs": [],
      "source": [
        "def outlierDropfrom_df(df,inxlist):\n",
        "    a=df\n",
        "    out=a.drop(inxlist,axis=0)\n",
        "    return(out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {},
      "outputs": [],
      "source": [
        "def outlierDropfrom_numpyarray(na,inxlist):\n",
        "    \n",
        "    out=np.delete (na ,inxlist, axis=0)\n",
        "    return(out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calc_the_range(y_train,hist):    \n",
        "    num1=sum(1 for val in y_train if val==1)\n",
        "    #print(num1)\n",
        "    num2=sum(1 for val in y_train if val==2)\n",
        "    #print(num2)\n",
        "    num3=sum(1 for val in y_train if val==3)\n",
        "    #print(num3)\n",
        "    \n",
        "    out=(num1,num1+num2,num1+num2+num3)\n",
        "    print(\"out: \" ,out)\n",
        "    steps=[]\n",
        "    for i in range(len(hist[0])):\n",
        "        print(hist[0][i], end=\"\")\n",
        "        if hist[0][i]>out[0]:\n",
        "            steps.append(i)\n",
        "            break\n",
        "    for i in range(len(hist[0])):\n",
        "        #print(hist[0][i])\n",
        "        if hist[0][i]>out[1]:\n",
        "            steps.append(i)\n",
        "            break\n",
        "    print(\"step:\",steps)\n",
        "    limits_out=(hist[1][steps[0]],hist[1][steps[1]])\n",
        "    print(\"Limits:\",limits_out)\n",
        "    return (limits_out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {},
      "outputs": [],
      "source": [
        "def conv_a_floatlist(alist, range_x):\n",
        "    o=[]\n",
        "    print(\"conv:\" ,range_x)\n",
        "    for i in alist:\n",
        "        if i<range_x[0]:\n",
        "            o.append(1)\n",
        "        if range_x[0]<= i <range_x[1]:\n",
        "            o.append(2)\n",
        "        if range_x[1]<=i:\n",
        "            o.append(3)\n",
        "    return(o)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "\n",
        "# explicitly require this experimental feature\n",
        "from xgboost import XGBClassifier  \n",
        "from sklearn.experimental import enable_hist_gradient_boosting  # noqa\n",
        "# now you can import normally from ensemble\n",
        "from sklearn.ensemble import HistGradientBoostingClassifier\n",
        "from sklearn.ensemble import HistGradientBoostingRegressor\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "#clf = GradientBoostingClassifier(random_state=0,n_estimators=525, max_depth=43, verbose=3, tol=0.0001)\n",
        "#clf = HistGradientBoostingClassifier(random_state=1, max_depth=53, verbose=3, tol=0.000001, learning_rate=0.6, max_iter=24000,early_stopping=False,min_samples_leaf=5000, scoring=\"balanced_accuracy\")\n",
        "clf=HistGradientBoostingRegressor(random_state=1, max_depth=113, verbose=3, tol=0.00000001, learning_rate=0.1, max_iter=4000,early_stopping=True,min_samples_leaf=3, loss=\"poisson\",n_iter_no_change=80, warm_start=True)\n",
        "#clf.fit(X_train_train, y_train_train)\n",
        "\n",
        "#clf1= MLPClassifier(random_state=1, max_iter=1,verbose=True, tol=1e-10, hidden_layer_sizes=(60,), warm_start=True)\n",
        "#clf2= MLPClassifier(random_state=1, max_iter=1,verbose=True, tol=1e-10, hidden_layer_sizes=(100,20), warm_start=True)\n",
        "#clf3= MLPClassifier(random_state=1, max_iter=1,verbose=True, tol=1e-10, hidden_layer_sizes=(100,30), warm_start=True)\n",
        "\n",
        "'''\n",
        "clf1=GradientBoostingRegressor(random_state=1, max_depth=113, verbose=1, tol=0.00000001, learning_rate=0.1, n_estimators=20,min_samples_leaf=3, loss=\"quantile\",n_iter_no_change=80, warm_start=True,)\n",
        "clf2=GradientBoostingRegressor(random_state=1, max_depth=113, verbose=1, tol=0.00000001, learning_rate=0.1, n_estimators=2,min_samples_leaf=3, loss=\"quantile\",n_iter_no_change=80, warm_start=True)\n",
        "clf3=GradientBoostingRegressor(random_state=1, max_depth=113, verbose=1, tol=0.00000001, learning_rate=0.1, n_estimators=2,min_samples_leaf=3, loss=\"quantile\",n_iter_no_change=80, warm_start=True)\n",
        "'''\n",
        "clf1=GradientBoostingRegressor(verbose=3,warm_start=True,n_estimators=5)\n",
        "clf2=GradientBoostingRegressor(verbose=3,warm_start=True,n_estimators=5)\n",
        "clf3=GradientBoostingRegressor(verbose=3,warm_start=True,n_estimators=5)\n",
        "\n",
        "\n",
        "# clf1= GradientBoostingClassifier(verbose=3, tol=1e-10, warm_start=True,n_estimators=10)\n",
        "# clf2= GradientBoostingClassifier(verbose=3, tol=1e-10, warm_start=True,n_estimators=10)\n",
        "# clf3= GradientBoostingClassifier(verbose=3, tol=1e-10, warm_start=True,n_estimators=10)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "yhiba=[]\n",
        "for inx,i in enumerate(y_train_train):\n",
        "    yhiba.append(0)\n",
        "yhiba[0]=1\n"
      ]
    },
    {
      "source": [
        "n=20\n",
        "i=0\n",
        "yt0=y_train_train[:,0]\n",
        "yt1=y_train_train[:,1]\n",
        "yt2=y_train_train[:,2]\n",
        "yt3=y_train_train[:,3]\n",
        "clf1.fit(X_train_train,yt1)\n",
        "clf2.fit(X_train_train,yt2)\n",
        "clf3.fit(X_train_train,yt3)\n"
      ],
      "cell_type": "code",
      "metadata": {},
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Iter       Train Loss   Remaining Time \n",
            "         1           0.0833            3.47s\n",
            "         2           0.0802            2.57s\n",
            "         3           0.0776            1.69s\n",
            "         4           0.0755            0.84s\n",
            "         5           0.0736            0.00s\n",
            "      Iter       Train Loss   Remaining Time \n",
            "         1           0.2407            3.42s\n",
            "         2           0.2370            2.55s\n",
            "         3           0.2339            1.69s\n",
            "         4           0.2314            0.84s\n",
            "         5           0.2291            0.00s\n",
            "      Iter       Train Loss   Remaining Time \n",
            "         1           0.2137            3.32s\n",
            "         2           0.2064            2.49s\n",
            "         3           0.2005            1.66s\n",
            "         4           0.1956            0.83s\n",
            "         5           0.1915            0.00s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GradientBoostingRegressor(n_estimators=5, verbose=3, warm_start=True)"
            ]
          },
          "metadata": {},
          "execution_count": 144
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 0.00295379\n",
            "Iteration 2, loss = 0.00010094\n",
            "Iteration 3, loss = 0.00007857\n",
            "Iteration 4, loss = 0.00007376\n",
            "Iteration 5, loss = 0.00006352\n",
            "Iteration 6, loss = 0.00005093\n",
            "Iteration 7, loss = 0.00004983\n",
            "Iteration 8, loss = 0.00004696\n",
            "Iteration 9, loss = 0.00004742\n",
            "Iteration 10, loss = 0.00004567\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPClassifier(hidden_layer_sizes=(60, 500, 5), max_iter=10, random_state=1,\n",
              "              verbose=True, warm_start=True)"
            ]
          },
          "metadata": {},
          "execution_count": 145
        }
      ],
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "clfhiba3=MLPClassifier(random_state=1, max_iter=10, warm_start=True, verbose=True,hidden_layer_sizes=(60,500,5))\n",
        "clfhiba3.fit(X_train_train,yhiba)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ITT kezdődik az okítási ciklus IDe jön vissza"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 162,
      "metadata": {},
      "outputs": [],
      "source": [
        "def reconvert (X):\n",
        "    yo=[]\n",
        "    y1=clf1.predict(X)\n",
        "    y2=clf2.predict(X)\n",
        "    y3=clf3.predict(X)\n",
        "    yerror=clfhiba3.predict(X)\n",
        "    o=1\n",
        "    \n",
        "    for idx,_ in enumerate(X):\n",
        "       if y1[idx]==max(y1[idx],y2[idx],y3[idx]):\n",
        "           o=1\n",
        "       if y2[idx]==max(y1[idx],y2[idx],y3[idx]):\n",
        "           o=2\n",
        "       if y3[idx]==max(y1[idx],y2[idx],y3[idx]):\n",
        "           o=3\n",
        "       '''\n",
        "       ox=o\n",
        "       if ox==3 and yerror[idx]:\n",
        "           o=2\n",
        "       if ox==1 and yerror[idx]:\n",
        "           o=2\n",
        "       if ox==2 and yerror[idx]:\n",
        "           # 2 eset van \n",
        "           if y3[idx]>y1[idx]:\n",
        "               o=3\n",
        "           else:\n",
        "               o=1\n",
        "\n",
        "       '''\n",
        "       \n",
        "       yo.append(o)\n",
        "    return(yo)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 196,
      "metadata": {
        "tags": [
          "outputPrepend"
        ]
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "996,0.131453\n",
            "n_hiba:6345\n",
            "damage grade in     6355 nextsave=6555. step: 0.046458,0.168974,0.131439\n",
            "n_hiba:6355\n",
            "damage grade in     6365 nextsave=6555. step: 0.046451,0.168959,0.131426\n",
            "n_hiba:6365\n",
            "damage grade in     6375 nextsave=6555. step: 0.046446,0.168942,0.131416\n",
            "n_hiba:6375\n",
            "damage grade in     6385 nextsave=6555. step: 0.046440,0.168928,0.131410\n",
            "n_hiba:6385\n",
            "damage grade in     6395 nextsave=6555. step: 0.046437,0.168911,0.131399\n",
            "n_hiba:6395\n",
            "damage grade in     6405 nextsave=6555. step: 0.046432,0.168890,0.131388\n",
            "n_hiba:6405\n",
            "damage grade in     6415 nextsave=6555. step: 0.046428,0.168873,0.131379\n",
            "n_hiba:6415\n",
            "damage grade in     6425 nextsave=6555. step: 0.046425,0.168857,0.131368\n",
            "n_hiba:6425\n",
            "damage grade in     6435 nextsave=6555. step: 0.046418,0.168844,0.131359\n",
            "n_hiba:6435\n",
            "damage grade in     6445 nextsave=6555. step: 0.046412,0.168828,0.131340\n",
            "n_hiba:6445\n",
            "damage grade in     6455 nextsave=6555. step: 0.046407,0.168815,0.131330\n",
            "n_hiba:6455\n",
            "damage grade in     6465 nextsave=6555. step: 0.046402,0.168806,0.131323\n",
            "n_hiba:6465\n",
            "damage grade in     6475 nextsave=6555. step: 0.046398,0.168793,0.131312\n",
            "n_hiba:6475\n",
            "damage grade in     6485 nextsave=6555. step: 0.046393,0.168784,0.131293\n",
            "n_hiba:6485\n",
            "damage grade in     6495 nextsave=6555. step: 0.046388,0.168767,0.131281\n",
            "n_hiba:6495\n",
            "damage grade in     6505 nextsave=6555. step: 0.046383,0.168749,0.131267\n",
            "n_hiba:6505\n",
            "damage grade in     6515 nextsave=6555. step: 0.046375,0.168737,0.131255\n",
            "n_hiba:6515\n",
            "damage grade in     6525 nextsave=6555. step: 0.046369,0.168724,0.131246\n",
            "n_hiba:6525\n",
            "damage grade in     6535 nextsave=6555. step: 0.046361,0.168711,0.131235\n",
            "n_hiba:6535\n",
            "damage grade in     6545 nextsave=6555. step: 0.046357,0.168692,0.131222\n",
            "n_hiba:6545\n",
            "damage grade in     6555 nextsave=6555. step: 0.046354,0.168667,0.131210\n",
            "n_hiba:6555\n",
            "damage grade in     6565 nextsave=7065. step: 0.046350,0.168647,0.131196\n",
            "n_hiba:6565\n",
            "damage grade in     6575 nextsave=7065. step: 0.046344,0.168634,0.131183\n",
            "n_hiba:6575\n",
            "damage grade in     6585 nextsave=7065. step: 0.046338,0.168623,0.131172\n",
            "n_hiba:6585\n",
            "damage grade in     6595 nextsave=7065. step: 0.046330,0.168612,0.131166\n",
            "n_hiba:6595\n",
            "damage grade in     6605 nextsave=7065. step: 0.046327,0.168594,0.131154\n",
            "n_hiba:6605\n",
            "damage grade in     6615 nextsave=7065. step: 0.046323,0.168583,0.131147\n",
            "n_hiba:6615\n",
            "damage grade in     6625 nextsave=7065. step: 0.046318,0.168569,0.131136\n",
            "n_hiba:6625\n",
            "damage grade in     6635 nextsave=7065. step: 0.046310,0.168553,0.131125\n",
            "n_hiba:6635\n",
            "damage grade in     6645 nextsave=7065. step: 0.046306,0.168539,0.131114\n",
            "n_hiba:6645\n",
            "damage grade in     6655 nextsave=7065. step: 0.046302,0.168528,0.131107\n",
            "n_hiba:6655\n",
            "damage grade in     6665 nextsave=7065. step: 0.046299,0.168518,0.131093\n",
            "n_hiba:6665\n",
            "damage grade in     6675 nextsave=7065. step: 0.046293,0.168504,0.131085\n",
            "n_hiba:6675\n",
            "damage grade in     6685 nextsave=7065. step: 0.046286,0.168494,0.131078\n",
            "n_hiba:6685\n",
            "damage grade in     6695 nextsave=7065. step: 0.046281,0.168477,0.131068\n",
            "n_hiba:6695\n",
            "damage grade in     6705 nextsave=7065. step: 0.046277,0.168462,0.131061\n",
            "n_hiba:6705\n",
            "damage grade in     6715 nextsave=7065. step: 0.046273,0.168448,0.131043\n",
            "n_hiba:6715\n",
            "damage grade in     6725 nextsave=7065. step: 0.046268,0.168436,0.131035\n",
            "n_hiba:6725\n",
            "damage grade in     6735 nextsave=7065. step: 0.046261,0.168422,0.131026\n",
            "n_hiba:6735\n",
            "damage grade in     6745 nextsave=7065. step: 0.046255,0.168407,0.131018\n",
            "n_hiba:6745\n",
            "damage grade in     6755 nextsave=7065. step: 0.046248,0.168395,0.131011\n",
            "n_hiba:6755\n",
            "damage grade in     6765 nextsave=7065. step: 0.046242,0.168379,0.131002\n",
            "n_hiba:6765\n",
            "damage grade in     6775 nextsave=7065. step: 0.046237,0.168368,0.130996\n",
            "n_hiba:6775\n",
            "damage grade in     6785 nextsave=7065. step: 0.046231,0.168357,0.130990\n",
            "n_hiba:6785\n",
            "damage grade in     6795 nextsave=7065. step: 0.046223,0.168348,0.130981\n",
            "n_hiba:6795\n",
            "damage grade in     6805 nextsave=7065. step: 0.046218,0.168336,0.130964\n",
            "n_hiba:6805\n",
            "damage grade in     6815 nextsave=7065. step: 0.046213,0.168325,0.130954\n",
            "n_hiba:6815\n",
            "damage grade in     6825 nextsave=7065. step: 0.046210,0.168312,0.130944\n",
            "n_hiba:6825\n",
            "damage grade in     6835 nextsave=7065. step: 0.046206,0.168303,0.130933\n",
            "n_hiba:6835\n",
            "damage grade in     6845 nextsave=7065. step: 0.046202,0.168291,0.130920\n",
            "n_hiba:6845\n",
            "damage grade in     6855 nextsave=7065. step: 0.046198,0.168283,0.130908\n",
            "n_hiba:6855\n",
            "damage grade in     6865 nextsave=7065. step: 0.046194,0.168274,0.130898\n",
            "n_hiba:6865\n",
            "damage grade in     6875 nextsave=7065. step: 0.046188,0.168256,0.130891\n",
            "n_hiba:6875\n",
            "damage grade in     6885 nextsave=7065. step: 0.046184,0.168241,0.130881\n",
            "n_hiba:6885\n",
            "damage grade in     6895 nextsave=7065. step: 0.046177,0.168224,0.130865\n",
            "n_hiba:6895\n",
            "damage grade in     6905 nextsave=7065. step: 0.046172,0.168208,0.130850\n",
            "n_hiba:6905\n",
            "damage grade in     6915 nextsave=7065. step: 0.046169,0.168199,0.130837\n",
            "n_hiba:6915\n",
            "damage grade in     6925 nextsave=7065. step: 0.046166,0.168189,0.130828\n",
            "n_hiba:6925\n",
            "damage grade in     6935 nextsave=7065. step: 0.046160,0.168178,0.130815\n",
            "n_hiba:6935\n",
            "damage grade in     6945 nextsave=7065. step: 0.046156,0.168164,0.130801\n",
            "n_hiba:6945\n",
            "damage grade in     6955 nextsave=7065. step: 0.046151,0.168147,0.130791\n",
            "n_hiba:6955\n",
            "damage grade in     6965 nextsave=7065. step: 0.046147,0.168128,0.130780\n",
            "n_hiba:6965\n",
            "damage grade in     6975 nextsave=7065. step: 0.046144,0.168115,0.130773\n",
            "n_hiba:6975\n",
            "damage grade in     6985 nextsave=7065. step: 0.046141,0.168101,0.130759\n",
            "n_hiba:6985\n",
            "damage grade in     6995 nextsave=7065. step: 0.046137,0.168090,0.130743\n",
            "n_hiba:6995\n",
            "damage grade in     7005 nextsave=7065. step: 0.046130,0.168081,0.130734\n",
            "n_hiba:7005\n",
            "damage grade in     7015 nextsave=7065. step: 0.046126,0.168071,0.130727\n",
            "n_hiba:7015\n",
            "damage grade in     7025 nextsave=7065. step: 0.046122,0.168061,0.130718\n",
            "n_hiba:7025\n",
            "damage grade in     7035 nextsave=7065. step: 0.046116,0.168052,0.130708\n",
            "n_hiba:7035\n",
            "damage grade in     7045 nextsave=7065. step: 0.046111,0.168037,0.130701\n",
            "n_hiba:7045\n",
            "damage grade in     7055 nextsave=7065. step: 0.046107,0.168028,0.130686\n",
            "n_hiba:7055\n",
            "damage grade in     7065 nextsave=7065. step: 0.046102,0.168018,0.130668\n",
            "n_hiba:7065\n",
            "damage grade in     7075 nextsave=7575. step: 0.046099,0.168011,0.130657\n",
            "n_hiba:7075\n",
            "damage grade in     7085 nextsave=7575. step: 0.046094,0.168003,0.130645\n",
            "n_hiba:7085\n",
            "damage grade in     7095 nextsave=7575. step: 0.046089,0.167984,0.130636\n",
            "n_hiba:7095\n",
            "damage grade in     7105 nextsave=7575. step: 0.046082,0.167962,0.130628\n",
            "n_hiba:7105\n",
            "damage grade in     7115 nextsave=7575. step: 0.046080,0.167947,0.130616\n",
            "n_hiba:7115\n",
            "damage grade in     7125 nextsave=7575. step: 0.046077,0.167936,0.130609\n",
            "n_hiba:7125\n",
            "damage grade in     7135 nextsave=7575. step: 0.046070,0.167926,0.130602\n",
            "n_hiba:7135\n",
            "damage grade in     7145 nextsave=7575. step: 0.046065,0.167915,0.130594\n",
            "n_hiba:7145\n",
            "damage grade in     7155 nextsave=7575. step: 0.046060,0.167903,0.130578\n",
            "n_hiba:7155\n",
            "damage grade in     7165 nextsave=7575. step: 0.046056,0.167892,0.130568\n",
            "n_hiba:7165\n",
            "damage grade in     7175 nextsave=7575. step: 0.046052,0.167884,0.130557\n",
            "n_hiba:7175\n",
            "damage grade in     7185 nextsave=7575. step: 0.046046,0.167872,0.130549\n",
            "n_hiba:7185\n",
            "damage grade in     7195 nextsave=7575. step: 0.046042,0.167863,0.130539\n",
            "n_hiba:7195\n",
            "damage grade in     7205 nextsave=7575. step: 0.046036,0.167853,0.130534\n",
            "n_hiba:7205\n",
            "damage grade in     7215 nextsave=7575. step: 0.046030,0.167845,0.130519\n",
            "n_hiba:7215\n",
            "damage grade in     7225 nextsave=7575. step: 0.046026,0.167835,0.130507\n",
            "n_hiba:7225\n",
            "damage grade in     7235 nextsave=7575. step: 0.046023,0.167825,0.130495\n",
            "n_hiba:7235\n",
            "damage grade in     7245 nextsave=7575. step: 0.046019,0.167810,0.130483\n",
            "n_hiba:7245\n",
            "damage grade in     7255 nextsave=7575. step: 0.046014,0.167797,0.130474\n",
            "n_hiba:7255\n",
            "damage grade in     7265 nextsave=7575. step: 0.046006,0.167782,0.130463\n",
            "n_hiba:7265\n",
            "damage grade in     7275 nextsave=7575. step: 0.046002,0.167769,0.130457\n",
            "n_hiba:7275\n",
            "damage grade in     7285 nextsave=7575. step: 0.045997,0.167758,0.130450\n",
            "n_hiba:7285\n",
            "damage grade in     7295 nextsave=7575. step: 0.045992,0.167746,0.130442\n",
            "n_hiba:7295\n",
            "damage grade in     7305 nextsave=7575. step: 0.045987,0.167737,0.130436\n",
            "n_hiba:7305\n",
            "damage grade in     7315 nextsave=7575. step: 0.045982,0.167725,0.130428\n",
            "n_hiba:7315\n",
            "damage grade in     7325 nextsave=7575. step: 0.045977,0.167710,0.130422\n",
            "n_hiba:7325\n",
            "damage grade in     7335 nextsave=7575. step: 0.045974,0.167699,0.130409\n",
            "n_hiba:7335\n",
            "damage grade in     7345 nextsave=7575. step: 0.045969,0.167682,0.130402\n",
            "n_hiba:7345\n",
            "damage grade in     7355 nextsave=7575. step: 0.045965,0.167673,0.130393\n",
            "n_hiba:7355\n",
            "damage grade in     7365 nextsave=7575. step: 0.045960,0.167661,0.130385\n",
            "n_hiba:7365\n",
            "damage grade in     7375 nextsave=7575. step: 0.045954,0.167649,0.130377\n",
            "n_hiba:7375\n",
            "damage grade in     7385 nextsave=7575. step: 0.045950,0.167636,0.130366\n",
            "n_hiba:7385\n",
            "damage grade in     7395 nextsave=7575. step: 0.045947,0.167617,0.130359\n",
            "n_hiba:7395\n",
            "damage grade in     7405 nextsave=7575. step: 0.045942,0.167607,0.130350\n",
            "n_hiba:7405\n",
            "damage grade in     7415 nextsave=7575. step: 0.045938,0.167594,0.130336\n",
            "n_hiba:7415\n",
            "damage grade in     7425 nextsave=7575. step: 0.045932,0.167578,0.130321\n",
            "n_hiba:7425\n",
            "damage grade in     7435 nextsave=7575. step: 0.045927,0.167563,0.130307\n",
            "n_hiba:7435\n",
            "damage grade in     7445 nextsave=7575. step: 0.045923,0.167546,0.130296\n",
            "n_hiba:7445\n",
            "damage grade in     7455 nextsave=7575. step: 0.045918,0.167534,0.130287\n",
            "n_hiba:7455\n",
            "damage grade in     7465 nextsave=7575. step: 0.045914,0.167521,0.130281\n",
            "n_hiba:7465\n",
            "damage grade in     7475 nextsave=7575. step: 0.045907,0.167510,0.130273\n",
            "n_hiba:7475\n",
            "damage grade in     7485 nextsave=7575. step: 0.045903,0.167497,0.130266\n",
            "n_hiba:7485\n",
            "damage grade in     7495 nextsave=7575. step: 0.045899,0.167480,0.130260\n",
            "n_hiba:7495\n",
            "damage grade in     7505 nextsave=7575. step: 0.045893,0.167472,0.130253\n",
            "n_hiba:7505\n",
            "damage grade in     7515 nextsave=7575. step: 0.045889,0.167463,0.130248\n",
            "n_hiba:7515\n",
            "damage grade in     7525 nextsave=7575. step: 0.045883,0.167452,0.130241\n",
            "n_hiba:7525\n",
            "damage grade in     7535 nextsave=7575. step: 0.045879,0.167442,0.130235\n",
            "n_hiba:7535\n",
            "damage grade in     7545 nextsave=7575. step: 0.045875,0.167434,0.130228\n",
            "n_hiba:7545\n",
            "damage grade in     7555 nextsave=7575. step: 0.045871,0.167422,0.130221\n",
            "n_hiba:7555\n",
            "damage grade in     7565 nextsave=7575. step: 0.045865,0.167411,0.130215\n",
            "n_hiba:7565\n",
            "damage grade in     7575 nextsave=7575. step: 0.045862,0.167399,0.130210\n",
            "n_hiba:7575\n",
            "damage grade in     7585 nextsave=8085. step: 0.045856,0.167381,0.130207\n",
            "n_hiba:7585\n",
            "damage grade in     7595 nextsave=8085. step: 0.045850,0.167368,0.130191\n",
            "n_hiba:7595\n",
            "damage grade in     7605 nextsave=8085. step: 0.045846,0.167355,0.130183\n",
            "n_hiba:7605\n",
            "damage grade in     7615 nextsave=8085. step: 0.045840,0.167341,0.130176\n",
            "n_hiba:7615\n",
            "damage grade in     7625 nextsave=8085. step: 0.045835,0.167331,0.130167\n",
            "n_hiba:7625\n",
            "damage grade in     7635 nextsave=8085. step: 0.045830,0.167318,0.130158\n",
            "n_hiba:7635\n",
            "damage grade in     7645 nextsave=8085. step: 0.045826,0.167302,0.130149\n",
            "n_hiba:7645\n",
            "damage grade in     7655 nextsave=8085. step: 0.045823,0.167290,0.130138\n",
            "n_hiba:7655\n",
            "damage grade in     7665 nextsave=8085. step: 0.045817,0.167279,0.130130\n",
            "n_hiba:7665\n",
            "damage grade in     7675 nextsave=8085. step: 0.045814,0.167267,0.130122\n",
            "n_hiba:7675\n",
            "damage grade in     7685 nextsave=8085. step: 0.045809,0.167255,0.130111\n",
            "n_hiba:7685\n",
            "damage grade in     7695 nextsave=8085. step: 0.045803,0.167241,0.130098\n",
            "n_hiba:7695\n",
            "damage grade in     7705 nextsave=8085. step: 0.045800,0.167230,0.130086\n",
            "n_hiba:7705\n",
            "damage grade in     7715 nextsave=8085. step: 0.045796,0.167213,0.130079\n",
            "n_hiba:7715\n",
            "damage grade in     7725 nextsave=8085. step: 0.045791,0.167198,0.130069\n",
            "n_hiba:7725\n",
            "damage grade in     7735 nextsave=8085. step: 0.045785,0.167184,0.130060\n",
            "n_hiba:7735\n",
            "damage grade in     7745 nextsave=8085. step: 0.045778,0.167173,0.130049\n",
            "n_hiba:7745\n",
            "damage grade in     7755 nextsave=8085. step: 0.045774,0.167158,0.130040\n",
            "n_hiba:7755\n",
            "damage grade in     7765 nextsave=8085. step: 0.045769,0.167151,0.130031\n",
            "n_hiba:7765\n",
            "damage grade in     7775 nextsave=8085. step: 0.045762,0.167141,0.130022\n",
            "n_hiba:7775\n",
            "damage grade in     7785 nextsave=8085. step: 0.045758,0.167129,0.130014\n",
            "n_hiba:7785\n",
            "damage grade in     7795 nextsave=8085. step: 0.045751,0.167121,0.130004\n",
            "n_hiba:7795\n",
            "damage grade in     7805 nextsave=8085. step: 0.045746,0.167107,0.129997\n",
            "n_hiba:7805\n",
            "damage grade in     7815 nextsave=8085. step: 0.045741,0.167093,0.129988\n",
            "n_hiba:7815\n",
            "damage grade in     7825 nextsave=8085. step: 0.045736,0.167079,0.129979\n",
            "n_hiba:7825\n",
            "damage grade in     7835 nextsave=8085. step: 0.045730,0.167072,0.129971\n",
            "n_hiba:7835\n",
            "damage grade in     7845 nextsave=8085. step: 0.045727,0.167062,0.129961\n",
            "n_hiba:7845\n",
            "damage grade in     7855 nextsave=8085. step: 0.045724,0.167050,0.129952\n",
            "n_hiba:7855\n",
            "damage grade in     7865 nextsave=8085. step: 0.045721,0.167041,0.129944\n",
            "n_hiba:7865\n",
            "damage grade in     7875 nextsave=8085. step: 0.045718,0.167022,0.129936\n",
            "n_hiba:7875\n",
            "damage grade in     7885 nextsave=8085. step: 0.045715,0.167009,0.129930\n",
            "n_hiba:7885\n",
            "damage grade in     7895 nextsave=8085. step: 0.045713,0.166995,0.129925\n",
            "n_hiba:7895\n",
            "damage grade in     7905 nextsave=8085. step: 0.045710,0.166987,0.129917\n",
            "n_hiba:7905\n",
            "damage grade in     7915 nextsave=8085. step: 0.045706,0.166976,0.129906\n",
            "n_hiba:7915\n",
            "damage grade in     7925 nextsave=8085. step: 0.045702,0.166966,0.129899\n",
            "n_hiba:7925\n",
            "damage grade in     7935 nextsave=8085. step: 0.045699,0.166957,0.129891\n",
            "n_hiba:7935\n",
            "damage grade in     7945 nextsave=8085. step: 0.045695,0.166950,0.129879\n",
            "n_hiba:7945\n",
            "damage grade in     7955 nextsave=8085. step: 0.045692,0.166934,0.129870\n",
            "n_hiba:7955\n",
            "damage grade in     7965 nextsave=8085. step: 0.045690,0.166923,0.129863\n",
            "n_hiba:7965\n",
            "damage grade in     7975 nextsave=8085. step: 0.045685,0.166912,0.129855\n",
            "n_hiba:7975\n",
            "damage grade in     7985 nextsave=8085. step: 0.045681,0.166903,0.129848\n",
            "n_hiba:7985\n",
            "damage grade in     7995 nextsave=8085. step: 0.045676,0.166891,0.129841\n",
            "n_hiba:7995\n",
            "damage grade in     8005 nextsave=8085. step: 0.045673,0.166879,0.129833\n",
            "n_hiba:8005\n",
            "damage grade in     8015 nextsave=8085. step: 0.045670,0.166870,0.129827\n",
            "n_hiba:8015\n",
            "damage grade in     8025 nextsave=8085. step: 0.045667,0.166857,0.129820\n",
            "n_hiba:8025\n",
            "damage grade in     8035 nextsave=8085. step: 0.045664,0.166842,0.129813\n",
            "n_hiba:8035\n",
            "damage grade in     8045 nextsave=8085. step: 0.045662,0.166831,0.129808\n",
            "n_hiba:8045\n",
            "damage grade in     8055 nextsave=8085. step: 0.045658,0.166823,0.129801\n",
            "n_hiba:8055\n",
            "damage grade in     8065 nextsave=8085. step: 0.045654,0.166810,0.129794\n",
            "n_hiba:8065\n",
            "damage grade in     8075 nextsave=8085. step: 0.045651,0.166796,0.129785\n",
            "n_hiba:8075\n",
            "damage grade in     8085 nextsave=8085. step: 0.045648,0.166778,0.129777\n",
            "n_hiba:8085\n",
            "damage grade in     8095 nextsave=8595. step: 0.045645,0.166769,0.129771\n",
            "n_hiba:8095\n",
            "damage grade in     8105 nextsave=8595. step: 0.045641,0.166756,0.129759\n",
            "n_hiba:8105\n",
            "damage grade in     8115 nextsave=8595. step: 0.045636,0.166746,0.129749\n",
            "n_hiba:8115\n",
            "damage grade in     8125 nextsave=8595. step: 0.045630,0.166738,0.129738\n",
            "n_hiba:8125\n",
            "damage grade in     8135 nextsave=8595. step: 0.045625,0.166728,0.129727\n",
            "n_hiba:8135\n",
            "damage grade in     8145 nextsave=8595. step: 0.045617,0.166712,0.129720\n",
            "n_hiba:8145\n",
            "damage grade in     8155 nextsave=8595. step: 0.045609,0.166695,0.129712\n",
            "n_hiba:8155\n",
            "damage grade in     8165 nextsave=8595. step: 0.045601,0.166683,0.129707\n",
            "n_hiba:8165\n",
            "damage grade in     8175 nextsave=8595. step: 0.045598,0.166666,0.129698\n",
            "n_hiba:8175\n",
            "damage grade in     8185 nextsave=8595. step: 0.045593,0.166648,0.129690\n",
            "n_hiba:8185\n",
            "damage grade in     8195 nextsave=8595. step: 0.045590,0.166629,0.129683\n",
            "n_hiba:8195\n",
            "damage grade in     8205 nextsave=8595. step: 0.045585,0.166619,0.129677\n",
            "n_hiba:8205\n",
            "damage grade in     8215 nextsave=8595. step: 0.045582,0.166608,0.129669\n",
            "n_hiba:8215\n",
            "damage grade in     8225 nextsave=8595. step: 0.045578,0.166593,0.129654\n",
            "n_hiba:8225\n",
            "damage grade in     8235 nextsave=8595. step: 0.045573,0.166579,0.129645\n",
            "n_hiba:8235\n",
            "damage grade in     8245 nextsave=8595. step: 0.045567,0.166565,0.129637\n",
            "n_hiba:8245\n",
            "damage grade in     8255 nextsave=8595. step: 0.045564,0.166553,0.129630\n",
            "n_hiba:8255\n",
            "damage grade in     8265 nextsave=8595. step: 0.045561,0.166542,0.129620\n",
            "n_hiba:8265\n",
            "damage grade in     8275 nextsave=8595. step: 0.045557,0.166528,0.129612\n",
            "n_hiba:8275\n",
            "damage grade in     8285 nextsave=8595. step: 0.045550,0.166519,0.129604\n",
            "n_hiba:8285\n",
            "damage grade in     8295 nextsave=8595. step: 0.045545,0.166505,0.129596\n",
            "n_hiba:8295\n",
            "damage grade in     8305 nextsave=8595. step: 0.045541,0.166493,0.129589\n",
            "n_hiba:8305\n",
            "damage grade in     8315 nextsave=8595. step: 0.045538,0.166482,0.129581\n",
            "n_hiba:8315\n",
            "damage grade in     8325 nextsave=8595. step: 0.045537,0.166472,0.129574\n",
            "n_hiba:8325\n",
            "damage grade in     8335 nextsave=8595. step: 0.045534,0.166463,0.129566\n",
            "n_hiba:8335\n",
            "damage grade in     8345 nextsave=8595. step: 0.045528,0.166453,0.129559\n",
            "n_hiba:8345\n",
            "damage grade in     8355 nextsave=8595. step: 0.045523,0.166442,0.129552\n",
            "n_hiba:8355\n",
            "damage grade in     8365 nextsave=8595. step: 0.045515,0.166427,0.129545\n",
            "n_hiba:8365\n",
            "damage grade in     8375 nextsave=8595. step: 0.045509,0.166416,0.129537\n",
            "n_hiba:8375\n",
            "damage grade in     8385 nextsave=8595. step: 0.045505,0.166405,0.129529\n",
            "n_hiba:8385\n",
            "damage grade in     8395 nextsave=8595. step: 0.045499,0.166394,0.129522\n",
            "n_hiba:8395\n",
            "damage grade in     8405 nextsave=8595. step: 0.045496,0.166379,0.129515\n",
            "n_hiba:8405\n",
            "damage grade in     8415 nextsave=8595. step: 0.045491,0.166369,0.129505\n",
            "n_hiba:8415\n",
            "damage grade in     8425 nextsave=8595. step: 0.045484,0.166358,0.129499\n",
            "n_hiba:8425\n",
            "damage grade in     8435 nextsave=8595. step: 0.045482,0.166350,0.129491\n",
            "n_hiba:8435\n",
            "damage grade in     8445 nextsave=8595. step: 0.045479,0.166341,0.129483\n",
            "n_hiba:8445\n",
            "damage grade in     8455 nextsave=8595. step: 0.045476,0.166325,0.129476\n",
            "n_hiba:8455\n",
            "damage grade in     8465 nextsave=8595. step: 0.045469,0.166313,0.129466\n",
            "n_hiba:8465\n",
            "damage grade in     8475 nextsave=8595. step: 0.045464,0.166303,0.129459\n",
            "n_hiba:8475\n",
            "damage grade in     8485 nextsave=8595. step: 0.045458,0.166292,0.129452\n",
            "n_hiba:8485\n",
            "damage grade in     8495 nextsave=8595. step: 0.045451,0.166284,0.129446\n",
            "n_hiba:8495\n",
            "damage grade in     8505 nextsave=8595. step: 0.045445,0.166274,0.129438\n",
            "n_hiba:8505\n",
            "damage grade in     8515 nextsave=8595. step: 0.045443,0.166263,0.129429\n",
            "n_hiba:8515\n",
            "damage grade in     8525 nextsave=8595. step: 0.045439,0.166255,0.129422\n",
            "n_hiba:8525\n",
            "damage grade in     8535 nextsave=8595. step: 0.045436,0.166240,0.129414\n",
            "n_hiba:8535\n",
            "damage grade in     8545 nextsave=8595. step: 0.045432,0.166227,0.129407\n",
            "n_hiba:8545\n",
            "damage grade in     8555 nextsave=8595. step: 0.045429,0.166218,0.129401\n",
            "n_hiba:8555\n",
            "damage grade in     8565 nextsave=8595. step: 0.045422,0.166211,0.129393\n",
            "n_hiba:8565\n",
            "damage grade in     8575 nextsave=8595. step: 0.045416,0.166202,0.129383\n",
            "n_hiba:8575\n",
            "damage grade in     8585 nextsave=8595. step: 0.045414,0.166191,0.129376\n",
            "n_hiba:8585\n",
            "damage grade in     8595 nextsave=8595. step: 0.045409,0.166180,0.129372\n",
            "n_hiba:8595\n",
            "damage grade in     8605 nextsave=9105. step: 0.045406,0.166174,0.129366\n",
            "n_hiba:8605\n",
            "damage grade in     8615 nextsave=9105. step: 0.045402,0.166162,0.129354\n",
            "n_hiba:8615\n",
            "damage grade in     8625 nextsave=9105. step: 0.045399,0.166156,0.129342\n",
            "n_hiba:8625\n",
            "damage grade in     8635 nextsave=9105. step: 0.045393,0.166145,0.129334\n",
            "n_hiba:8635\n",
            "damage grade in     8645 nextsave=9105. step: 0.045389,0.166132,0.129327\n",
            "n_hiba:8645\n",
            "damage grade in     8655 nextsave=9105. step: 0.045383,0.166121,0.129320\n",
            "n_hiba:8655\n",
            "damage grade in     8665 nextsave=9105. step: 0.045379,0.166115,0.129311\n",
            "n_hiba:8665\n",
            "damage grade in     8675 nextsave=9105. step: 0.045376,0.166104,0.129301\n",
            "n_hiba:8675\n",
            "damage grade in     8685 nextsave=9105. step: 0.045373,0.166097,0.129292\n",
            "n_hiba:8685\n",
            "damage grade in     8695 nextsave=9105. step: 0.045369,0.166084,0.129286\n",
            "n_hiba:8695\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-196-485e1b58b75f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mclf1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0myt1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[0mclf2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0myt2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m     \u001b[0mclf3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0myt3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[0;32m    496\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    497\u001b[0m         \u001b[1;31m# fit the boosting stages\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 498\u001b[1;33m         n_stages = self._fit_stages(\n\u001b[0m\u001b[0;32m    499\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraw_predictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_rng\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    500\u001b[0m             sample_weight_val, begin_at_stage, monitor, X_idx_sorted)\n",
            "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\u001b[0m in \u001b[0;36m_fit_stages\u001b[1;34m(self, X, y, raw_predictions, sample_weight, random_state, X_val, y_val, sample_weight_val, begin_at_stage, monitor, X_idx_sorted)\u001b[0m\n\u001b[0;32m    553\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    554\u001b[0m             \u001b[1;31m# fit next stage of trees\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 555\u001b[1;33m             raw_predictions = self._fit_stage(\n\u001b[0m\u001b[0;32m    556\u001b[0m                 \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraw_predictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    557\u001b[0m                 random_state, X_idx_sorted, X_csc, X_csr)\n",
            "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\u001b[0m in \u001b[0;36m_fit_stage\u001b[1;34m(self, i, X, y, raw_predictions, sample_weight, sample_mask, random_state, X_idx_sorted, X_csc, X_csr)\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m             \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_csr\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mX_csr\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 211\u001b[1;33m             tree.fit(X, residual, sample_weight=sample_weight,\n\u001b[0m\u001b[0;32m    212\u001b[0m                      check_input=False, X_idx_sorted=X_idx_sorted)\n\u001b[0;32m    213\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\tree\\_classes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m   1240\u001b[0m         \"\"\"\n\u001b[0;32m   1241\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1242\u001b[1;33m         super().fit(\n\u001b[0m\u001b[0;32m   1243\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1244\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\tree\\_classes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    373\u001b[0m                                            min_impurity_split)\n\u001b[0;32m    374\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 375\u001b[1;33m         \u001b[0mbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    376\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    377\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mis_classifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "n=len(clf1.train_score_)\n",
        "nextsave=n+500\n",
        "while 1==1:\n",
        "    \n",
        "    n=n+1    \n",
        "    nhiba=len(clf1.train_score_)\n",
        "    print(f\"n_hiba:{nhiba}\")\n",
        "    nhiba=nhiba+10    \n",
        "    \n",
        "    clf1.set_params(alpha=0.99,n_estimators=nhiba,warm_start=True,verbose=0)\n",
        "    clf2.set_params(alpha=0.99,n_estimators=nhiba,warm_start=True,verbose=0)\n",
        "    clf3.set_params(alpha=0.99,n_estimators=nhiba,warm_start=True,verbose=0)\n",
        "   \n",
        "    clf1.fit(X_train_train,yt1)\n",
        "    clf2.fit(X_train_train,yt2)\n",
        "    clf3.fit(X_train_train,yt3)\n",
        "   \n",
        "  \n",
        "    o1=clf1.train_score_[-1]\n",
        "    o2=clf2.train_score_[-1]\n",
        "    o3=clf3.train_score_[-1]\n",
        "    #\n",
        "    # Save the model regularly!\n",
        "    #\n",
        "    if nhiba>nextsave:\n",
        "        nextsave=nhiba+500\n",
        "        import pickle\n",
        "        nhibastr=str(nhiba)\n",
        "        fname1=basedir+\"/clf1_\"+nhibastr+\"_data.pickled\"\n",
        "        with open(fname1, 'wb') as file:\n",
        "            pickle.dump(clf1, file)\n",
        "\n",
        "        fname2=basedir+\"/clf2_\"+nhibastr+\"_data.pickled\"\n",
        "        with open(fname2, 'wb') as file:\n",
        "            pickle.dump(clf2, file)\n",
        "\n",
        "        fname3=basedir+\"/clf3_\"+nhibastr+\"_data.pickled\"\n",
        "        with open(fname3, 'wb') as file:\n",
        "            pickle.dump(clf3, file)\n",
        "\n",
        "\n",
        "    if nhiba>12070:\n",
        "        break    \n",
        "        \n",
        "    print(f\"damage grade in {nhiba:8} nextsave={nextsave}. step: {o1:8.6f},{o2:8.6f},{o3:8.6f}\" )\n",
        " "
      ]
    },
    {
      "source": [
        "\n",
        "import pickle\n",
        "fname1=basedir+\"/clf1_3500_data.pickle\"\n",
        "with open(fname1, 'wb') as file:\n",
        "    pickle.dump(clf1, file)\n",
        "\n",
        "fname2=basedir+\"/clf2_3500_data.pickle\"\n",
        "with open(fname2, 'wb') as file:\n",
        "    pickle.dump(clf2, file)\n",
        "\n",
        "fname3=basedir+\"/clf3_3500_data.pickle\"\n",
        "with open(fname3, 'wb') as file:\n",
        "    pickle.dump(clf3, file)\n",
        "\n",
        "\n",
        "\n"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2. 0. 1. 0.] : 0.009 _  0.225 _  0.620 _ False_ 0.000\n",
            "[1. 1. 0. 0.] : 0.593 _  0.363 _  0.126 _ True_ 0.000\n",
            "[2. 0. 1. 0.] : 0.476 _  0.494 _  0.048 _ True_ 0.000\n",
            "[2. 0. 1. 0.] :-0.003 _  0.432 _  0.602 _ False_ 0.000\n",
            "[2. 0. 1. 0.] : 0.177 _  0.727 _  0.109 _ True_ 0.000\n",
            "[3. 0. 0. 1.] :-0.007 _  0.429 _  0.575 _ True_ 0.000\n",
            "[2. 0. 1. 0.] : 0.140 _  0.759 _  0.128 _ True_ 0.000\n",
            "[2. 0. 1. 0.] : 0.154 _  0.659 _  0.166 _ True_ 0.000\n",
            "[3. 0. 0. 1.] :-0.005 _  0.120 _  0.877 _ True_ 0.000\n",
            "[1. 1. 0. 0.] : 0.147 _  0.615 _  0.228 _ False_ 0.000\n",
            "[2. 0. 1. 0.] :-0.010 _  0.148 _  0.862 _ False_ 0.000\n",
            "[1. 1. 0. 0.] : 0.670 _  0.397 _ -0.040 _ True_ 0.000\n",
            "[2. 0. 1. 0.] : 0.004 _  0.193 _  0.829 _ False_ 0.000\n",
            "[3. 0. 0. 1.] : 0.002 _  0.458 _  0.531 _ True_ 0.000\n",
            "[3. 0. 0. 1.] :-0.009 _  0.104 _  0.893 _ True_ 0.000\n",
            "[3. 0. 0. 1.] : 0.004 _  0.452 _  0.556 _ True_ 0.000\n",
            "[1. 1. 0. 0.] : 0.320 _  0.561 _  0.132 _ False_ 0.000\n",
            "[2. 0. 1. 0.] : 0.013 _  0.816 _  0.161 _ True_ 0.000\n",
            "[3. 0. 0. 1.] :-0.009 _  0.659 _  0.395 _ False_ 1.000\n",
            "[3. 0. 0. 1.] :-0.004 _  0.502 _  0.550 _ True_ 0.000\n",
            "[3. 0. 0. 1.] : 0.050 _  0.338 _  0.449 _ True_ 0.000\n",
            "[2. 0. 1. 0.] : 0.325 _  0.395 _  0.441 _ False_ 0.000\n",
            "[2. 0. 1. 0.] : 0.344 _  0.585 _  0.076 _ True_ 0.000\n",
            "[2. 0. 1. 0.] : 0.016 _  0.728 _  0.260 _ True_ 0.000\n",
            "[2. 0. 1. 0.] : 0.011 _  0.818 _  0.241 _ True_ 0.000\n",
            "[2. 0. 1. 0.] : 0.007 _  0.752 _  0.255 _ True_ 0.000\n",
            "257994 177400 0.6876128902222532\n"
          ]
        }
      ],
      "source": [
        "\n",
        "yhibaold=[]\n",
        "maxlen=len(y_train_train)\n",
        "#maxlen=1000\n",
        "stimm=0\n",
        "for idx,_ in enumerate(y1):\n",
        "    if idx in range(0,maxlen):\n",
        "        ou1=y1[idx]\n",
        "        ou2=y2[idx]\n",
        "        ou3=y3[idx]\n",
        "        ouh=yhiba_pred[idx]\n",
        "        match=(y_train_train[idx][0]==1 and ou1==max(ou1,ou2,ou3)) or (y_train_train[idx][0]==2 and ou2==max(ou1,ou2,ou3)) or (y_train_train[idx][0]==3 and ou3==max(ou1,ou2,ou3)) \n",
        "        if match:\n",
        "            yhibaold.append(0)\n",
        "            stimm+=1\n",
        "        else:\n",
        "            yhibaold.append(1)\n",
        "        if idx%10000==1:\n",
        "            print(f\"{y_train_train[idx]} :{ou1:6.3f} _ {ou2:6.3f} _ {ou3:6.3f} _ {match}_{ouh:6.3f}\")\n",
        "\n",
        "print(maxlen,stimm,stimm/maxlen) \n",
        "#print(yhiba)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 203,
      "metadata": {},
      "outputs": [],
      "source": [
        "yhiba=yhibaold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 208,
      "metadata": {
        "tags": [
          "outputPrepend"
        ]
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "n 625, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      634. step: 0.713505\n",
            "Iteration 626, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      635. step: 0.713505\n",
            "Iteration 627, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      636. step: 0.713505\n",
            "Iteration 628, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      637. step: 0.713505\n",
            "Iteration 629, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      638. step: 0.713505\n",
            "Iteration 630, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      639. step: 0.713505\n",
            "Iteration 631, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      640. step: 0.713505\n",
            "Iteration 632, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      641. step: 0.713505\n",
            "Iteration 633, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      642. step: 0.713505\n",
            "Iteration 634, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      643. step: 0.713505\n",
            "Iteration 635, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      644. step: 0.713505\n",
            "Iteration 636, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      645. step: 0.713505\n",
            "Iteration 637, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      646. step: 0.713505\n",
            "Iteration 638, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      647. step: 0.713505\n",
            "Iteration 639, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      648. step: 0.713505\n",
            "Iteration 640, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      649. step: 0.713505\n",
            "Iteration 641, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      650. step: 0.713505\n",
            "Iteration 642, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      651. step: 0.713505\n",
            "Iteration 643, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      652. step: 0.713505\n",
            "Iteration 644, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      653. step: 0.713505\n",
            "Iteration 645, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      654. step: 0.713505\n",
            "Iteration 646, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      655. step: 0.713505\n",
            "Iteration 647, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      656. step: 0.713505\n",
            "Iteration 648, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      657. step: 0.713505\n",
            "Iteration 649, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      658. step: 0.713505\n",
            "Iteration 650, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      659. step: 0.713505\n",
            "Iteration 651, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      660. step: 0.713505\n",
            "Iteration 652, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      661. step: 0.713505\n",
            "Iteration 653, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      662. step: 0.713505\n",
            "Iteration 654, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      663. step: 0.713505\n",
            "Iteration 655, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      664. step: 0.713505\n",
            "Iteration 656, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      665. step: 0.713505\n",
            "Iteration 657, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      666. step: 0.713505\n",
            "Iteration 658, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      667. step: 0.713505\n",
            "Iteration 659, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      668. step: 0.713505\n",
            "Iteration 660, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      669. step: 0.713505\n",
            "Iteration 661, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      670. step: 0.713505\n",
            "Iteration 662, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      671. step: 0.713505\n",
            "Iteration 663, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      672. step: 0.713505\n",
            "Iteration 664, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      673. step: 0.713505\n",
            "Iteration 665, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      674. step: 0.713505\n",
            "Iteration 666, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      675. step: 0.713505\n",
            "Iteration 667, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      676. step: 0.713505\n",
            "Iteration 668, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      677. step: 0.713505\n",
            "Iteration 669, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      678. step: 0.713505\n",
            "Iteration 670, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      679. step: 0.713505\n",
            "Iteration 671, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      680. step: 0.713505\n",
            "Iteration 672, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      681. step: 0.713505\n",
            "Iteration 673, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      682. step: 0.713505\n",
            "Iteration 674, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      683. step: 0.713505\n",
            "Iteration 675, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      684. step: 0.713505\n",
            "Iteration 676, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      685. step: 0.713505\n",
            "Iteration 677, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      686. step: 0.713505\n",
            "Iteration 678, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      687. step: 0.713505\n",
            "Iteration 679, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      688. step: 0.713505\n",
            "Iteration 680, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      689. step: 0.713505\n",
            "Iteration 681, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      690. step: 0.713505\n",
            "Iteration 682, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      691. step: 0.713505\n",
            "Iteration 683, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      692. step: 0.713505\n",
            "Iteration 684, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      693. step: 0.713505\n",
            "Iteration 685, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      694. step: 0.713505\n",
            "Iteration 686, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      695. step: 0.713505\n",
            "Iteration 687, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      696. step: 0.713505\n",
            "Iteration 688, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      697. step: 0.713505\n",
            "Iteration 689, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      698. step: 0.713505\n",
            "Iteration 690, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      699. step: 0.713505\n",
            "Iteration 691, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      700. step: 0.713505\n",
            "Iteration 692, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      701. step: 0.713505\n",
            "Iteration 693, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      702. step: 0.713505\n",
            "Iteration 694, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      703. step: 0.713505\n",
            "Iteration 695, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      704. step: 0.713505\n",
            "Iteration 696, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      705. step: 0.713505\n",
            "Iteration 697, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      706. step: 0.713505\n",
            "Iteration 698, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      707. step: 0.713505\n",
            "Iteration 699, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      708. step: 0.713505\n",
            "Iteration 700, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      709. step: 0.713505\n",
            "Iteration 701, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      710. step: 0.713505\n",
            "Iteration 702, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      711. step: 0.713505\n",
            "Iteration 703, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      712. step: 0.713505\n",
            "Iteration 704, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      713. step: 0.713505\n",
            "Iteration 705, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      714. step: 0.713505\n",
            "Iteration 706, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      715. step: 0.713505\n",
            "Iteration 707, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      716. step: 0.713505\n",
            "Iteration 708, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      717. step: 0.713505\n",
            "Iteration 709, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      718. step: 0.713505\n",
            "Iteration 710, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      719. step: 0.713505\n",
            "Iteration 711, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      720. step: 0.713505\n",
            "Iteration 712, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      721. step: 0.713505\n",
            "Iteration 713, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      722. step: 0.713505\n",
            "Iteration 714, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      723. step: 0.713505\n",
            "Iteration 715, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      724. step: 0.713505\n",
            "Iteration 716, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      725. step: 0.713505\n",
            "Iteration 717, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      726. step: 0.713505\n",
            "Iteration 718, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      727. step: 0.713505\n",
            "Iteration 719, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      728. step: 0.713505\n",
            "Iteration 720, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      729. step: 0.713505\n",
            "Iteration 721, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      730. step: 0.713505\n",
            "Iteration 722, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      731. step: 0.713505\n",
            "Iteration 723, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      732. step: 0.713505\n",
            "Iteration 724, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      733. step: 0.713505\n",
            "Iteration 725, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      734. step: 0.713505\n",
            "Iteration 726, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      735. step: 0.713505\n",
            "Iteration 727, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      736. step: 0.713505\n",
            "Iteration 728, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      737. step: 0.713505\n",
            "Iteration 729, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      738. step: 0.713505\n",
            "Iteration 730, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      739. step: 0.713505\n",
            "Iteration 731, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      740. step: 0.713505\n",
            "Iteration 732, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      741. step: 0.713505\n",
            "Iteration 733, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      742. step: 0.713505\n",
            "Iteration 734, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      743. step: 0.713505\n",
            "Iteration 735, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      744. step: 0.713505\n",
            "Iteration 736, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      745. step: 0.713505\n",
            "Iteration 737, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      746. step: 0.713505\n",
            "Iteration 738, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      747. step: 0.713505\n",
            "Iteration 739, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      748. step: 0.713505\n",
            "Iteration 740, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      749. step: 0.713505\n",
            "Iteration 741, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      750. step: 0.713505\n",
            "Iteration 742, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      751. step: 0.713505\n",
            "Iteration 743, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      752. step: 0.713505\n",
            "Iteration 744, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      753. step: 0.713505\n",
            "Iteration 745, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      754. step: 0.713505\n",
            "Iteration 746, loss = 0.55731866\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
            "damage grade in      755. step: 0.713505\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-208-93a8f0108563>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[0mohiba\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclfhiba3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0myhiba\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mnhiba\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;36m950\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36mscore\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    497\u001b[0m         \"\"\"\n\u001b[0;32m    498\u001b[0m         \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 499\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    500\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    501\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_more_tags\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m   1002\u001b[0m         \"\"\"\n\u001b[0;32m   1003\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1004\u001b[1;33m         \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1005\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1006\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\u001b[0m in \u001b[0;36m_predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    697\u001b[0m                                          layer_units[i + 1])))\n\u001b[0;32m    698\u001b[0m         \u001b[1;31m# forward propagate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 699\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_pass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactivations\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    700\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mactivations\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    701\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\u001b[0m in \u001b[0;36m_forward_pass\u001b[1;34m(self, activations)\u001b[0m\n\u001b[0;32m    102\u001b[0m         \u001b[1;31m# Iterate over the hidden layers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_layers_\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 104\u001b[1;33m             activations[i + 1] = safe_sparse_dot(activations[i],\n\u001b[0m\u001b[0;32m    105\u001b[0m                                                  self.coefs_[i])\n\u001b[0;32m    106\u001b[0m             \u001b[0mactivations\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mintercepts_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m                           FutureWarning)\n\u001b[0;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\utils\\extmath.py\u001b[0m in \u001b[0;36msafe_sparse_dot\u001b[1;34m(a, b, dense_output)\u001b[0m\n\u001b[0;32m    151\u001b[0m             \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 153\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ma\u001b[0m \u001b[1;33m@\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    154\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m     if (sparse.issparse(a) and sparse.issparse(b)\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "i=0\n",
        "\n",
        "n=n+10\n",
        "\n",
        "while 1==1:\n",
        "   \n",
        "    nhiba=clfhiba3.n_iter_\n",
        "    nhiba=nhiba+10    \n",
        "    \n",
        "\n",
        "    clfhiba3.set_params(warm_start=True,verbose=3,learning_rate=\"adaptive\",max_iter=nhiba, tol=0.00000001, alpha=0.000001, solver=\"adam\")\n",
        " \n",
        "    clfhiba3.fit(X_train_train,yhiba)\n",
        "  \n",
        "\n",
        "    ohiba=clfhiba3.score(X_train_train,yhiba)\n",
        "    if nhiba>950:\n",
        "        break    \n",
        "    print(f\"damage grade in {nhiba:8}. step: {ohiba:8.6f}\" )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 201,
      "metadata": {},
      "outputs": [],
      "source": [
        "y1=clf1.predict(X_train_train)\n",
        "y2=clf2.predict(X_train_train)\n",
        "y3=clf3.predict(X_train_train)\n",
        "yhiba_pred=clfhiba3.predict(X_train_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 202,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.12928582902762065"
            ]
          },
          "metadata": {},
          "execution_count": 202
        }
      ],
      "source": [
        "clf3.train_score_[-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "metadata": {},
      "outputs": [],
      "source": [
        "def show_similarity(a,b):\n",
        "    for i in range(min(len(a),1000)):\n",
        "        print(f\"{a[i]}-{b[i]} \",end=\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 155,
      "metadata": {},
      "outputs": [],
      "source": [
        "def search_outliers(y_t1,y_pred,X_orig,yrealpred):\n",
        "    y1=[] # y orig \n",
        "    x1=[] # X orig\n",
        "    yp=[] # y predikt\n",
        "    yrp=[] # y predicted real value\n",
        "    for inx,i in enumerate(y_t1):\n",
        "        if y_t1[inx][0]==y_pred[inx]:\n",
        "            pass\n",
        "        else:\n",
        "            y1.append(y_t1[inx][0])\n",
        "            x1.append(X_orig[inx])\n",
        "            yp.append(y_pred[inx])\n",
        "            yrp.append(yrealpred[inx])\n",
        "    out=(y1,yp,x1,yrp)\n",
        "    return out\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "metadata": {},
      "outputs": [],
      "source": [
        "# futtass ez felett !!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.25      , 0.02512563, 0.1010101 , ..., 0.        , 1.        ,\n",
              "        0.22846051],\n",
              "       [0.125     , 0.00502513, 0.05050505, ..., 0.        , 1.        ,\n",
              "        0.59586796],\n",
              "       [0.125     , 0.        , 0.1010101 , ..., 0.        , 1.        ,\n",
              "        0.87090745],\n",
              "       ...,\n",
              "       [0.        , 0.        , 0.04040404, ..., 0.        , 1.        ,\n",
              "        0.47198577],\n",
              "       [0.25      , 0.02512563, 0.1010101 , ..., 0.        , 1.        ,\n",
              "        0.70982493],\n",
              "       [0.125     , 0.0201005 , 0.07070707, ..., 0.        , 1.        ,\n",
              "        0.67640778]])"
            ]
          },
          "metadata": {},
          "execution_count": 157
        }
      ],
      "source": [
        "X_train_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 197,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hiba:63732 max:257994 -- error: 24.7029 good %: 75.2971 %\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7529709993255658"
            ]
          },
          "metadata": {},
          "execution_count": 197
        }
      ],
      "source": [
        "ypred1=reconvert(X_train_train)\n",
        "check_similarity_np(ypred1,y_train_train[:,0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 198,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hiba:696 max:2607 -- error: 26.6974 good %: 73.3026 %\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7330264672036824"
            ]
          },
          "metadata": {},
          "execution_count": 198
        }
      ],
      "source": [
        "ypred2=reconvert(X_train_test)\n",
        "check_similarity_np(ypred2,y_train_test[:,0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 199,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction End\n"
          ]
        }
      ],
      "source": [
        "#outfile generation\n",
        "y_pred_ok=reconvert(X_pred_scale)\n",
        "print(\"Prediction End\")\n",
        "\n",
        "X_pred_bd=pd.read_csv(basedir+\"/orig/test_values.csv\")\n",
        "\n",
        "#y_pred_ok_int=conv_a_floatlist(y_pred_ok,range_x)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wD9rzAN4pBR5",
        "outputId": "ab2363f0-11eb-4e5a-c1a4-a71d12168323"
      },
      "source": [
        "buildingid=X_pred_bd[\"building_id\"]\n",
        "\n",
        "head2=y_pred_ok\n",
        "\n",
        "\n",
        "outdf=pd.DataFrame(data={\"damage_grade\":y_pred_ok} ,index=buildingid)\n",
        "outdf.index.name=\"building_id\"\n",
        "\n",
        "\n",
        "outdf.head()\n",
        "st=41\n",
        "sts=str(st)\n",
        "outdf.to_csv(basedir+\"/out/submission_\"+sts+\"_xgboost.csv\")\n",
        "print()\n",
        "print(basedir+\"/out/submission_\"+sts+\"_xgboost.csv\")"
      ],
      "execution_count": 200,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\nC:/Users/sipocz/OneDrive/Dokumentumok/GitHub/_EarthQuake/gpos_lin/out/submission_41_xgboost.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PjkOA2B1Pys6",
        "outputId": "e76ee986-189a-401e-9df9-722850ff2e5b"
      },
      "source": [
        "if not( _PCVERSION_):\r\n",
        "    !head \"/content/drive/My Drive/001_AI/_EarthQuake/gpos_lin/out/submission_24_xgboost.csv\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 167,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.28571429, 0.0201005 , 0.06593407, ..., 0.        , 1.        ,\n",
              "        0.56170323],\n",
              "       [0.14285714, 0.02512563, 0.13186813, ..., 0.        , 1.        ,\n",
              "        0.19576828],\n",
              "       [0.14285714, 0.00502513, 0.03296703, ..., 0.        , 1.        ,\n",
              "        0.71028087],\n",
              "       ...,\n",
              "       [0.        , 0.05025126, 0.02197802, ..., 0.        , 1.        ,\n",
              "        0.73558512],\n",
              "       [0.14285714, 0.00502513, 0.08791209, ..., 0.        , 0.        ,\n",
              "        0.21615666],\n",
              "       [0.14285714, 0.01005025, 0.10989011, ..., 0.        , 1.        ,\n",
              "        0.83998508]])"
            ]
          },
          "metadata": {},
          "execution_count": 167
        }
      ],
      "source": [
        "X_pred_scale"
      ]
    },
    {
      "source": [],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "source": [
        "<img src=\"https://www.python.org/static/community_logos/python-logo-master-v3-TM.png\" title=\"Python Logo\"/>"
      ],
      "cell_type": "markdown",
      "metadata": {}
    }
  ]
}